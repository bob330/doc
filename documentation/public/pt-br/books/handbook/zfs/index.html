<!doctype html><html class=theme-light lang=pt-br><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content><meta name=keywords content><meta name=copyright content="1995-2024 The FreeBSD Foundation"><link rel=canonical href=http://172.16.201.134:1313/pt-br/books/handbook/zfs/><title>Capítulo 19. O sistema de arquivos Z (ZFS) | FreeBSD Documentation Portal</title>
<meta name=theme-color content="#790000"><meta name=color-scheme content="system light dark high-contrast"><link rel="shortcut icon" href=http://172.16.201.134:1313/favicon.ico><link rel=stylesheet href=http://172.16.201.134:1313/styles/main.min.css><link rel=stylesheet href=http://172.16.201.134:1313/css/font-awesome-min.css><script defer src=/js/theme-chooser.min.js></script><script defer src=/js/copy-clipboard.min.js></script><script defer src=/js/search.min.js></script><meta name=twitter:card content="summary"><meta name=twitter:domain content="docs.FreeBSD.org"><meta name=twitter:site content="@freebsd"><meta name=twitter:url content="https://twitter.com/freebsd"><meta property="og:title" content="Capítulo 19. O sistema de arquivos Z (ZFS)"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:image" content="http://172.16.201.134:1313/favicon.ico"><meta property="og:image:alt" content="FreeBSD Logo"><meta property="og:locale" content="pt-br"><meta property="og:url" content="http://172.16.201.134:1313/pt-br/books/handbook/zfs/"><meta property="og:site_name" content="FreeBSD Documentation Portal"><script type=application/ld+json>{"@context":"http://schema.org","@type":"Article","url":"http:\/\/172.16.201.134:1313\/pt-br\/books\/handbook\/zfs\/","name":"FreeBSD Documentation Portal","headline":"FreeBSD Documentation Portal","description":"FreeBSD Documentation Portal"}</script></head><body><header><div class=header-container><div class=logo-menu-bars-container><a href=https://www.FreeBSD.org class=logo><img src=http://172.16.201.134:1313/images/FreeBSD-monochromatic.svg width=160 height=50 alt="FreeBSD logo">
</a><label class=menu-bars for=menu-bars><i class="fa fa-bars" aria-hidden=true></i></label></div><input id=menu-bars type=checkbox><nav><ul class=menu><li class=menu-item><input id=about type=checkbox>
<a href=# aria-label="Navigate to About section"><label class=menu-item-description for=about>About
<i class="fa fa-angle-down fa-lg" aria-hidden=true></i></label></a><ul class=sub-menu><li class=title><a href=https://www.freebsd.org/about/ target=_blank>About</a></li><li><a href=https://www.freebsd.org/about/ target=_blank>FreeBSD</a></li><li><a href=https://freebsdfoundation.org/about-us/about-the-foundation/ target=_blank>FreeBSD Foundation</a></li><li><a href=https://www.freebsd.org/internal/code-of-conduct/ target=_blank>Code of Conduct</a></li></ul></li><li class=menu-item><input id=download type=checkbox>
<a href=# aria-label="Navigate to get FreeBSD section"><label class=menu-item-description for=download>Get FreeBSD
<i class="fa fa-angle-down fa-lg" aria-hidden=true></i></label></a><ul class=sub-menu><li class=title><a href=https://www.freebsd.org/where/ target=_blank>Get FreeBSD</a></li><li><a href=https://www.freebsd.org/releases/ target=_blank>Release Information</a></li><li><a href=https://www.freebsd.org/releng/ target=_blank>Release Engineering</a></li><li><a href=https://www.freebsd.org/security/ target=_blank>Security Advisories</a></li></ul></li><li class=menu-item><input id=documentation type=checkbox>
<a href=# aria-label="Navigate to get Documentation section"><label class=menu-item-description for=documentation>Documentation
<i class="fa fa-angle-down fa-lg" aria-hidden=true></i></label></a><ul class=sub-menu><li class=title><a href=/pt-br>Documentation portal</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook>FreeBSD Handbook</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/porters-handbook>Porter's Handbook</a></li><li><a href=https://docs.FreeBSD.org/en/books/fdp-primer>Documentation Project Handbook</a></li><li><a href=https://man.FreeBSD.org target=_blank>Manual pages</a></li><li><a href=https://papers.FreeBSD.org target=_blank>Presentations and papers</a></li><li><a href=https://wiki.FreeBSD.org target=_blank>Wiki</a></li><li><a href=http://172.16.201.134:1313/pt-br/books>Books</a></li><li><a href=http://172.16.201.134:1313/pt-br/articles>Articles</a></li></ul></li><li class=menu-item><input id=community type=checkbox>
<a href=# aria-label="Navigate to get Community section"><label class=menu-item-description for=community>Community
<i class="fa fa-angle-down fa-lg" aria-hidden=true></i></label></a><ul class=sub-menu><li class=title><a href=https://www.freebsd.org/community/>Community</a></li><li><a href=http://172.16.201.134:1313/pt-br/articles/contributing>Get involved</a></li><li><a href=https://forums.freebsd.org/ target=_blank>Forum</a></li><li><a href=https://lists.freebsd.org/ target=_blank>Mailing lists</a></li><li><a href=https://wiki.freebsd.org/IRC/Channels target=_blank>IRC Channels</a></li><li><a href=https://bugs.freebsd.org/bugzilla/ target=_blank>Bug Tracker</a></li><li><a href=https://www.freebsd.org/support/ target=_blank>Support</a></li></ul></li></ul></nav><div class=search-donate-container><form class=search method=get id=search-header-form action=https://docs.freebsd.org/search name=search-header-form><input type=hidden name=DB value=pt-br>
<input id=words name=P type=text size=20 maxlength=255>
<button>
<i class="fa fa-search" aria-hidden=true></i></button></form><div class=donate><a href=https://freebsdfoundation.org/donate/ target=_blank><span class=heart>♥</span>
Donate</a></div></div></div></header><input type=checkbox class="hidden toggle" id=menu-control><main class=main-wrapper-book><a id=top></a><aside class=book-menu><div class=book-menu-content><input id=search-book type=text placeholder=Pesquisar aria-label=Pesquisar maxlength=128><nav id=MenuContents><ul><li><input type=checkbox id=chapter-eb2a98ce203d8afd517726e6d8776be3 class=toggle>
<label class="icon cursor" for=chapter-eb2a98ce203d8afd517726e6d8776be3><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/preface/>Prefácio</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/preface/#preface-audience>Audiência Pretendida</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/preface/#preface-changes-from3>Mudanças desde a Terceira Edição</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/preface/#preface-changes-from2>Mudanças desde a Segunda Edição (2004)</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/preface/#preface-changes>Mudanças desde a Primeira Edição (2001)</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/preface/#preface-overview>Organização deste Livro</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/preface/#preface-conv>Convenções utilizadas neste livro</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/preface/#preface-acknowledgements>Agradecimentos</a></li></ul></li><li><input type=checkbox id=chapter-14a525fce014b90b8a458a894818255a class=toggle>
<label for=chapter-14a525fce014b90b8a458a894818255a><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/parti/>Parte I. Primeiros Passos</a></li><li><input type=checkbox id=chapter-f9c9f3451644df30d224350da97d5da6 class=toggle>
<label class="icon cursor" for=chapter-f9c9f3451644df30d224350da97d5da6><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/introduction/>Capítulo 1. Introdução</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/introduction/#introduction-synopsis>1.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/introduction/#nutshell>1.2. Bem vindo ao FreeBSD!</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/introduction/#history>1.3. Sobre o Projeto FreeBSD</a></li></ul></li><li><input type=checkbox id=chapter-f693a3fa687a72d63ec8129ee302d664 class=toggle>
<label class="icon cursor" for=chapter-f693a3fa687a72d63ec8129ee302d664><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/bsdinstall/>Capítulo 2. Instalando o FreeBSD</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/bsdinstall/#bsdinstall-synopsis>2.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/bsdinstall/#bsdinstall-hardware>2.2. Requisitos mínimos de hardware</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/bsdinstall/#bsdinstall-pre>2.3. Tarefas de Pré-instalação</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/bsdinstall/#bsdinstall-start>2.4. Iniciando a instalação</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/bsdinstall/#using-bsdinstall>2.5. Usando o bsdinstall</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/bsdinstall/#bsdinstall-partitioning>2.6. Alocando o espaço em disco</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/bsdinstall/#bsdinstall-fetching-distribution>2.7. Fazendo o download dos arquivos de distribuição</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/bsdinstall/#bsdinstall-post>2.8. Contas, Time Zone, Serviços e Hardening</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/bsdinstall/#bsdinstall-network>2.9. Interfaces de Rede</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/bsdinstall/#bsdinstall-install-trouble>2.10. Solução de problemas</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/bsdinstall/#using-live-cd>2.11. Usando o Live CD</a></li></ul></li><li><input type=checkbox id=chapter-9f6db261075f578742036fcc6000eecd class=toggle>
<label class="icon cursor" for=chapter-9f6db261075f578742036fcc6000eecd><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/basics/>Capítulo 3. Fundamentos do FreeBSD</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/basics/#basics-synopsis>3.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/basics/#consoles>3.2. Consoles e Terminais Virtuais</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/basics/#users-synopsis>3.3. Usuários e Gerenciamento Básico de Contas</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/basics/#permissions>3.4. Permissões</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/basics/#dirstructure>3.5. Estrutura de Diretórios</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/basics/#disk-organization>3.6. Organização dos Discos</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/basics/#mount-unmount>3.7. Montando e Desmontando Sistemas de Arquivos</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/basics/#basics-processes>3.8. Processos e Daemons</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/basics/#shells>3.9. Shells</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/basics/#editors>3.10. Editores de Texto</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/basics/#basics-devices>3.11. Dispositivos e nós de dispositivos</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/basics/#basics-more-information>3.12. Páginas de Manual</a></li></ul></li><li><input type=checkbox id=chapter-01c5707e95d14c0ff84bf62600c958d1 class=toggle>
<label class="icon cursor" for=chapter-01c5707e95d14c0ff84bf62600c958d1><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/ports/>Capítulo 4. Instalando Aplicativos: Pacotes e Ports</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/ports/#ports-synopsis>4.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/ports/#ports-overview>4.2. Visão geral sobre a Instalação de Software</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/ports/#ports-finding-applications>4.3. Encontrando Software</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/ports/#pkgng-intro>4.4. Usando o pkg para o gerenciamento de pacotes binários</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/ports/#ports-using>4.5. Usando a Coleção de Ports</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/ports/#ports-poudriere>4.6. Compilando Pacotes com o Poudriere</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/ports/#ports-nextsteps>4.7. Considerações pós-instalação</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/ports/#ports-broken>4.8. Lidando com ports quebrados</a></li></ul></li><li><input type=checkbox id=chapter-3405c00581365a8b5d16af70fe4d1b72 class=toggle>
<label class="icon cursor" for=chapter-3405c00581365a8b5d16af70fe4d1b72><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/x11/>Capítulo 5. O sistema X Window</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/x11/#x11-synopsis>5.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/x11/#x-understanding>5.2. Terminologia</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/x11/#x-install>5.3. Instalando o Xorg</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/x11/#x-config>5.4. Configuração do Xorg</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/x11/#x-fonts>5.5. Usando fontes no Xorg</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/x11/#x-xdm>5.6. O Gerenciador de Display X</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/x11/#x11-wm>5.7. Ambientes de desktop</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/x11/#x-compiz-fusion>5.8. Instalando o Compiz Fusion</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/x11/#x11-troubleshooting>5.9. Solução de problemas</a></li></ul></li><li><input type=checkbox id=chapter-0eab3565e8f59f5a8a896dfba7eb3680 class=toggle>
<label for=chapter-0eab3565e8f59f5a8a896dfba7eb3680><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/partii/>Parte II. Tarefas comuns</a></li><li><input type=checkbox id=chapter-b33cf28993f3f7bf5baf036e79da0f39 class=toggle>
<label class="icon cursor" for=chapter-b33cf28993f3f7bf5baf036e79da0f39><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/desktop/>Capítulo 6. Aplicações de Desktop</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/desktop/#desktop-synopsis>6.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/desktop/#desktop-browsers>6.2. Navegadores</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/desktop/#desktop-productivity>6.3. Produtividade</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/desktop/#desktop-viewers>6.4. Visualizadores de Documentos</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/desktop/#desktop-finance>6.5. Finanças</a></li></ul></li><li><input type=checkbox id=chapter-152f694a19312ad72ec7bb4e1c3c33b2 class=toggle>
<label class="icon cursor" for=chapter-152f694a19312ad72ec7bb4e1c3c33b2><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/multimedia/>Capítulo 7. Multimídia</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/multimedia/#multimedia-synopsis>7.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/multimedia/#sound-setup>7.2. Configurando a Placa de Som</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/multimedia/#sound-mp3>7.3. Áudio MP3</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/multimedia/#video-playback>7.4. Reprodução de Vídeo</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/multimedia/#tvcard>7.5. Placas de TV</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/multimedia/#mythtv>7.6. MythTV</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/multimedia/#scanners>7.7. Scanners de Imagem</a></li></ul></li><li><input type=checkbox id=chapter-80888b4ee02e3e409e5f71cf97a36450 class=toggle>
<label class="icon cursor" for=chapter-80888b4ee02e3e409e5f71cf97a36450><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/kernelconfig/>Capítulo 8. Configurando o kernel do FreeBSD</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/kernelconfig/#kernelconfig-synopsis>8.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/kernelconfig/#kernelconfig-custom-kernel>8.2. Por que compilar um kernel personalizado?</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/kernelconfig/#kernelconfig-devices>8.3. Encontrando o hardware do sistema</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/kernelconfig/#kernelconfig-config>8.4. O Arquivo de Configuração</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/kernelconfig/#kernelconfig-building>8.5. Criando e Instalando um Kernel Customizado</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/kernelconfig/#kernelconfig-trouble>8.6. Se algo der errado</a></li></ul></li><li><input type=checkbox id=chapter-cb174c55879b17ab955f2f16989a79e0 class=toggle>
<label class="icon cursor" for=chapter-cb174c55879b17ab955f2f16989a79e0><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/printing/>Capítulo 9. Impressão</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/printing/#printing-quick-start>9.1. Inicio Rápido</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/printing/#printing-connections>9.2. Conexões de Impressora</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/printing/#printing-pdls>9.3. Linguagens de Descrição de Página Comuns</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/printing/#printing-direct>9.4. Impressão Direta</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/printing/#printing-lpd>9.5. LPD (Daemon de impressora de linha)</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/printing/#printing-other>9.6. Outros sistemas de impressão</a></li></ul></li><li><input type=checkbox id=chapter-c12b8c3f2a8fcefce87087241f695c83 class=toggle>
<label class="icon cursor" for=chapter-c12b8c3f2a8fcefce87087241f695c83><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/linuxemu/>Capítulo 10. Compatibilidade binária com o Linux®</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/linuxemu/#linuxemu-synopsis>10.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/linuxemu/#linuxemu-lbc-install>10.2. Configurando a compatibilidade binária com o Linux™</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/linuxemu/#linuxemu-advanced>10.3. Tópicos Avançados</a></li></ul></li><li><input type=checkbox id=chapter-03b11ba627b9a0c85b247f5641bde272 class=toggle>
<label for=chapter-03b11ba627b9a0c85b247f5641bde272><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/partiii/>Parte III. Administração do Sistema</a></li><li><input type=checkbox id=chapter-6c31587f8d736319f099cd4dc1961301 class=toggle>
<label class="icon cursor" for=chapter-6c31587f8d736319f099cd4dc1961301><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/config/>Capítulo 11. Configuração e Ajuste</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/config/#config-synopsis>11.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/config/#configtuning-starting-services>11.2. Inicialização de Serviços</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/config/#configtuning-cron>11.3. Configurando o cron(8)</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/config/#configtuning-rcd>11.4. Gerenciando Serviços no FreeBSD</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/config/#config-network-setup>11.5. Configurando Placas de Interface de Rede</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/config/#configtuning-virtual-hosts>11.6. Hosts Virtuais</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/config/#configtuning-syslog>11.7. Configurando o log do sistema</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/config/#configtuning-configfiles>11.8. Arquivos de Configuração</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/config/#configtuning-sysctl>11.9. Efetuando ajustes com o sysctl(8)</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/config/#configtuning-disk>11.10. Otimização de Discos</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/config/#configtuning-kernel-limits>11.11. Ajustando os Limites do Kernel</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/config/#adding-swap-space>11.12. Adicionando Espaço de Swap</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/config/#acpi-overview>11.13. Gerenciamento de energia e recursos</a></li></ul></li><li><input type=checkbox id=chapter-459f0012b3b4f0b6b123010f029da5e4 class=toggle>
<label class="icon cursor" for=chapter-459f0012b3b4f0b6b123010f029da5e4><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/boot/>Capítulo 12. O processo de inicialização do FreeBSD</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/boot/#boot-synopsis>12.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/boot/#boot-introduction>12.2. Processo de Inicialização do FreeBSD</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/boot/#boot-splash>12.3. Configurando telas iniciais de inicialização</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/boot/#device-hints>12.4. Sugestões de dispositivos</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/boot/#boot-shutdown>12.5. Sequência de Desligamento</a></li></ul></li><li><input type=checkbox id=chapter-917c75fcffbb14d48ed6d0a48e7028f2 class=toggle>
<label class="icon cursor" for=chapter-917c75fcffbb14d48ed6d0a48e7028f2><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/security/>Capítulo 13. Segurança</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/security/#security-synopsis>13.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/security/#security-intro>13.2. Introdução</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/security/#one-time-passwords>13.3. Senhas de Uso Unico</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/security/#tcpwrappers>13.4. TCP Wrapper</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/security/#kerberos5>13.5. Kerberos</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/security/#openssl>13.6. OpenSSL</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/security/#ipsec>13.7. VPN Sobre IPsec</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/security/#openssh>13.8. OpenSSH</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/security/#fs-acl>13.9. Listas de Controle de Acesso</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/security/#security-pkg>13.10. Monitorando Problemas de Segurança de Terceiros</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/security/#security-advisories>13.11. Avisos de Segurança do FreeBSD</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/security/#security-accounting>13.12. Auditoria de Processo</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/security/#security-resourcelimits>13.13. Limites de Recursos</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/security/#security-sudo>13.14. Administração Compartilhada com Sudo</a></li></ul></li><li><input type=checkbox id=chapter-1a2a8e719703649c2c66d99aa7a25fd4 class=toggle>
<label class="icon cursor" for=chapter-1a2a8e719703649c2c66d99aa7a25fd4><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/jails/>Capítulo 14. Jails</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/jails/#jails-synopsis>14.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/jails/#jails-terms>14.2. Termos Relacionados à Jails</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/jails/#jails-build>14.3. Criando e Controlando Jails</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/jails/#jails-tuning>14.4. Tuning e Administração</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/jails/#jails-application>14.5. Atualizando Múltiplas Jails</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/jails/#jails-ezjail>14.6. Gerenciando Jails com o ezjail</a></li></ul></li><li><input type=checkbox id=chapter-8f4620c77e572cbb58917911a33c73cf class=toggle>
<label class="icon cursor" for=chapter-8f4620c77e572cbb58917911a33c73cf><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/mac/>Capítulo 15. Controle de acesso obrigatório</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/mac/#mac-synopsis>15.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/mac/#mac-inline-glossary>15.2. Termos chave</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/mac/#mac-understandlabel>15.3. Entendendo os rótulos MAC</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/mac/#mac-planning>15.4. Planejando a configuração de segurança</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/mac/#mac-policies>15.5. Políticas MAC Disponíveis</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/mac/#mac-userlocked>15.6. Bloqueio do Usuário</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/mac/#mac-implementing>15.7. Nagios em Jail MAC</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/mac/#mac-troubleshoot>15.8. Solução de problemas do framework MAC</a></li></ul></li><li><input type=checkbox id=chapter-9598d66a76cb3182057b6bcd775149a0 class=toggle>
<label class="icon cursor" for=chapter-9598d66a76cb3182057b6bcd775149a0><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/audit/>Capítulo 16. Auditoria de Evento de Segurança</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/audit/#audit-synopsis>16.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/audit/#audit-inline-glossary>16.2. Termos chave</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/audit/#audit-config>16.3. Configuração de Auditoria</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/audit/#audit-administration>16.4. Trabalhando com Trilhas de Auditoria</a></li></ul></li><li><input type=checkbox id=chapter-e1edcad13d9db6e8e4cb645d378ecfaf class=toggle>
<label class="icon cursor" for=chapter-e1edcad13d9db6e8e4cb645d378ecfaf><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/disks/>Capítulo 17. Armazenamento</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/disks/#disks-synopsis>17.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/disks/#disks-adding>17.2. Adicionando Discos</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/disks/#disks-growing>17.3. Redimensionando e Ampliando Discos</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/disks/#usb-disks>17.4. Dispositivos de Armazenamento USB</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/disks/#creating-cds>17.5. Criando e Usando Mídia em CD</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/disks/#creating-dvds>17.6. Criando e Usando Mídia de DVD</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/disks/#floppies>17.7. Criando e Usando Disquetes</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/disks/#backup-basics>17.8. Noções Básicas de Backup</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/disks/#disks-virtual>17.9. Discos de Memória</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/disks/#snapshots>17.10. Snapshots de Sistemas de Arquivos</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/disks/#quotas>17.11. Cotas de Disco</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/disks/#disks-encrypting>17.12. Criptografando Partições de Disco</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/disks/#swap-encrypting>17.13. Criptografando Swap</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/disks/#disks-hast>17.14. Alta Disponibilidade de Armazenamento (HAST)</a></li></ul></li><li><input type=checkbox id=chapter-dde37901a0e0ea32745b67607854900f class=toggle>
<label class="icon cursor" for=chapter-dde37901a0e0ea32745b67607854900f><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/geom/>Capítulo 18. GEOM: Framework de Transformação de Disco Modular</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/geom/#geom-synopsis>18.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/geom/#geom-striping>18.2. RAID0 - Striping</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/geom/#geom-mirror>18.3. RAID1 - Espelhamento</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/geom/#geom-raid3>18.4. RAID3 - Distribuição em Nível de Byte com Paridade Dedicada</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/geom/#geom-graid>18.5. Dispositivos RAID por Software</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/geom/#geom-ggate>18.6. GEOM Network Gate</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/geom/#geom-glabel>18.7. Rotulando Dispositivos de Disco</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/geom/#geom-gjournal>18.8. Journaling UFS através do GEOM</a></li></ul></li><li><input type=checkbox id=chapter-73e82560fcb7145b7c0e2ec47af8fc04 class=toggle checked>
<label class="icon cursor" for=chapter-73e82560fcb7145b7c0e2ec47af8fc04><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/zfs/>Capítulo 19. O sistema de arquivos Z (ZFS)</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/zfs/#zfs-differences>19.1. O que torna o ZFS diferente</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/zfs/#zfs-quickstart>19.2. Guia de Início Rápido</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/zfs/#zfs-zpool>19.3. Administração <code>zpool</code></a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/zfs/#zfs-zfs>19.4. Administração do <code>zfs</code></a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/zfs/#zfs-zfs-allow>19.5. Administração Delegada</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/zfs/#zfs-advanced>19.6. Tópicos Avançados</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/zfs/#zfs-links>19.7. Recursos adicionais</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/zfs/#zfs-term>19.8. Recursos e terminologia do ZFS</a></li></ul></li><li><input type=checkbox id=chapter-7af71270807eb7b70cd3eedc6577b254 class=toggle>
<label class="icon cursor" for=chapter-7af71270807eb7b70cd3eedc6577b254><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/filesystems/>Capítulo 20. Outros Sistemas de Arquivos</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/filesystems/#filesystems-synopsis>20.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/filesystems/#filesystems-linux>20.2. Sistemas de arquivos do Linux™</a></li></ul></li><li><input type=checkbox id=chapter-375257268d95faaf87faf4f7a2e6aa67 class=toggle>
<label class="icon cursor" for=chapter-375257268d95faaf87faf4f7a2e6aa67><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/virtualization/>Capítulo 21. Virtualização</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/virtualization/#virtualization-synopsis>21.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/virtualization/#virtualization-guest-parallels>21.2. FreeBSD como Sistema Operacional Convidado no Parallels para Mac OS™ X</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/virtualization/#virtualization-guest-virtualpc>21.3. FreeBSD como sistema convidado no Virtual PC para Windows™</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/virtualization/#virtualization-guest-vmware>21.4. FreeBSD como Sistema Operacional Convidado no VMware Fusion para Mac OS™</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/virtualization/#virtualization-guest-virtualbox>21.5. FreeBSD como Sistema Operacional Convidado no VirtualBox™</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/virtualization/#virtualization-host-virtualbox>21.6. FreeBSD como Host com VirtualBox™</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/virtualization/#virtualization-host-bhyve>21.7. FreeBSD como um Host bhyve</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/virtualization/#virtualization-host-xen>21.8. FreeBSD como Host Xen™</a></li></ul></li><li><input type=checkbox id=chapter-49f1e96591c090304ea532012257f4ef class=toggle>
<label class="icon cursor" for=chapter-49f1e96591c090304ea532012257f4ef><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/l10n/>Capítulo 22. Localização - Uso e Configuração do i18n/L10n</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/l10n/#l10n-synopsis>22.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/l10n/#using-localization>22.2. Usando Localização</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/l10n/#l10n-compiling>22.3. Encontrando Aplicações i18n</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/l10n/#lang-setup>22.4. Configuração de Localização para Idiomas Específicos</a></li></ul></li><li><input type=checkbox id=chapter-dead2b4c5ea325dd390a9b0dccd8f763 class=toggle>
<label class="icon cursor" for=chapter-dead2b4c5ea325dd390a9b0dccd8f763><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/cutting-edge/>Capítulo 23. Atualização e Upgrade do FreeBSD</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/cutting-edge/#updating-upgrading-synopsis>23.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/cutting-edge/#updating-upgrading-freebsdupdate>23.2. Atualização do FreeBSD</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/cutting-edge/#updating-upgrading-documentation>23.3. Atualizando o Conjunto de Documentação</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/cutting-edge/#current-stable>23.4. Acompanhando um ramo de desenvolvimento</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/cutting-edge/#makeworld>23.5. Atualizando o FreeBSD a partir do código fonte</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/cutting-edge/#small-lan>23.6. Atualização de várias máquinas</a></li></ul></li><li><input type=checkbox id=chapter-29c1eeb0e9dedc487a98399e2737ee8a class=toggle>
<label class="icon cursor" for=chapter-29c1eeb0e9dedc487a98399e2737ee8a><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/dtrace/>Chapter 24. DTrace</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/dtrace/#dtrace-synopsis>24.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/dtrace/#dtrace-implementation>24.2. Diferenças de Implementação</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/dtrace/#dtrace-enable>24.3. Ativando o Suporte do DTrace</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/dtrace/#dtrace-using>24.4. Usando o DTrace</a></li></ul></li><li><input type=checkbox id=chapter-913e72bfb3d6947b2869d3e9447a6eaa class=toggle>
<label class="icon cursor" for=chapter-913e72bfb3d6947b2869d3e9447a6eaa><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/usb-device-mode/>Capítulo 25. Modo de dispositivo USB/USB OTG</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/usb-device-mode/#usb-device-mode-synopsis>25.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/usb-device-mode/#usb-device-mode-terminals>25.2. Portas Seriais Virtuais USB</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/usb-device-mode/#usb-device-mode-network>25.3. Interfaces de rede do modo de dispositivo USB</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/usb-device-mode/#usb-device-mode-storage>25.4. Dispositivo de armazenamento virtual USB</a></li></ul></li><li><input type=checkbox id=chapter-bbd25f9a194f9c39ca2d658c75767db5 class=toggle>
<label for=chapter-bbd25f9a194f9c39ca2d658c75767db5><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/partiv/>Parte IV. Comunicação de rede</a></li><li><input type=checkbox id=chapter-499dab596afd7ddac77e80295314e0dd class=toggle>
<label class="icon cursor" for=chapter-499dab596afd7ddac77e80295314e0dd><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/serialcomms/>Capítulo 26. Comunicações Seriais</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/serialcomms/#serial-synopsis>26.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/serialcomms/#serial>26.2. Terminologia serial e hardware</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/serialcomms/#term>26.3. Terminais</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/serialcomms/#dialup>26.4. Serviço Dial-in</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/serialcomms/#dialout>26.5. Serviço de Dial-in</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/serialcomms/#serialconsole-setup>26.6. Configurando o Console Serial</a></li></ul></li><li><input type=checkbox id=chapter-95e4571c48bee1cced5e84a538d302e3 class=toggle>
<label class="icon cursor" for=chapter-95e4571c48bee1cced5e84a538d302e3><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/ppp-and-slip/>Capítulo 27. PPP</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/ppp-and-slip/#ppp-and-slip-synopsis>27.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/ppp-and-slip/#userppp>27.2. Configurando o PPP</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/ppp-and-slip/#ppp-troubleshoot>27.3. Solução de problemas de conexões PPP</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/ppp-and-slip/#pppoe>27.4. Usando o PPP sobre Ethernet (PPPoE)</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/ppp-and-slip/#pppoa>27.5. Usando PPP sobre ATM (PPPoA)</a></li></ul></li><li><input type=checkbox id=chapter-f089ac726c401c9b4bd5c34a295e11bb class=toggle>
<label class="icon cursor" for=chapter-f089ac726c401c9b4bd5c34a295e11bb><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/mail/>Capítulo 28. Correio Eletrônico</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/mail/#mail-synopsis>28.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/mail/#mail-using>28.2. Componentes de Email</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/mail/#sendmail>28.3. Arquivos de Configuração do Sendmail</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/mail/#mail-changingmta>28.4. Alterando o Mail Transfer Agent</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/mail/#mail-trouble>28.5. Solução de problemas</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/mail/#mail-advanced>28.6. Tópicos Avançados</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/mail/#outgoing-only>28.7. Configurando Apenas Envio</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/mail/#SMTP-dialup>28.8. Usando Email com uma Conexão Dialup</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/mail/#SMTP-Auth>28.9. Autenticação SMTP</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/mail/#mail-agents>28.10. Mail User Agents</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/mail/#mail-fetchmail>28.11. Usando o fetchmail</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/mail/#mail-procmail>28.12. Usando o procmail</a></li></ul></li><li><input type=checkbox id=chapter-6de4de3fe925639d4175ce4b6f8c1829 class=toggle>
<label class="icon cursor" for=chapter-6de4de3fe925639d4175ce4b6f8c1829><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/network-servers/>Capítulo 29. Servidores de Rede</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/network-servers/#network-servers-synopsis>29.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/network-servers/#network-inetd>29.2. O super-servidor inetd</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/network-servers/#network-nfs>29.3. Network File System (NFS)</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/network-servers/#network-nis>29.4. Sistema de Informação de Rede (NIS)</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/network-servers/#network-ldap>29.5. Protocolo leve de acesso de diretório ( LDAP )</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/network-servers/#network-dhcp>29.6. Protocolo de configuração dinâmica de hosts (DHCP)</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/network-servers/#network-dns>29.7. Sistema de Nomes de Domínio (DNS)</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/network-servers/#network-apache>29.8. Servidor HTTP Apache</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/network-servers/#network-ftp>29.9. Protocolo de Transferência de Arquivos (FTP)</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/network-servers/#network-samba>29.10. Serviços de arquivos e impressão para clientes Microsoft™Windows™ Clients (Samba)</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/network-servers/#network-ntp>29.11. Sincronização de Relógio com NTP</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/network-servers/#network-iscsi>29.12. Inicializador iSCSI e Configuração Alvo</a></li></ul></li><li><input type=checkbox id=chapter-776d855c7b75e048f90b5c2c9b35ffe0 class=toggle>
<label class="icon cursor" for=chapter-776d855c7b75e048f90b5c2c9b35ffe0><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/firewalls/>Capítulo 30. Firewalls</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/firewalls/#firewalls-intro>30.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/firewalls/#firewalls-concepts>30.2. Conceitos de Firewall</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/firewalls/#firewalls-pf>30.3. PF</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/firewalls/#firewalls-ipfw>30.4. IPFW</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/firewalls/#firewalls-ipf>30.5. IPFILTER (IPF)</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/firewalls/#firewalls-blacklistd>30.6. Blacklistd</a></li></ul></li><li><input type=checkbox id=chapter-5b07f776a0e6155c1c89aa0d15610380 class=toggle>
<label class="icon cursor" for=chapter-5b07f776a0e6155c1c89aa0d15610380><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/advanced-networking/>Capítulo 31. Rede Avançada</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/advanced-networking/#advanced-networking-synopsis>31.1. Sinopse</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/advanced-networking/#network-routing>31.2. Gateways e Rotas</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/advanced-networking/#network-wireless>31.3. Rede sem fio</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/advanced-networking/#network-usb-tethering>31.4. USB Tethering</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/advanced-networking/#network-bluetooth>31.5. Bluetooth</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/advanced-networking/#network-bridging>31.6. Bridging</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/advanced-networking/#network-aggregation>31.7. Agregação de links e failover</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/advanced-networking/#network-diskless>31.8. Operação Diskless com PXE</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/advanced-networking/#network-ipv6>31.9. IPv6</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/advanced-networking/#carp>31.10. Protocolo Comum de Redundância de Endereços (CARP)</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/advanced-networking/#network-vlan>31.11. VLANs</a></li></ul></li><li><input type=checkbox id=chapter-171a77aa9d067a1024f849470e1f33e8 class=toggle>
<label for=chapter-171a77aa9d067a1024f849470e1f33e8><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/partv/>Parte V. Apêndices</a></li><li><input type=checkbox id=chapter-8050f436a0a7986a4aaded93d8e49469 class=toggle>
<label class="icon cursor" for=chapter-8050f436a0a7986a4aaded93d8e49469><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/mirrors/>Apêndice A. Obtendo o FreeBSD</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/mirrors/#mirrors-cdrom>A.1. CD and DVD Sets</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/mirrors/#mirrors-ftp>A.2. Sites de FTP</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/mirrors/#svn>A.3. Usando o Subversion</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/mirrors/#mirrors-rsync>A.4. Usando o rsync</a></li></ul></li><li><input type=checkbox id=chapter-128b630a8f88f158e7027fe6c2184d21 class=toggle>
<label class="icon cursor" for=chapter-128b630a8f88f158e7027fe6c2184d21><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/bibliography/>Apêndice B. Bibliografia</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/bibliography/#bibliography-freebsd>B.1. Livros específicos para o FreeBSD</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/bibliography/#bibliography-userguides>B.2. Guias de usuários</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/bibliography/#bibliography-adminguides>B.3. Guias de Administradores</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/bibliography/#bibliography-programmers>B.4. Guias de programadores</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/bibliography/#bibliography-osinternals>B.5. Internals do sistema operacional</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/bibliography/#bibliography-security>B.6. Referências de segurança</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/bibliography/#bibliography-hardware>B.7. Referências de Hardware</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/bibliography/#bibliography-history>B.8. História do UNIX™</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/bibliography/#bibliography-journals>B.9. Periódicos, Jornais e Revistas</a></li></ul></li><li><input type=checkbox id=chapter-8bbb8867c46dac315e2253945d8c18a8 class=toggle>
<label class="icon cursor" for=chapter-8bbb8867c46dac315e2253945d8c18a8><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/eresources/>Apêndice C. Recursos na Internet</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/eresources/#eresources-www>C.1. Websites</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/eresources/#eresources-mail>C.2. Listas de Discussão</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/eresources/#eresources-news>C.3. Grupos de Notícias Usenet</a></li><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/eresources/#eresources-web>C.4. Espelhos Oficiais</a></li></ul></li><li><input type=checkbox id=chapter-a80ea4f5a4480b8725422710f954ef36 class=toggle>
<label class="icon cursor" for=chapter-a80ea4f5a4480b8725422710f954ef36><a role=button></a></label><a href=http://172.16.201.134:1313/pt-br/books/handbook/pgpkeys/>Apêndice D. Chaves OpenPGP</a><ul><li><a href=http://172.16.201.134:1313/pt-br/books/handbook/pgpkeys/#pgpkeys-officers>D.1. Administradores</a></li></ul></li><li></li></ul></nav></div></aside><div class=book><div class=book-menu-mobile><label for=menu-control><span class=menu-control-button><i class="fa fa-list" aria-hidden=true title=Menu></i>
Menu</span></label></div><h1 class=title>Capítulo 19. O sistema de arquivos Z (ZFS)</h1><div class="admonitionblock note"><p><i class="fa fa-exclamation-circle" aria-hidden=true></i>
Esta tradução pode estar desatualizada. Para ajudar com as traduções, acesse a <a href=https://translate-dev.freebsd.org/ target=_blank>ferramenta de traduções do FreeBSD</a>.</p></div><div class=toc-mobile><h3>Índice</h3><nav id=TableOfContents><ul><li><a href=#zfs-differences>19.1. O que torna o ZFS diferente</a></li><li><a href=#zfs-quickstart>19.2. Guia de Início Rápido</a></li><li><a href=#zfs-zpool>19.3. Administração <code>zpool</code></a></li><li><a href=#zfs-zfs>19.4. Administração do <code>zfs</code></a></li><li><a href=#zfs-zfs-allow>19.5. Administração Delegada</a></li><li><a href=#zfs-advanced>19.6. Tópicos Avançados</a></li><li><a href=#zfs-links>19.7. Recursos adicionais</a></li><li><a href=#zfs-term>19.8. Recursos e terminologia do ZFS</a></li></ul></nav></div><div class=book-content><div id=preamble><div class=sectionbody><div class=paragraph><p>O <em>Sistema de Arquivos Z</em>, ou ZFS, é um sistema de arquivos avançado projetado para superar muitos dos principais problemas encontrados em projetos anteriores.</p></div><div class=paragraph><p>Originalmente desenvolvido pela Sun™, o desenvolvimento contínuo do ZFS em código aberto foi movido para o <a href=http://open-zfs.org>Projeto OpenZFS</a>.</p></div><div class=paragraph><p>O ZFS tem três metas principais de design:</p></div><div class=ulist><ul><li><p>Integridade de dados: Todos os dados incluem um <a href=#zfs-term-checksum>checksum</a> dos dados. Quando os dados são gravados, o checksum é calculado e gravado junto com eles. Quando esses dados são lidos posteriormente, o checksum é calculado novamente. Se os checksum’s não corresponderem, um erro de dados foi detectado. O ZFS tentará corrigir automaticamente os erros quando houver redundância de dados disponível.</p></li><li><p>Armazenamento em pool: os dispositivos de armazenamento físico são adicionados em um pool e o espaço de armazenamento é alocado a partir desse pool compartilhado. O espaço está disponível para todos os sistemas de arquivos e pode ser aumentado pela adição de novos dispositivos de armazenamento ao pool.</p></li><li><p>Performance: vários mecanismos de cache fornecem uma maior performance. O <a href=#zfs-term-arc>ARC</a> é um avançado cache de leitura baseado em memória. Um segundo nível de cache de leitura baseado em disco pode ser adicionado com o <a href=#zfs-term-l2arc>L2ARC</a>, e o cache síncrono de escrita baseado em disco está disponível com <a href=#zfs-term-zil>ZIL</a>.</p></li></ul></div><div class=paragraph><p>Uma lista completa de features e terminologias é mostrada em <a href=#zfs-term>Recursos e terminologia do ZFS</a>.</p></div></div></div><div class=sect1><h2 id=zfs-differences>19.1. O que torna o ZFS diferente<a class=anchor href=#zfs-differences></a></h2><div class=sectionbody><div class=paragraph><p>O ZFS é significativamente diferente de qualquer outro sistema de arquivos existente, porque ele é mais do que apenas um simples sistema de arquivos. A combinação das funções tradicionalmente separadas de gerenciamento de volume e de sistema de arquivos, fornece ao ZFS vantagens exclusivas. O sistema de arquivos agora conhece a estrutura abaixo dos discos. Os sistemas de arquivos tradicionais só podem ser criados em um único disco por vez. Se houvesse dois discos, dois sistemas de arquivos separados teriam que ser criados. Em uma configuração de hardware tradicional RAID, esse problema foi contornado apresentando ao sistema operacional um único disco lógico composto pelo espaço fornecido por vários discos físicos, sobre o qual o sistema operacional colocava um sistema de arquivos. Mesmo no caso de soluções de software RAID como as fornecidas pelo GEOM, o sistema de arquivos UFS, que está no topo da transformação RAID, acreditava que estava lidando com um único dispositivo físico. A combinação feita pelo ZFS do gerenciador de volumes e do sistema de arquivos resolve isso e permite a criação de vários sistemas de arquivos, todos compartilhando um pool de armazenamento disponível. Uma das maiores vantagens do reconhecimento do layout físico dos discos pelo ZFS é que os sistemas de arquivos existentes podem ser expandidos automaticamente quando novos discos são adicionados ao pool. Esse novo espaço é disponibilizado para todos os sistemas de arquivos. O ZFS também possui várias propriedades diferentes que podem ser aplicadas a cada sistema de arquivos, oferecendo muitas vantagens para a criação de vários sistemas de arquivos e datasets diferentes, em vez de um único sistema de arquivos monolítico.</p></div></div></div><div class=sect1><h2 id=zfs-quickstart>19.2. Guia de Início Rápido<a class=anchor href=#zfs-quickstart></a></h2><div class=sectionbody><div class=paragraph><p>Existe um mecanismo de inicialização que permite ao FreeBSD montar pools do ZFS durante a inicialização do sistema. Para habilitá-lo, adicione esta linha ao <span class=filename>/etc/rc.conf</span>:</p></div><div class="literalblock programlisting"><div class=content><pre>zfs_enable=&#34;YES&#34;</pre></div></div><div class=paragraph><p>Então inicie o serviço:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># service zfs start</span></code></pre></div></div><div class=paragraph><p>Os exemplos nesta seção assumem três discos SCSI com os seguintes nomes de dispositivo <span class=filename>da0</span>, <span class=filename>da1</span> e <span class=filename>da2</span>. Usuários de hardware do tipo SATA devem usar nomes de dispositivo <span class=filename>ada</span>.</p></div><div class=sect2><h3 id=zfs-quickstart-single-disk-pool>19.2.1. Pool de Disco Único<a class=anchor href=#zfs-quickstart-single-disk-pool></a></h3><div class=paragraph><p>Para criar um pool simples e não-redundante usando um único disco:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool create example /dev/da0</span></code></pre></div></div><div class=paragraph><p>Para visualizar o novo pool, verifique a saída do comando <code>df</code>:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># df</span>
Filesystem  1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a   2026030  235230  1628718    13%    /
devfs               1       1        0   100%    /dev
/dev/ad0s1d  54098308 1032846 48737598     2%    /usr
example      17547136       0 17547136     0%    /example</code></pre></div></div><div class=paragraph><p>Esta saída mostra que o pool <code>example</code> foi criado e montado e agora está acessível como um sistema de arquivos. Arquivos podem ser criados nele e os usuários podem navegar nele:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># cd /example</span>
<span class=c># ls</span>
<span class=c># touch testfile</span>
<span class=c># ls -al</span>
total 4
drwxr-xr-x   2 root  wheel    3 Aug 29 23:15 <span class=nb>.</span>
drwxr-xr-x  21 root  wheel  512 Aug 29 23:12 ..
<span class=nt>-rw-r--r--</span>   1 root  wheel    0 Aug 29 23:15 testfile</code></pre></div></div><div class=paragraph><p>No entanto, esse pool não está aproveitando nenhuma feature do ZFS. Para criar um dataset neste pool com a compressão ativada:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs create example/compressed</span>
<span class=c># zfs set compression=gzip example/compressed</span></code></pre></div></div><div class=paragraph><p>O dataset <code>example/compressed</code> é agora um sistema de arquivos ZFS compactado. Tente copiar alguns arquivos grandes para <span class=filename>/example/compressed</span>.</p></div><div class=paragraph><p>A compactação pode ser desativada com:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set compression=off example/compressed</span></code></pre></div></div><div class=paragraph><p>Para desmontar um sistema de arquivos, use <code>zfs umount</code> e, em seguida, verifique com <code>df</code>:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs umount example/compressed</span>
<span class=c># df</span>
Filesystem  1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a   2026030  235232  1628716    13%    /
devfs               1       1        0   100%    /dev
/dev/ad0s1d  54098308 1032864 48737580     2%    /usr
example      17547008       0 17547008     0%    /example</code></pre></div></div><div class=paragraph><p>Para remontar o sistema de arquivos para torná-lo acessível novamente, use <code>zfs mount</code> e verifique com o <code>df</code>:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs mount example/compressed</span>
<span class=c># df</span>
Filesystem         1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a          2026030  235234  1628714    13%    /
devfs                      1       1        0   100%    /dev
/dev/ad0s1d         54098308 1032864 48737580     2%    /usr
example             17547008       0 17547008     0%    /example
example/compressed  17547008       0 17547008     0%    /example/compressed</code></pre></div></div><div class=paragraph><p>O pool e o sistema de arquivos também podem ser observados visualizando a saída do comando <code>mount</code>:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># mount</span>
/dev/ad0s1a on / <span class=o>(</span>ufs, <span class=nb>local</span><span class=o>)</span>
devfs on /dev <span class=o>(</span>devfs, <span class=nb>local</span><span class=o>)</span>
/dev/ad0s1d on /usr <span class=o>(</span>ufs, <span class=nb>local</span>, soft-updates<span class=o>)</span>
example on /example <span class=o>(</span>zfs, <span class=nb>local</span><span class=o>)</span>
example/compressed on /example/compressed <span class=o>(</span>zfs, <span class=nb>local</span><span class=o>)</span></code></pre></div></div><div class=paragraph><p>Após a criação, os datasets do ZFS podem ser usados como qualquer sistema de arquivos. No entanto, muitos outros recursos estão disponíveis, e podem ser definidos por conjunto de dados. No exemplo abaixo, um novo sistema de arquivos chamado <code>data</code> é criado. Arquivos importantes serão armazenados nele, portanto, ele é configurado para manter duas cópias de cada bloco de dados:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs create example/data</span>
<span class=c># zfs set copies=2 example/data</span></code></pre></div></div><div class=paragraph><p>Agora é possível ver o sistema de arquivos <code>data</code> e o espaço utilizado através do comando <code>df</code>:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># df</span>
Filesystem         1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a          2026030  235234  1628714    13%    /
devfs                      1       1        0   100%    /dev
/dev/ad0s1d         54098308 1032864 48737580     2%    /usr
example             17547008       0 17547008     0%    /example
example/compressed  17547008       0 17547008     0%    /example/compressed
example/data        17547008       0 17547008     0%    /example/data</code></pre></div></div><div class=paragraph><p>Observe que cada sistema de arquivos no pool tem a mesma quantidade de espaço disponível. Esta é a razão para usar o <code>df</code> nestes exemplos, para mostrar que os sistemas de arquivos usam apenas a quantidade de espaço de que precisam e todos utilizam o mesmo pool. O ZFS elimina conceitos como volumes e partições e permite que vários sistemas de arquivos ocupem o mesmo pool.</p></div><div class=paragraph><p>Para destruir os sistemas de arquivos e, em seguida, destruir o pool, se ele não for mais necessário:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs destroy example/compressed</span>
<span class=c># zfs destroy example/data</span>
<span class=c># zpool destroy example</span></code></pre></div></div></div><div class=sect2><h3 id=zfs-quickstart-raid-z>19.2.2. RAID-Z<a class=anchor href=#zfs-quickstart-raid-z></a></h3><div class=paragraph><p>Discos falham. Um método para evitar perda de dados devido a falhas no disco é implementar RAID. O ZFS suporta esse recurso em seu design de pool. Os pools RAID-Z exigem três ou mais discos, mas fornecem mais espaço utilizável do que os pools espelhados.</p></div><div class=paragraph><p>Este exemplo cria um pool RAID-Z, especificando os discos a serem adicionados ao pool:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool create storage raidz da0 da1 da2</span></code></pre></div></div><div class="admonitionblock note"><table><tbody><tr><td class=icon><i class="fa icon-note" title=Note></i></td><td class=content><div class=paragraph><p>A Sun™ recomenda que o número de dispositivos usados em uma configuração RAID-Z seja entre três e nove. Para ambientes que exigem um único conjunto de 10 discos ou mais, considere dividi-lo em grupos menores de RAID-Z. Se apenas dois discos estiverem disponíveis e a redundância for um requisito, considere usar o ZFS mirror. Consulte <a href="https://man.freebsd.org/cgi/man.cgi?query=zpool&amp;sektion=8&amp;format=html">zpool(8)</a> para obter maiores detalhes.</p></div></td></tr></tbody></table></div><div class=paragraph><p>O exemplo anterior criou o zpool <code>storage</code>. Este exemplo cria um novo sistema de arquivos chamado <code>home</code> neste pool:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs create storage/home</span></code></pre></div></div><div class=paragraph><p>A compressão e a criação de cópias extras de diretórios e arquivos podem ser ativadas:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set copies=2 storage/home</span>
<span class=c># zfs set compression=gzip storage/home</span></code></pre></div></div><div class=paragraph><p>Para tornar este o novo diretório home para usuários, copie os dados de usuários para este diretório e crie os links simbólicos apropriados:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># cp -rp /home/* /storage/home</span>
<span class=c># rm -rf /home /usr/home</span>
<span class=c># ln -s /storage/home /home</span>
<span class=c># ln -s /storage/home /usr/home</span></code></pre></div></div><div class=paragraph><p>Os dados dos usuários agora são armazenados no recém-criado diretório <span class=filename>/storage/home</span>. Teste adicionando um novo usuário e efetuando login como este usuário.</p></div><div class=paragraph><p>Tente criar um snapshot do sistema de arquivos que possa ser revertido posteriormente:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs snapshot storage/home@08-30-08</span></code></pre></div></div><div class=paragraph><p>Os snapshots só podem ser realizados de um sistema de arquivos completo, não de um único diretório ou arquivo.</p></div><div class=paragraph><p>O caractere <code>@</code> é um delimitador entre o nome do sistema de arquivos ou o nome do volume. Se um diretório importante tiver sido excluído acidentalmente, o backup do sistema de arquivos poderá ser feito e, em seguida, revertido para um snapshot anterior, quando o diretório ainda existia:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs rollback storage/home@08-30-08</span></code></pre></div></div><div class=paragraph><p>Para listar todos os snapshots disponíveis, execute <code>ls</code> no diretório <span class=filename>.zfs/snapshot</span> no sistema de arquivos. Por exemplo, para ver o snapshot obtido anteriormente:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># ls /storage/home/.zfs/snapshot</span></code></pre></div></div><div class=paragraph><p>É possível escrever um script para criar snapshots frequentes dos dados do usuário. No entanto, com o tempo, os snapshots podem consumir muito espaço em disco. O snapshot anterior pode ser removido usando o comando:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs destroy storage/home@08-30-08</span></code></pre></div></div><div class=paragraph><p>Após o teste, <span class=filename>/storage/home</span> pode ser o verdadeiro <span class=filename>/home</span> usando este comando:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set mountpoint=/home storage/home</span></code></pre></div></div><div class=paragraph><p>Execute o <code>df</code> e o <code>mount</code> para confirmar que o sistema agora trata o sistema de arquivos como o real <span class=filename>/home</span>:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># mount</span>
/dev/ad0s1a on / <span class=o>(</span>ufs, <span class=nb>local</span><span class=o>)</span>
devfs on /dev <span class=o>(</span>devfs, <span class=nb>local</span><span class=o>)</span>
/dev/ad0s1d on /usr <span class=o>(</span>ufs, <span class=nb>local</span>, soft-updates<span class=o>)</span>
storage on /storage <span class=o>(</span>zfs, <span class=nb>local</span><span class=o>)</span>
storage/home on /home <span class=o>(</span>zfs, <span class=nb>local</span><span class=o>)</span>
<span class=c># df</span>
Filesystem   1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a    2026030  235240  1628708    13%    /
devfs                1       1        0   100%    /dev
/dev/ad0s1d   54098308 1032826 48737618     2%    /usr
storage       26320512       0 26320512     0%    /storage
storage/home  26320512       0 26320512     0%    /home</code></pre></div></div><div class=paragraph><p>Isso conclui a configuração do RAID-Z. Atualizações de status diárias sobre os sistemas de arquivos criados podem ser geradas como parte das execuções noturnas do<a href="https://man.freebsd.org/cgi/man.cgi?query=periodic&amp;sektion=8&amp;format=html">periodic(8)</a>. Adicione esta linha ao <span class=filename>/etc/periodic.conf</span>:</p></div><div class="literalblock programlisting"><div class=content><pre>daily_status_zfs_enable=&#34;YES&#34;</pre></div></div></div><div class=sect2><h3 id=zfs-quickstart-recovering-raid-z>19.2.3. Recuperando o RAID-Z<a class=anchor href=#zfs-quickstart-recovering-raid-z></a></h3><div class=paragraph><p>Todo software RAID tem um método de monitorar seu <code>status</code>. O status dos dispositivos RAID-Z pode ser visualizado com este comando:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool status -x</span></code></pre></div></div><div class=paragraph><p>Se todos os pools estiverem <a href=#zfs-term-online>Online</a> e tudo estiver normal, a mensagem mostrará:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell>all pools are healthy</code></pre></div></div><div class=paragraph><p>Se houver um problema, talvez um disco que esteja no estado <a href=#zfs-term-offline>Offline</a>, o status do pool será semelhante a:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell>  pool: storage
 state: DEGRADED
status: One or more devices has been taken offline by the administrator.
	Sufficient replicas exist <span class=k>for </span>the pool to <span class=k>continue </span>functioning <span class=k>in </span>a
	degraded state.
action: Online the device using <span class=s1>&#39;zpool online&#39;</span> or replace the device with
	<span class=s1>&#39;zpool replace&#39;</span><span class=nb>.</span>
 scrub: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	storage     DEGRADED     0     0     0
	  raidz1    DEGRADED     0     0     0
	    da0     ONLINE       0     0     0
	    da1     OFFLINE      0     0     0
	    da2     ONLINE       0     0     0

errors: No known data errors</code></pre></div></div><div class=paragraph><p>Isso indica que o dispositivo foi colocado off-line anteriormente pelo administrador com este comando:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool offline storage da1</span></code></pre></div></div><div class=paragraph><p>Agora o sistema pode ser desligado para substituir o <span class=filename>da1</span>. Quando o sistema estiver novamente online, o disco com falha poderá ser substituído no pool:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool replace storage da1</span></code></pre></div></div><div class=paragraph><p>Agora, o status pode ser verificado novamente, desta vez sem <code>-x</code>, para que todos os pools sejam mostrados:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool status storage</span>
 pool: storage
 state: ONLINE
 scrub: resilver completed with 0 errors on Sat Aug 30 19:44:11 2008
config:

	NAME        STATE     READ WRITE CKSUM
	storage     ONLINE       0     0     0
	  raidz1    ONLINE       0     0     0
	    da0     ONLINE       0     0     0
	    da1     ONLINE       0     0     0
	    da2     ONLINE       0     0     0

errors: No known data errors</code></pre></div></div><div class=paragraph><p>Neste exemplo, tudo está normal.</p></div></div><div class=sect2><h3 id=zfs-quickstart-data-verification>19.2.4. Verificação de dados<a class=anchor href=#zfs-quickstart-data-verification></a></h3><div class=paragraph><p>O ZFS utiliza checksums para verificar a integridade dos dados armazenados. Estes são ativados automaticamente na criação dos sistemas de arquivos.</p></div><div class="admonitionblock warning"><table><tbody><tr><td class=icon><i class="fa icon-warning" title=Warning></i></td><td class=content><div class=paragraph><p>Os checksums podem ser desabilitados, mas isto <em>não</em> é recomendado! Os checksums ocupam muito pouco espaço de armazenamento e fornecem integridade dos dados. Muitos recursos do ZFS não funcionarão adequadamente com os checksums desabilitados. Não há nenhum ganho perceptível de desempenho ao desativar os checksums.</p></div></td></tr></tbody></table></div><div class=paragraph><p>A verificação de checksum é conhecida como <em>scrubbing</em>. Verifique a integridade dos dados do pool <code>storage</code> com este comando:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool scrub storage</span></code></pre></div></div><div class=paragraph><p>A duração de um scrub depende da quantidade de dados armazenados. Quantidades maiores de dados levarão proporcionalmente mais tempo para serem verificadas. Scrubs utilizam muito I/O, e apenas um scrub tem permissão para ser executado por vez. Após a conclusão do scrub, o status pode ser visualizado com <code>status</code>:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool status storage</span>
 pool: storage
 state: ONLINE
 scrub: scrub completed with 0 errors on Sat Jan 26 19:57:37 2013
config:

	NAME        STATE     READ WRITE CKSUM
	storage     ONLINE       0     0     0
	  raidz1    ONLINE       0     0     0
	    da0     ONLINE       0     0     0
	    da1     ONLINE       0     0     0
	    da2     ONLINE       0     0     0

errors: No known data errors</code></pre></div></div><div class=paragraph><p>A data de conclusão da última operação de scrub é exibida para ajudar a rastrear quando outro scrub é necessário. Uma rotina recorrente de scrubs ajuda a proteger os dados contra corrupção silenciosa e garante a integridade do pool.</p></div><div class=paragraph><p>Consulte <a href="https://man.freebsd.org/cgi/man.cgi?query=zfs&amp;sektion=8&amp;format=html">zfs(8)</a> e <a href="https://man.freebsd.org/cgi/man.cgi?query=zpool&amp;sektion=8&amp;format=html">zpool(8)</a> para outras opções do ZFS.</p></div></div></div></div><div class=sect1><h2 id=zfs-zpool>19.3. Administração <code>zpool</code><a class=anchor href=#zfs-zpool></a></h2><div class=sectionbody><div class=paragraph><p>A administração do ZFS é dividida entre dois utilitários principais. O utilitário <code>zpool</code> controla a operação do pool e trata da adição, remoção, substituição e gerenciamento de discos. O utilitário <a href=#zfs-zfs><code>zfs</code></a> lida com a criação, destruição e gerenciamento de datasets, tanto para <a href=#zfs-term-filesystem>sistemas de arquivos</a> quanto para <a href=#zfs-term-volume>volumes</a>.</p></div><div class=sect2><h3 id=zfs-zpool-create>19.3.1. Criando e destruindo pools de armazenamento<a class=anchor href=#zfs-zpool-create></a></h3><div class=paragraph><p>A criação de um pool de armazenamento do ZFS (<em>zpool</em>) envolve a tomada de várias decisões que são relativamente permanentes porque a estrutura do pool não pode ser alterada depois que o pool é criado. A decisão mais importante é quais tipos de vdevs usar para agrupar os discos físicos. Consulte a lista de <a href=#zfs-term-vdev>tipos vdev</a> para obter detalhes sobre as opções possíveis. Após o pool ter sido criado, a maioria dos tipos de vdev não permite que discos adicionais sejam adicionados ao vdev. As exceções são os mirrors, que permitem que discos adicionais sejam adicionados ao vdev, e stripes, que podem ser atualizados para mirrors ao anexar um disco adicional ao vdev. Embora vdevs adicionais possam ser adicionados para expandir um pool, o layout do pool não pode ser alterado após a criação do pool. Em vez disso, os dados devem ser salvos em um backup e o pool destruído e recriado.</p></div><div class=paragraph><p>Crie um pool do tipo mirror simples:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool create mypool mirror /dev/ada1 /dev/ada2</span>
<span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada1    ONLINE       0     0     0
            ada2    ONLINE       0     0     0

errors: No known data errors</code></pre></div></div><div class=paragraph><p>Vários vdevs podem ser criados de uma só vez. Especifique vários grupos de discos separados pela palavra-chave do tipo vdev, <code>mirror</code> neste exemplo:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool create mypool mirror /dev/ada1 /dev/ada2 mirror /dev/ada3 /dev/ada4</span>
<span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada1    ONLINE       0     0     0
            ada2    ONLINE       0     0     0
          mirror-1  ONLINE       0     0     0
            ada3    ONLINE       0     0     0
            ada4    ONLINE       0     0     0

errors: No known data errors</code></pre></div></div><div class=paragraph><p>Os pools também podem ser construídos usando partições em vez de discos inteiros. Colocar o ZFS em uma partição separada permite que o mesmo disco tenha outras partições para outras finalidades. Em particular, partições com bootcode e sistemas de arquivos necessários para a inicialização podem ser adicionadas. Isso permite inicializar a partir de discos que também são membros de um pool. Não há penalidade de desempenho no FreeBSD ao usar uma partição em vez de um disco inteiro. O uso de partições também permite ao administrador <em>sub-provisionar</em> os discos, usando menos que a capacidade total. Se um disco de substituição futuro com o mesmo tamanho nominal do original tiver uma capacidade ligeiramente menor, a partição menor ainda se ajustará e o disco de substituição ainda poderá ser usado.</p></div><div class=paragraph><p>Crie um pool <a href=#zfs-term-vdev-raidz>RAID-Z2</a> usando partições:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool create mypool raidz2 /dev/ada0p3 /dev/ada1p3 /dev/ada2p3 /dev/ada3p3 /dev/ada4p3 /dev/ada5p3</span>
<span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          raidz2-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0
            ada3p3  ONLINE       0     0     0
            ada4p3  ONLINE       0     0     0
            ada5p3  ONLINE       0     0     0

errors: No known data errors</code></pre></div></div><div class=paragraph><p>Um pool que não é mais necessário pode ser destruído para que os discos possam ser reutilizados. Destruir um pool envolve primeiro desmontar todos os datasets nesse pool. Se os datasets estiverem em uso, a operação de desmontagem falhará e o pool não será destruído. A destruição do pool pode ser forçada com <code>-f</code>, mas isso pode causar um comportamento indefinido em aplicações que tiverem arquivos abertos nesses datasets.</p></div></div><div class=sect2><h3 id=zfs-zpool-attach>19.3.2. Adicionando e Removendo Dispositivos<a class=anchor href=#zfs-zpool-attach></a></h3><div class=paragraph><p>Existem dois casos para adicionar discos a um zpool: anexar um disco a um vdev existente com <code>zpool attach</code> ou incluir vdevs ao pool com <code>zpool add</code>. Apenas alguns <a href=#zfs-term-vdev>vdev types</a> permitem que discos sejam adicionados ao vdev após a criação.</p></div><div class=paragraph><p>Um pool criado com um único disco não tem redundância. Dados corrompidos podem ser detectados, mas não reparados, porque não há outra cópia dos dados. A propriedade <a href=#zfs-term-copies>copies</a> pode ser capaz de se recuperar de uma pequena falha, como um setor defeituoso, mas não fornece o mesmo nível de proteção que o mirror ou o RAID-Z. Começando com um pool de um único disco vdev, o <code>zpool attach</code> pode ser usado para adicionar um disco adicional ao vdev, criando um mirror. O <code>zpool attach</code> também pode ser usado para adicionar discos adicionais a um mirror group, aumentando a redundância e o desempenho de leitura. Se os discos usados para o pool forem particionados, replicar o layout do primeiro disco para o segundo, <code>gpart backup</code> e <code>gpart restore</code> pode ser usado para facilitar esse processo .</p></div><div class=paragraph><p>Atualize o disco único (stripe) vdev <em>ada0p3</em> para um mirror anexando <em>ada1p3</em>:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          ada0p3    ONLINE       0     0     0

errors: No known data errors
<span class=c># zpool attach mypool ada0p3 ada1p3</span>
Make sure to <span class=nb>wait </span><span class=k>until </span>resilver is <span class=k>done </span>before rebooting.

If you boot from pool <span class=s1>&#39;mypool&#39;</span>, you may need to update
boot code on newly attached disk <span class=s1>&#39;ada1p3&#39;</span><span class=nb>.</span>

Assuming you use GPT partitioning and <span class=s1>&#39;da0&#39;</span> is your new boot disk
you may use the following <span class=nb>command</span>:

        gpart bootcode <span class=nt>-b</span> /boot/pmbr <span class=nt>-p</span> /boot/gptzfsboot <span class=nt>-i</span> 1 da0
<span class=c># gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 ada1</span>
bootcode written to ada1
<span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
status: One or more devices is currently being resilvered.  The pool will
        <span class=k>continue </span>to <span class=k>function</span>, possibly <span class=k>in </span>a degraded state.
action: Wait <span class=k>for </span>the resilver to complete.
  scan: resilver <span class=k>in </span>progress since Fri May 30 08:19:19 2014
        527M scanned out of 781M at 47.9M/s, 0h0m to go
        527M resilvered, 67.53% <span class=k>done
</span>config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0  <span class=o>(</span>resilvering<span class=o>)</span>

errors: No known data errors
<span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: resilvered 781M <span class=k>in </span>0h0m with 0 errors on Fri May 30 08:15:58 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0

errors: No known data errors</code></pre></div></div><div class=paragraph><p>Quando adicionar discos ao vdev existente não é uma opção, como para RAID-Z, um método alternativo é adicionar outro vdev ao pool. Vdevs adicionais fornecem desempenho mais alto, distribuindo as operações de escrita nos vdevs. Cada vdev é responsável por fornecer a sua própria redundância. É possível, mas desencorajado, misturar tipos de vdev, como <code>mirror</code> e <code>RAID-Z</code>. Adicionar um vdev não-redundante a um pool que contenha um vdev mirror ou o RAID-Z arrisca os dados em todo o pool. As gravações são distribuídas, portanto, a falha do disco não-redundante resultará na perda de uma fração de cada bloco que foi gravado no pool.</p></div><div class=paragraph><p>Os dados são distribuídos em cada um dos vdevs. Por exemplo, com dois vdevs mirror, esse é efetivamente um RAID 10 que escreve em dois conjuntos de mirrors. O espaço é alocado de forma que cada vdev chegue a 100% de uso ao mesmo tempo. Há uma penalidade de desempenho se os vdevs tiverem quantidades diferentes de espaço livre, pois uma quantidade desproporcional dos dados é gravada no vdev menos cheio.</p></div><div class=paragraph><p>Ao anexar dispositivos adicionais a um pool de inicialização, lembre-se de atualizar o bootcode.</p></div><div class=paragraph><p>Anexe um segundo grupo de mirror’s (<span class=filename>ada2p3</span> and <span class=filename>ada3p3</span>) ao mirror existente:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: resilvered 781M <span class=k>in </span>0h0m with 0 errors on Fri May 30 08:19:35 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0

errors: No known data errors
<span class=c># zpool add mypool mirror ada2p3 ada3p3</span>
<span class=c># gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 ada2</span>
bootcode written to ada2
<span class=c># gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 ada3</span>
bootcode written to ada3
<span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: scrub repaired 0 <span class=k>in </span>0h0m with 0 errors on Fri May 30 08:29:51 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0
          mirror-1  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0
            ada3p3  ONLINE       0     0     0

errors: No known data errors</code></pre></div></div><div class=paragraph><p>Atualmente, os vdevs não podem ser removidos de um pool e os discos só podem ser removidos de um mirror se houver redundância restante suficiente. Se apenas um disco em um grupo de mirror’s permanecer, ele deixará de ser um mirror e voltará a ser um srtipe, arriscando todo o pool se o disco restante falhar.</p></div><div class=paragraph><p>Remova um disco de um grupo de mirror’s triplo:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: scrub repaired 0 <span class=k>in </span>0h0m with 0 errors on Fri May 30 08:29:51 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0

errors: No known data errors
<span class=c># zpool detach mypool ada2p3</span>
<span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: scrub repaired 0 <span class=k>in </span>0h0m with 0 errors on Fri May 30 08:29:51 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0

errors: No known data errors</code></pre></div></div></div><div class=sect2><h3 id=zfs-zpool-status>19.3.3. Verificando o status de um pool<a class=anchor href=#zfs-zpool-status></a></h3><div class=paragraph><p>O status do pool é importante. Se uma unidade ficar off-line ou for detectado um erro de leitura, gravação ou de checksum, a contagem de erros correspondente aumentará. A saída <code>status</code> mostra a configuração e o status de cada dispositivo no pool e o status de todo o pool. Ações que precisam ser tomadas e detalhes sobre o último <a href=#zfs-zpool-scrub><code>scrub</code></a> também são mostrados.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: scrub repaired 0 <span class=k>in </span>2h25m with 0 errors on Sat Sep 14 04:25:50 2013
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          raidz2-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0
            ada3p3  ONLINE       0     0     0
            ada4p3  ONLINE       0     0     0
            ada5p3  ONLINE       0     0     0

errors: No known data errors</code></pre></div></div></div><div class=sect2><h3 id=zfs-zpool-clear>19.3.4. Limpando Erros<a class=anchor href=#zfs-zpool-clear></a></h3><div class=paragraph><p>Quando um erro é detectado, os contadores de leitura, escrita ou checksum são incrementados. A mensagem de erro pode ser apagada e os contadores resetados com <code>zpool clear <em>mypool</em></code>. Limpar o estado de erro pode ser importante para scripts automatizados que alertam o administrador quando o pool encontra um erro. Erros adicionais podem não ser relatados se os erros antigos não forem apagados.</p></div></div><div class=sect2><h3 id=zfs-zpool-replace>19.3.5. Substituindo um dispositivo em funcionamento<a class=anchor href=#zfs-zpool-replace></a></h3><div class=paragraph><p>Há várias situações em que pode ser desejável substituir um disco por um disco diferente. Ao substituir um disco em funcionamento, o processo mantém o disco antigo online durante a substituição. O pool nunca entra no estado <a href=#zfs-term-degraded>degradado </a>, reduzindo o risco de perda de dados. <code>zpool replace</code> copia todos os dados do disco antigo para o novo. Após a conclusão da operação, o disco antigo é desconectado do vdev. Se o novo disco for maior que o disco antigo, pode ser possível aumentar o zpool usando o novo espaço. Veja <a href=#zfs-zpool-online>Aumentando um Pool </a>.</p></div><div class=paragraph><p>Substitua um dispositivo em funcionamento no pool:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0

errors: No known data errors
<span class=c># zpool replace mypool ada1p3 ada2p3</span>
Make sure to <span class=nb>wait </span><span class=k>until </span>resilver is <span class=k>done </span>before rebooting.

If you boot from pool <span class=s1>&#39;zroot&#39;</span>, you may need to update
boot code on newly attached disk <span class=s1>&#39;ada2p3&#39;</span><span class=nb>.</span>

Assuming you use GPT partitioning and <span class=s1>&#39;da0&#39;</span> is your new boot disk
you may use the following <span class=nb>command</span>:

        gpart bootcode <span class=nt>-b</span> /boot/pmbr <span class=nt>-p</span> /boot/gptzfsboot <span class=nt>-i</span> 1 da0
<span class=c># gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 ada2</span>
<span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
status: One or more devices is currently being resilvered.  The pool will
        <span class=k>continue </span>to <span class=k>function</span>, possibly <span class=k>in </span>a degraded state.
action: Wait <span class=k>for </span>the resilver to complete.
  scan: resilver <span class=k>in </span>progress since Mon Jun  2 14:21:35 2014
        604M scanned out of 781M at 46.5M/s, 0h0m to go
        604M resilvered, 77.39% <span class=k>done
</span>config:

        NAME             STATE     READ WRITE CKSUM
        mypool           ONLINE       0     0     0
          mirror-0       ONLINE       0     0     0
            ada0p3       ONLINE       0     0     0
            replacing-1  ONLINE       0     0     0
              ada1p3     ONLINE       0     0     0
              ada2p3     ONLINE       0     0     0  <span class=o>(</span>resilvering<span class=o>)</span>

errors: No known data errors
<span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: resilvered 781M <span class=k>in </span>0h0m with 0 errors on Mon Jun  2 14:21:52 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0

errors: No known data errors</code></pre></div></div></div><div class=sect2><h3 id=zfs-zpool-resilver>19.3.6. Lidando com dispositivos com falha<a class=anchor href=#zfs-zpool-resilver></a></h3><div class=paragraph><p>Quando um disco em um pool falha, o vdev ao qual o disco pertence entra no estado <a href=#zfs-term-degraded>degradado</a>. Todos os dados ainda estão disponíveis, mas o desempenho pode ser reduzido porque os dados ausentes devem ser calculados a partir da redundância disponível. Para restaurar o vdev para um estado totalmente funcional, o dispositivo físico com falha deve ser substituído. O ZFS é então instruído a iniciar a operação <a href=#zfs-term-resilver>resilver</a>. Os dados que estavam no dispositivo com falha são recalculados da redundância disponível e gravados no dispositivo de substituição. Após a conclusão, o vdev retorna ao status <a href=#zfs-term-online>online</a>.</p></div><div class=paragraph><p>Se o vdev não tiver redundância, ou se vários dispositivos falharem e não houver redundância suficiente para compensar, o pool entrará no estado <a href=#zfs-term-faulted>failed</a>. Se um número suficiente de dispositivos não puder ser reconectado ao pool, o pool se tornará inoperante e os dados deverão ser restaurados dos backups.</p></div><div class=paragraph><p>Ao substituir um disco com falha, o nome do disco com falha é substituído pelo GUID do dispositivo. Um novo parâmetro de nome de dispositivo para o <code>zpool replace</code> não é necessário se o dispositivo de substituição tiver o mesmo nome de dispositivo.</p></div><div class=paragraph><p>Substitua um disco com falha usando o <code>zpool replace</code>:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool status</span>
  pool: mypool
 state: DEGRADED
status: One or more devices could not be opened.  Sufficient replicas exist <span class=k>for
        </span>the pool to <span class=k>continue </span>functioning <span class=k>in </span>a degraded state.
action: Attach the missing device and online it using <span class=s1>&#39;zpool online&#39;</span><span class=nb>.</span>
   see: http://illumos.org/msg/ZFS-8000-2Q
  scan: none requested
config:

        NAME                    STATE     READ WRITE CKSUM
        mypool                  DEGRADED     0     0     0
          mirror-0              DEGRADED     0     0     0
            ada0p3              ONLINE       0     0     0
            316502962686821739  UNAVAIL      0     0     0  was /dev/ada1p3

errors: No known data errors
<span class=c># zpool replace mypool 316502962686821739 ada2p3</span>
<span class=c># zpool status</span>
  pool: mypool
 state: DEGRADED
status: One or more devices is currently being resilvered.  The pool will
        <span class=k>continue </span>to <span class=k>function</span>, possibly <span class=k>in </span>a degraded state.
action: Wait <span class=k>for </span>the resilver to complete.
  scan: resilver <span class=k>in </span>progress since Mon Jun  2 14:52:21 2014
        641M scanned out of 781M at 49.3M/s, 0h0m to go
        640M resilvered, 82.04% <span class=k>done
</span>config:

        NAME                        STATE     READ WRITE CKSUM
        mypool                      DEGRADED     0     0     0
          mirror-0                  DEGRADED     0     0     0
            ada0p3                  ONLINE       0     0     0
            replacing-1             UNAVAIL      0     0     0
              15732067398082357289  UNAVAIL      0     0     0  was /dev/ada1p3/old
              ada2p3                ONLINE       0     0     0  <span class=o>(</span>resilvering<span class=o>)</span>

errors: No known data errors
<span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: resilvered 781M <span class=k>in </span>0h0m with 0 errors on Mon Jun  2 14:52:38 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0

errors: No known data errors</code></pre></div></div></div><div class=sect2><h3 id=zfs-zpool-scrub>19.3.7. Limpeza do Pool<a class=anchor href=#zfs-zpool-scrub></a></h3><div class=paragraph><p>Recomenda-se que os pools sejam regularmente <a href=#zfs-term-scrub>scrubbed</a>, idealmente pelo menos uma vez por mês. A operação <code>scrub</code> requer muito disco e reduzirá o desempenho durante a execução. Evite períodos de alta demanda ao agendar o <code>scrub</code> ou use <a href=#zfs-advanced-tuning-scrub_delay><code>vfs.zfs.scrub_delay</code></a> para ajustar a prioridade relativa do <code>scrub</code> para evitar que ele interfira com outras cargas de trabalho.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool scrub mypool</span>
<span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: scrub <span class=k>in </span>progress since Wed Feb 19 20:52:54 2014
        116G scanned out of 8.60T at 649M/s, 3h48m to go
        0 repaired, 1.32% <span class=k>done
</span>config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          raidz2-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0
            ada3p3  ONLINE       0     0     0
            ada4p3  ONLINE       0     0     0
            ada5p3  ONLINE       0     0     0

errors: No known data errors</code></pre></div></div><div class=paragraph><p>No caso de uma operação de limpeza precisar ser cancelada, emita <code>zpool scrub -s <em>mypool</em></code>.</p></div></div><div class=sect2><h3 id=zfs-zpool-selfheal>19.3.8. Auto Cura (Self-Healing)<a class=anchor href=#zfs-zpool-selfheal></a></h3><div class=paragraph><p>Os checksums armazenados com os blocos de dados habilitam o sistema de arquivos a se <em>autocorrigirem</em>. Esse recurso reparará automaticamente os dados cujo checksum não corresponde à registrada em outro dispositivo que faz parte do pool de armazenamento. Por exemplo, um espelho com dois discos em que uma unidade está começando a funcionar incorretamente e não pode armazenar os dados adequadamente. Isso é ainda pior quando os dados não são acessados há muito tempo, como no armazenamento de arquivos de longo prazo. Os sistemas de arquivos tradicionais precisam executar algoritmos que verificam e reparam os dados como o <a href="https://man.freebsd.org/cgi/man.cgi?query=fsck&amp;sektion=8&amp;format=html">fsck(8)</a>. Esses comandos levam tempo e, em casos graves, um administrador precisa decidir manualmente qual operação de reparo deve ser executada. Quando o ZFS detecta um bloco de dados com um checksum que não corresponde, ele tenta ler os dados do disco de espelhamento. Se esse disco puder fornecer os dados corretos, ele não apenas fornecerá esses dados ao aplicativo que os está solicitando, mas também corrigirá os dados errados no disco que continha o checksum incorreto. Isso acontece sem qualquer interação de um administrador do sistema durante a operação normal do pool.</p></div><div class=paragraph><p>O próximo exemplo demonstra esse comportamento de autocura. Um conjunto espelhado de discos <span class=filename>/dev/ada0</span> e <span class=filename>/dev/ada1</span> é criado.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool create healer mirror /dev/ada0 /dev/ada1</span>
<span class=c># zpool status healer</span>
  pool: healer
 state: ONLINE
  scan: none requested
config:

    NAME        STATE     READ WRITE CKSUM
    healer      ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
       ada0     ONLINE       0     0     0
       ada1     ONLINE       0     0     0

errors: No known data errors
<span class=c># zpool list</span>
NAME     SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG   CAP  DEDUP  HEALTH  ALTROOT
healer   960M  92.5K   960M         -         -     0%    0%  1.00x  ONLINE  -</code></pre></div></div><div class=paragraph><p>Alguns dados importantes que devem ser protegidos de erros de dados usando o recurso de correção automática são copiados para o pool. É criado um checksum do pool para comparação posterior.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># cp /some/important/data /healer</span>
<span class=c># zfs list</span>
NAME     SIZE  ALLOC   FREE    CAP  DEDUP  HEALTH  ALTROOT
healer   960M  67.7M   892M     7%  1.00x  ONLINE  -
<span class=c># sha1 /healer &gt; checksum.txt</span>
<span class=c># cat checksum.txt</span>
SHA1 <span class=o>(</span>/healer<span class=o>)</span> <span class=o>=</span> 2753eff56d77d9a536ece6694bf0a82740344d1f</code></pre></div></div><div class=paragraph><p>A corrupção de dados é simulada escrevendo dados aleatórios no início de um dos discos no espelho. Para evitar que o ZFS cure os dados assim que forem detectados, o pool é exportado antes da corrupção e importado novamente depois.</p></div><div class="admonitionblock warning"><table><tbody><tr><td class=icon><i class="fa icon-warning" title=Warning></i></td><td class=content><div class=paragraph><p>Esta é uma operação perigosa que pode destruir dados vitais. Ele é mostrado aqui apenas para fins demonstrativos e não deve ser tentado durante a operação normal de um pool de armazenamento. Nem este exemplo de corrupção intencional deve ser executado em qualquer disco com um sistema de arquivos diferente. Não use outros nomes de dispositivos de disco diferentes daqueles que fazem parte do pool. Certifique-se de que os backups apropriados do pool sejam criados antes de executar o comando!</p></div></td></tr></tbody></table></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool export healer</span>
<span class=c># dd if=/dev/random of=/dev/ada1 bs=1m count=200</span>
200+0 records <span class=k>in
</span>200+0 records out
209715200 bytes transferred <span class=k>in </span>62.992162 secs <span class=o>(</span>3329227 bytes/sec<span class=o>)</span>
<span class=c># zpool import healer</span></code></pre></div></div><div class=paragraph><p>O status do pool mostra que um dispositivo teve um erro. Observe que os aplicativos que leem dados do pool não receberam dados incorretos. O ZFS forneceu dados do dispositivo <span class=filename>ada0</span> com os checksums corretos. O dispositivo com o checksum incorreto pode ser encontrado facilmente, pois a coluna <code>CKSUM</code> contém um valor diferente de zero.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool status healer</span>
    pool: healer
   state: ONLINE
  status: One or more devices has experienced an unrecoverable error.  An
          attempt was made to correct the error.  Applications are unaffected.
  action: Determine <span class=k>if </span>the device needs to be replaced, and clear the errors
          using <span class=s1>&#39;zpool clear&#39;</span> or replace the device with <span class=s1>&#39;zpool replace&#39;</span><span class=nb>.</span>
     see: http://illumos.org/msg/ZFS-8000-4J
    scan: none requested
  config:

      NAME        STATE     READ WRITE CKSUM
      healer      ONLINE       0     0     0
        mirror-0  ONLINE       0     0     0
         ada0     ONLINE       0     0     0
         ada1     ONLINE       0     0     1

errors: No known data errors</code></pre></div></div><div class=paragraph><p>O erro foi detectado e tratado usando a redundância presente no disco de espelhamento <span class=filename>ada0</span> não afetado. Uma comparação de checksum com o original irá revelar se o pool está consistente novamente.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># sha1 /healer &gt;&gt; checksum.txt</span>
<span class=c># cat checksum.txt</span>
SHA1 <span class=o>(</span>/healer<span class=o>)</span> <span class=o>=</span> 2753eff56d77d9a536ece6694bf0a82740344d1f
SHA1 <span class=o>(</span>/healer<span class=o>)</span> <span class=o>=</span> 2753eff56d77d9a536ece6694bf0a82740344d1f</code></pre></div></div><div class=paragraph><p>Os dois checksums que foram gerados antes e depois da adulteração intencional dos dados do conjunto ainda correspondem. Isso mostra como o ZFS é capaz de detectar e corrigir erros automaticamente quando os checksums são diferentes. Observe que isso só é possível quando há redundância suficiente presente no pool. Um pool que consiste em um único dispositivo não possui recursos de autocorreção. Essa também é a razão pela qual os cheksuma são tão importantes no ZFS e não devem ser desabilitados por nenhum motivo. Nenhum <a href="https://man.freebsd.org/cgi/man.cgi?query=fsck&amp;sektion=8&amp;format=html">fsck(8)</a> ou programa semelhante de verificação de consistência do sistema de arquivos é necessário para detectar e corrigir isso e o pool ainda estava disponível durante o problema. Uma operação de scrub agora é necessária para sobrescrever os dados corrompidos em <span class=filename>ada1</span>.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool scrub healer</span>
<span class=c># zpool status healer</span>
  pool: healer
 state: ONLINE
status: One or more devices has experienced an unrecoverable error.  An
            attempt was made to correct the error.  Applications are unaffected.
action: Determine <span class=k>if </span>the device needs to be replaced, and clear the errors
            using <span class=s1>&#39;zpool clear&#39;</span> or replace the device with <span class=s1>&#39;zpool replace&#39;</span><span class=nb>.</span>
   see: http://illumos.org/msg/ZFS-8000-4J
  scan: scrub <span class=k>in </span>progress since Mon Dec 10 12:23:30 2012
        10.4M scanned out of 67.0M at 267K/s, 0h3m to go
        9.63M repaired, 15.56% <span class=k>done
</span>config:

    NAME        STATE     READ WRITE CKSUM
    healer      ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
       ada0     ONLINE       0     0     0
       ada1     ONLINE       0     0   627  <span class=o>(</span>repairing<span class=o>)</span>

errors: No known data errors</code></pre></div></div><div class=paragraph><p>A operação scrub lê os dados do <span class=filename>ada0</span> e reescreve todos os dados com um checksum incorreto no <span class=filename>ada1</span>. Isso é indicado pela saída <code>(repairing)</code> do <code>zpool status</code>. Após a conclusão da operação, o status do conjunto é alterado para:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool status healer</span>
  pool: healer
 state: ONLINE
status: One or more devices has experienced an unrecoverable error.  An
        attempt was made to correct the error.  Applications are unaffected.
action: Determine <span class=k>if </span>the device needs to be replaced, and clear the errors
             using <span class=s1>&#39;zpool clear&#39;</span> or replace the device with <span class=s1>&#39;zpool replace&#39;</span><span class=nb>.</span>
   see: http://illumos.org/msg/ZFS-8000-4J
  scan: scrub repaired 66.5M <span class=k>in </span>0h2m with 0 errors on Mon Dec 10 12:26:25 2012
config:

    NAME        STATE     READ WRITE CKSUM
    healer      ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
       ada0     ONLINE       0     0     0
       ada1     ONLINE       0     0 2.72K

errors: No known data errors</code></pre></div></div><div class=paragraph><p>Após a conclusão da operação scrub e todos os dados terem sido sincronizados de <span class=filename>ada0</span> para <span class=filename>ada1</span>, as mensagens de erro podem ser <a href=#zfs-zpool-clear>Limpando Erros</a> do status do pool executando <code>zpool clear</code>.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool clear healer</span>
<span class=c># zpool status healer</span>
  pool: healer
 state: ONLINE
  scan: scrub repaired 66.5M <span class=k>in </span>0h2m with 0 errors on Mon Dec 10 12:26:25 2012
config:

    NAME        STATE     READ WRITE CKSUM
    healer      ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
       ada0     ONLINE       0     0     0
       ada1     ONLINE       0     0     0

errors: No known data errors</code></pre></div></div><div class=paragraph><p>O pool está agora de volta a um estado totalmente funcional e todos os erros foram apagados.</p></div></div><div class=sect2><h3 id=zfs-zpool-online>19.3.9. Crescendo um Pool<a class=anchor href=#zfs-zpool-online></a></h3><div class=paragraph><p>O tamanho utilizável de um pool redundante é limitado pela capacidade do menor dispositivo em cada vdev. O menor dispositivo pode ser substituído por um dispositivo maior. Depois de concluir uma operação <a href=#zfs-zpool-replace>replace</a> ou <a href=#zfs-term-resilver>resilver</a>, o pool pode crescer para usar a capacidade do Novo dispositivo. Por exemplo, considere um espelho de uma unidade de 1 TB e uma unidade de 2 TB. O espaço utilizável é de 1 TB. Quando a unidade de 1 TB é substituída por outra unidade de 2 TB, o processo de resilverização copia os dados existentes para a nova unidade. Como os dois dispositivos agora têm capacidade para 2 TB, o espaço disponível do espelho pode ser aumentado para 2 TB.</p></div><div class=paragraph><p>A expansão é acionada usando o <code>zpool online -e</code> em cada dispositivo. Após a expansão de todos os dispositivos, o espaço adicional fica disponível para o pool.</p></div></div><div class=sect2><h3 id=zfs-zpool-import>19.3.10. Importando e exportando pools<a class=anchor href=#zfs-zpool-import></a></h3><div class=paragraph><p>Os pools são <em>exportados</em> antes de serem movidos para outro sistema. Todos os conjuntos de dados são desmontados e cada dispositivo é marcado como exportado, mas ainda estarão bloqueados, para que não possam ser usados por outros subsistemas de disco. Isso permite que pools sejam <em>importados</em> em outras máquinas, outros sistemas operacionais que suportem ZFS , e até mesmo arquiteturas de hardware diferentes (com algumas advertências, veja <a href="https://man.freebsd.org/cgi/man.cgi?query=zpool&amp;sektion=8&amp;format=html">zpool(8)</a>). Quando um conjunto de dados tem arquivos abertos, o <code>zpool export -f</code> pode ser usado para forçar a exportação de um pool. Use isso com cautela. Os conjuntos de dados são forçosamente desmontados, resultando potencialmente em um comportamento inesperado dos aplicativos que tinham arquivos abertos nesses conjuntos de dados.</p></div><div class=paragraph><p>Exportar um pool que não está em uso:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool export mypool</span></code></pre></div></div><div class=paragraph><p>Importar um pool automaticamente monta os conjuntos de dados. Este pode não ser o comportamento desejado e pode ser evitado com <code>zpool import -N</code>. O <code>zpool import -o</code> define propriedades temporárias apenas para esta importação. O <code>zpool import altroot=</code> permite importar um pool com um ponto base de montagem em vez da raiz do sistema de arquivos. Se o pool foi usado pela última vez em um sistema diferente e não foi exportado corretamente, uma importação pode ter que ser forçada com <code>zpool import -f</code>. O <code>zpool import -a</code> importa todos os pools que não parecem estar em uso por outro sistema.</p></div><div class=paragraph><p>Listar todos os pools disponíveis para importação:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool import</span>
   pool: mypool
     <span class=nb>id</span>: 9930174748043525076
  state: ONLINE
 action: The pool can be imported using its name or numeric identifier.
 config:

        mypool      ONLINE
          ada2p3    ONLINE</code></pre></div></div><div class=paragraph><p>Importe o pool com um diretório raiz alternativo:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool import -o altroot=/mnt mypool</span>
<span class=c># zfs list</span>
zfs list
NAME                 USED  AVAIL  REFER  MOUNTPOINT
mypool               110K  47.0G    31K  /mnt/mypool</code></pre></div></div></div><div class=sect2><h3 id=zfs-zpool-upgrade>19.3.11. Atualizando um pool de armazenamento<a class=anchor href=#zfs-zpool-upgrade></a></h3><div class=paragraph><p>Após a atualização do FreeBSD, ou se um pool foi importado de um sistema usando uma versão mais antiga do ZFS, o pool pode ser atualizado manualmente para a versão mais recente do ZFS para suportar as funcionalidades mais recentes. Considere se o pool pode precisar ser importado em um sistema antigo antes de atualizar. A atualização é um processo unidirecional. Os pools mais antigos podem ser atualizados, mas os pools com funcionalidades mais recentes não podem ser desatualizados.</p></div><div class=paragraph><p>Atualize um pool v28 para suportar <code>Feature Flags</code>:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
status: The pool is formatted using a legacy on-disk format.  The pool can
        still be used, but some features are unavailable.
action: Upgrade the pool using <span class=s1>&#39;zpool upgrade&#39;</span><span class=nb>.</span>  Once this is <span class=k>done</span>, the
        pool will no longer be accessible on software that does not support feat
        flags.
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
	    ada0    ONLINE       0     0     0
	    ada1    ONLINE       0     0     0

errors: No known data errors
<span class=c># zpool upgrade</span>
This system supports ZFS pool feature flags.

The following pools are formatted with legacy version numbers and can
be upgraded to use feature flags.  After being upgraded, these pools
will no longer be accessible by software that does not support feature
flags.

VER  POOL
<span class=nt>---</span>  <span class=nt>------------</span>
28   mypool

Use <span class=s1>&#39;zpool upgrade -v&#39;</span> <span class=k>for </span>a list of available legacy versions.
Every feature flags pool has all supported features enabled.
<span class=c># zpool upgrade mypool</span>
This system supports ZFS pool feature flags.

Successfully upgraded <span class=s1>&#39;mypool&#39;</span> from version 28 to feature flags.
Enabled the following features on <span class=s1>&#39;mypool&#39;</span>:
  async_destroy
  empty_bpobj
  lz4_compress
  multi_vdev_crash_dump</code></pre></div></div><div class=paragraph><p>Os recursos mais recentes do ZFS não estarão disponíveis até que o <code>zpool upgrade</code> seja concluído. O <code>zpool upgrade -v</code> pode ser usado para ver quais os novos recursos que serão fornecidos pela atualização, bem como quais recursos já são suportados.</p></div><div class=paragraph><p>Atualize um pool para suportar feature flags adicionais:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
status: Some supported features are not enabled on the pool. The pool can
        still be used, but some features are unavailable.
action: Enable all features using <span class=s1>&#39;zpool upgrade&#39;</span><span class=nb>.</span> Once this is <span class=k>done</span>,
        the pool may no longer be accessible by software that does not support
        the features. See zpool-features<span class=o>(</span>7<span class=o>)</span> <span class=k>for </span>details.
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
	    ada0    ONLINE       0     0     0
	    ada1    ONLINE       0     0     0

errors: No known data errors
<span class=c># zpool upgrade</span>
This system supports ZFS pool feature flags.

All pools are formatted using feature flags.

Some supported features are not enabled on the following pools. Once a
feature is enabled the pool may become incompatible with software
that does not support the feature. See zpool-features<span class=o>(</span>7<span class=o>)</span> <span class=k>for </span>details.

POOL  FEATURE
<span class=nt>---------------</span>
zstore
      multi_vdev_crash_dump
      spacemap_histogram
      enabled_txg
      hole_birth
      extensible_dataset
      bookmarks
      filesystem_limits
<span class=c># zpool upgrade mypool</span>
This system supports ZFS pool feature flags.

Enabled the following features on <span class=s1>&#39;mypool&#39;</span>:
  spacemap_histogram
  enabled_txg
  hole_birth
  extensible_dataset
  bookmarks
  filesystem_limits</code></pre></div></div><div class="admonitionblock warning"><table><tbody><tr><td class=icon><i class="fa icon-warning" title=Warning></i></td><td class=content><div class=paragraph><p>O boot code em sistemas que inicializam a partir de um pool deve ser atualizado para suportar a nova versão do pool. Use <code>gpart bootcode</code> na partição que contém o boot code. Existem dois tipos de bootcode disponíveis, dependendo da forma como o sistema inicializa: GPT (a opção mais comum) e EFI (para sistemas mais modernos).</p></div><div class=paragraph><p>Para inicialização legada usando o GPT, use o seguinte comando:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 ada1</span></code></pre></div></div><div class=paragraph><p>Para sistemas que usam o EFI para inicializar, execute o seguinte comando:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># gpart bootcode -p /boot/boot1.efifat -i 1 ada1</span></code></pre></div></div><div class=paragraph><p>Aplique o bootcode a todos os discos inicializáveis no pool. Veja <a href="https://man.freebsd.org/cgi/man.cgi?query=gpart&amp;sektion=8&amp;format=html">gpart(8)</a> para obter maiores informações.</p></div></td></tr></tbody></table></div></div><div class=sect2><h3 id=zfs-zpool-history>19.3.12. Exibindo o histórico gravado do pool<a class=anchor href=#zfs-zpool-history></a></h3><div class=paragraph><p>Comandos que modificam o pool são registrados. As ações registradas incluem a criação de conjuntos de dados, a alteração de propriedades ou a substituição de um disco. Esse histórico é útil para revisar como um pool foi criado e qual usuário executou uma ação específica e quando. O histórico não é mantido em um arquivo de log, mas faz parte do próprio pool. O comando para revisar este histórico é apropriadamente chamado de <code>zpool history</code>:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool history</span>
History <span class=k>for</span> <span class=s1>&#39;tank&#39;</span>:
2013-02-26.23:02:35 zpool create tank mirror /dev/ada0 /dev/ada1
2013-02-27.18:50:58 zfs <span class=nb>set </span><span class=nv>atime</span><span class=o>=</span>off tank
2013-02-27.18:51:09 zfs <span class=nb>set </span><span class=nv>checksum</span><span class=o>=</span>fletcher4 tank
2013-02-27.18:51:18 zfs create tank/backup</code></pre></div></div><div class=paragraph><p>A saída mostra os comandos <code>zpool</code> e <code>zfs</code> que foram executados no pool juntamente com um registro de data e hora. Somente comandos que alteram o pool de alguma forma são registrados. Comandos como <code>zfs list</code> não estão incluídos. Quando nenhum nome de pool é especificado, é exibido o histórico de todos os pools.</p></div><div class=paragraph><p>O <code>zpool history</code> pode mostrar ainda mais informações quando as opções <code>-i</code> ou <code>-l</code> são fornecidas. A opção <code>-i</code> exibe eventos iniciados pelo usuário, bem como eventos do ZFS registrados internamente.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool history -i</span>
History <span class=k>for</span> <span class=s1>&#39;tank&#39;</span>:
2013-02-26.23:02:35 <span class=o>[</span>internal pool create txg:5] pool spa 28<span class=p>;</span> zfs spa 28<span class=p>;</span> zpl 5<span class=p>;</span>uts  9.1-RELEASE 901000 amd64
2013-02-27.18:50:53 <span class=o>[</span>internal property <span class=nb>set </span>txg:50] <span class=nv>atime</span><span class=o>=</span>0 dataset <span class=o>=</span> 21
2013-02-27.18:50:58 zfs <span class=nb>set </span><span class=nv>atime</span><span class=o>=</span>off tank
2013-02-27.18:51:04 <span class=o>[</span>internal property <span class=nb>set </span>txg:53] <span class=nv>checksum</span><span class=o>=</span>7 dataset <span class=o>=</span> 21
2013-02-27.18:51:09 zfs <span class=nb>set </span><span class=nv>checksum</span><span class=o>=</span>fletcher4 tank
2013-02-27.18:51:13 <span class=o>[</span>internal create txg:55] dataset <span class=o>=</span> 39
2013-02-27.18:51:18 zfs create tank/backup</code></pre></div></div><div class=paragraph><p>Mais detalhes podem ser mostrados adicionando a opção <code>-l</code>. Os registros de histórico são mostrados em um formato longo, incluindo informações como o nome do usuário que emitiu o comando e o nome do host no qual a alteração foi feita.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool history -l</span>
History <span class=k>for</span> <span class=s1>&#39;tank&#39;</span>:
2013-02-26.23:02:35 zpool create tank mirror /dev/ada0 /dev/ada1 <span class=o>[</span>user 0 <span class=o>(</span>root<span class=o>)</span> on :global]
2013-02-27.18:50:58 zfs <span class=nb>set </span><span class=nv>atime</span><span class=o>=</span>off tank <span class=o>[</span>user 0 <span class=o>(</span>root<span class=o>)</span> on myzfsbox:global]
2013-02-27.18:51:09 zfs <span class=nb>set </span><span class=nv>checksum</span><span class=o>=</span>fletcher4 tank <span class=o>[</span>user 0 <span class=o>(</span>root<span class=o>)</span> on myzfsbox:global]
2013-02-27.18:51:18 zfs create tank/backup <span class=o>[</span>user 0 <span class=o>(</span>root<span class=o>)</span> on myzfsbox:global]</code></pre></div></div><div class=paragraph><p>A saída mostra que o usuário <code>root</code> criou o pool espelhado com os discos <span class=filename>/dev/ada0</span> e <span class=filename>/dev/ada1</span>. O nome do host <code>myzfsbox</code> também é mostrado nos comandos após a criação do pool. A exibição do nome do host se torna importante quando o pool é exportado de um sistema e importado para outro. Os comandos que são emitidos no outro sistema podem claramente ser distinguidos pelo nome do host que é registrado para cada comando.</p></div><div class=paragraph><p>Ambas as opções para o <code>zpool history</code> podem ser combinadas para fornecer as informações mais detalhadas possíveis para qualquer pool. O histórico do pool fornece informações valiosas ao rastrear as ações que foram executadas ou quando é necessária uma saída mais detalhada para a depuração.</p></div></div><div class=sect2><h3 id=zfs-zpool-iostat>19.3.13. Monitoramento de Desempenho<a class=anchor href=#zfs-zpool-iostat></a></h3><div class=paragraph><p>Um sistema de monitoramento integrado pode exibir estatísticas de I/O do pool em tempo real. Ele mostra a quantidade de espaço livre e usado no pool, quantas operações de leitura e gravação estão sendo executadas por segundo e quanto de largura de banda de I/O está sendo utilizada no momento. Por padrão, todos os pools no sistema são monitorados e exibidos. Um nome de pool pode ser fornecido para limitar o monitoramento apenas a esse pool. Um exemplo básico:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool iostat</span>
               capacity     operations    bandwidth
pool        alloc   free   <span class=nb>read  </span>write   <span class=nb>read  </span>write
<span class=nt>----------</span>  <span class=nt>-----</span>  <span class=nt>-----</span>  <span class=nt>-----</span>  <span class=nt>-----</span>  <span class=nt>-----</span>  <span class=nt>-----</span>
data         288G  1.53T      2     11  11.3K  57.1K</code></pre></div></div><div class=paragraph><p>Para monitorar continuamente a atividade de I/O, um número pode ser especificado como o último parâmetro, indicando um intervalo em segundos para aguardar entre as atualizações. A próxima linha de estatística é impressa após cada intervalo. Pressione <span class=keyseq><kbd>Ctrl</kbd>+<kbd>C</kbd></span> para interromper este monitoramento contínuo. Como alternativa, forneça um segundo número na linha de comando após o intervalo para especificar o número total de estatísticas a serem exibidas.</p></div><div class=paragraph><p>Estatísticas mais detalhadas de I/O podem ser exibidas com a opção <code>-v</code>. Cada dispositivo no pool é mostrado com uma linha de estatísticas. Isso é útil para ver quantas operações de leitura e gravação estão sendo executadas em cada dispositivo e pode ajudar a determinar se algum dispositivo individual está reduzindo a velocidade do pool. Este exemplo mostra um pool espelhado com dois dispositivos:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool iostat -v</span>
                            capacity     operations    bandwidth
pool                     alloc   free   <span class=nb>read  </span>write   <span class=nb>read  </span>write
<span class=nt>-----------------------</span>  <span class=nt>-----</span>  <span class=nt>-----</span>  <span class=nt>-----</span>  <span class=nt>-----</span>  <span class=nt>-----</span>  <span class=nt>-----</span>
data                      288G  1.53T      2     12  9.23K  61.5K
  mirror                  288G  1.53T      2     12  9.23K  61.5K
    ada1                     -      -      0      4  5.61K  61.7K
    ada2                     -      -      1      4  5.04K  61.7K
<span class=nt>-----------------------</span>  <span class=nt>-----</span>  <span class=nt>-----</span>  <span class=nt>-----</span>  <span class=nt>-----</span>  <span class=nt>-----</span>  <span class=nt>-----</span></code></pre></div></div></div><div class=sect2><h3 id=zfs-zpool-split>19.3.14. Dividindo um pool de armazenamento<a class=anchor href=#zfs-zpool-split></a></h3><div class=paragraph><p>Um pool que consiste em um ou mais vdevs espelhados pode ser dividido em dois conjuntos. A menos que seja especificado de outra forma, o último membro de cada espelho é desanexado e usado para criar um novo pool contendo os mesmos dados. A operação deve primeiro ser tentada com <code>-n</code>. Os detalhes da operação proposta são exibidos sem que sejam realmente executados. Isso ajuda a confirmar que a operação fará o que o usuário pretende.</p></div></div></div></div><div class=sect1><h2 id=zfs-zfs>19.4. Administração do <code>zfs</code><a class=anchor href=#zfs-zfs></a></h2><div class=sectionbody><div class=paragraph><p>O utilitário <code>zfs</code> é responsável por criar, destruir e gerenciar todos os conjuntos de dados ZFS existentes em um pool. O pool é gerenciado usando o <a href=#zfs-zpool><code>zpool</code></a>.</p></div><div class=sect2><h3 id=zfs-zfs-create>19.4.1. Criando e destruindo conjuntos de dados<a class=anchor href=#zfs-zfs-create></a></h3><div class=paragraph><p>Ao contrário dos discos tradicionais e gerenciadores de volume, o espaço no ZFS_não_ é pré-alocado. Nos sistemas de arquivos tradicionais, depois que todo o espaço é particionado e atribuído, não há como adicionar um sistema de arquivos adicional sem adicionar um novo disco. Com o ZFS, novos sistemas de arquivos podem ser criados a qualquer momento. Cada <a href=#zfs-term-dataset><em>conjunto de dados</em></a> tem propriedades incluindo recursos como compactação, deduplicação, armazenamento em cache e cotas, bem como outras propriedades úteis como somente leitura, diferenciação de maiúsculas e minúsculas , compartilhamento de arquivos de rede e um ponto de montagem. Os conjuntos de dados podem ser aninhados uns dentro dos outros e os conjuntos de dados filhos herdarão propriedades de seus pais. Cada conjunto de dados pode ser administrado, <a href=#zfs-zfs-allow>delegado</a>, <a href=#zfs-zfs-send>replicado</a>, preservado por um <a href=#zfs-zfs-snapshot>snapshot</a>, <a href=#zfs-zfs-jail>preso</a>, e destruído como uma unidade. Há muitas vantagens em criar um conjunto de dados separado para cada tipo ou conjunto de arquivos diferente. A única desvantagem de ter um número extremamente grande de conjuntos de dados é que alguns comandos como <code>zfs list</code> serão mais lentos, e a montagem de centenas ou mesmo milhares de conjuntos de dados pode retardar o processo de inicialização do FreeBSD.</p></div><div class=paragraph><p>Crie um novo conjunto de dados e ative a <a href=#zfs-term-compression-lz4>compactação LZ4</a> nele:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs list</span>
NAME                  USED  AVAIL  REFER  MOUNTPOINT
mypool                781M  93.2G   144K  none
mypool/ROOT           777M  93.2G   144K  none
mypool/ROOT/default   777M  93.2G   777M  /
mypool/tmp            176K  93.2G   176K  /tmp
mypool/usr            616K  93.2G   144K  /usr
mypool/usr/home       184K  93.2G   184K  /usr/home
mypool/usr/ports      144K  93.2G   144K  /usr/ports
mypool/usr/src        144K  93.2G   144K  /usr/src
mypool/var           1.20M  93.2G   608K  /var
mypool/var/crash      148K  93.2G   148K  /var/crash
mypool/var/log        178K  93.2G   178K  /var/log
mypool/var/mail       144K  93.2G   144K  /var/mail
mypool/var/tmp        152K  93.2G   152K  /var/tmp
<span class=c># zfs create -o compress=lz4 mypool/usr/mydataset</span>
<span class=c># zfs list</span>
NAME                   USED  AVAIL  REFER  MOUNTPOINT
mypool                 781M  93.2G   144K  none
mypool/ROOT            777M  93.2G   144K  none
mypool/ROOT/default    777M  93.2G   777M  /
mypool/tmp             176K  93.2G   176K  /tmp
mypool/usr             704K  93.2G   144K  /usr
mypool/usr/home        184K  93.2G   184K  /usr/home
mypool/usr/mydataset  87.5K  93.2G  87.5K  /usr/mydataset
mypool/usr/ports       144K  93.2G   144K  /usr/ports
mypool/usr/src         144K  93.2G   144K  /usr/src
mypool/var            1.20M  93.2G   610K  /var
mypool/var/crash       148K  93.2G   148K  /var/crash
mypool/var/log         178K  93.2G   178K  /var/log
mypool/var/mail        144K  93.2G   144K  /var/mail
mypool/var/tmp         152K  93.2G   152K  /var/tmp</code></pre></div></div><div class=paragraph><p>A destruição de um conjunto de dados é muito mais rápida que a exclusão de todos os arquivos que residem no conjunto de dados, pois não envolve a verificação de todos os arquivos e a atualização de todos os metadados correspondentes.</p></div><div class=paragraph><p>Destrua o conjunto de dados criado anteriormente:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs list</span>
NAME                   USED  AVAIL  REFER  MOUNTPOINT
mypool                 880M  93.1G   144K  none
mypool/ROOT            777M  93.1G   144K  none
mypool/ROOT/default    777M  93.1G   777M  /
mypool/tmp             176K  93.1G   176K  /tmp
mypool/usr             101M  93.1G   144K  /usr
mypool/usr/home        184K  93.1G   184K  /usr/home
mypool/usr/mydataset   100M  93.1G   100M  /usr/mydataset
mypool/usr/ports       144K  93.1G   144K  /usr/ports
mypool/usr/src         144K  93.1G   144K  /usr/src
mypool/var            1.20M  93.1G   610K  /var
mypool/var/crash       148K  93.1G   148K  /var/crash
mypool/var/log         178K  93.1G   178K  /var/log
mypool/var/mail        144K  93.1G   144K  /var/mail
mypool/var/tmp         152K  93.1G   152K  /var/tmp
<span class=c># zfs destroy mypool/usr/mydataset</span>
<span class=c># zfs list</span>
NAME                  USED  AVAIL  REFER  MOUNTPOINT
mypool                781M  93.2G   144K  none
mypool/ROOT           777M  93.2G   144K  none
mypool/ROOT/default   777M  93.2G   777M  /
mypool/tmp            176K  93.2G   176K  /tmp
mypool/usr            616K  93.2G   144K  /usr
mypool/usr/home       184K  93.2G   184K  /usr/home
mypool/usr/ports      144K  93.2G   144K  /usr/ports
mypool/usr/src        144K  93.2G   144K  /usr/src
mypool/var           1.21M  93.2G   612K  /var
mypool/var/crash      148K  93.2G   148K  /var/crash
mypool/var/log        178K  93.2G   178K  /var/log
mypool/var/mail       144K  93.2G   144K  /var/mail
mypool/var/tmp        152K  93.2G   152K  /var/tmp</code></pre></div></div><div class=paragraph><p>Nas versões modernas do ZFS, o <code>zfs destroy</code> é assíncrono, e o espaço livre pode levar vários minutos para aparecer no pool. Use o <code>zpool get freeing <em>poolname</em></code> para ver a propriedade <code>freeing</code>, indicando quantos conjuntos de dados estão tendo seus blocos liberados em segundo plano. Se houver conjuntos de dados filhos, como <a href=#zfs-term-snapshot>snapshots</a> ou outros conjuntos de dados, o pai não poderá ser destruído. Para destruir um conjunto de dados e todos os seus filhos, use <code>-r</code> para destruir recursivamente o conjunto de dados e todos os seus filhos. Use <code>-n -v</code> para listar os conjuntos de dados e snapshots que seriam destruídos por esta operação, mas na verdade não destruirão nada. O espaço que seria recuperado pela destruição dos snapshots também é mostrado.</p></div></div><div class=sect2><h3 id=zfs-zfs-volume>19.4.2. Criando e Destruindo Volumes<a class=anchor href=#zfs-zfs-volume></a></h3><div class=paragraph><p>Um volume é um tipo especial de conjunto de dados. Em vez de ser montado como um sistema de arquivos, ele é exposto como um dispositivo de bloco em <span class=filename>/dev/zvol/poolname/dataset</span>. Isso permite que o volume seja usado para outros sistemas de arquivos, para fazer backup dos discos de uma máquina virtual ou para ser exportado usando protocolos como iSCSI ou HAST.</p></div><div class=paragraph><p>Um volume pode ser formatado com qualquer sistema de arquivos ou usado sem um sistema de arquivos para armazenar dados brutos. Para o usuário, um volume parece ser um disco normal. Colocar sistemas de arquivos comuns nesses <em>zvols</em> fornece recursos que os discos comuns ou sistemas de arquivos normalmente não possuem. Por exemplo, o uso da propriedade de compactação em um volume de 250 MB permite a criação de um sistema de arquivos FAT compactado.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs create -V 250m -o compression=on tank/fat32</span>
<span class=c># zfs list tank</span>
NAME USED AVAIL REFER MOUNTPOINT
tank 258M  670M   31K /tank
<span class=c># newfs_msdos -F32 /dev/zvol/tank/fat32</span>
<span class=c># mount -t msdosfs /dev/zvol/tank/fat32 /mnt</span>
<span class=c># df -h /mnt | grep fat32</span>
Filesystem           Size Used Avail Capacity Mounted on
/dev/zvol/tank/fat32 249M  24k  249M     0%   /mnt
<span class=c># mount | grep fat32</span>
/dev/zvol/tank/fat32 on /mnt <span class=o>(</span>msdosfs, <span class=nb>local</span><span class=o>)</span></code></pre></div></div><div class=paragraph><p>Destruir um volume é o mesmo que destruir um conjunto de dados regular do sistema de arquivos. A operação é quase instantânea, mas pode levar vários minutos para que o espaço livre seja recuperado em segundo plano.</p></div></div><div class=sect2><h3 id=zfs-zfs-rename>19.4.3. Renomeando um Conjunto de Dados<a class=anchor href=#zfs-zfs-rename></a></h3><div class=paragraph><p>O nome de um conjunto de dados pode ser alterado com <code>zfs rename</code>. O pai de um conjunto de dados também pode ser alterado com esse comando. A renomeação de um conjunto de dados para um conjunto de dados pai diferente alterará o valor das propriedades herdadas do conjunto de dados pai. Quando um conjunto de dados é renomeado, ele é desmontado e, em seguida, remontado no novo local (que é herdado do novo conjunto de dados pai). Esse comportamento pode ser evitado com <code>-u</code>.</p></div><div class=paragraph><p>Renomeie um conjunto de dados e mova-o para um conjunto de dados pai diferente:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs list</span>
NAME                   USED  AVAIL  REFER  MOUNTPOINT
mypool                 780M  93.2G   144K  none
mypool/ROOT            777M  93.2G   144K  none
mypool/ROOT/default    777M  93.2G   777M  /
mypool/tmp             176K  93.2G   176K  /tmp
mypool/usr             704K  93.2G   144K  /usr
mypool/usr/home        184K  93.2G   184K  /usr/home
mypool/usr/mydataset  87.5K  93.2G  87.5K  /usr/mydataset
mypool/usr/ports       144K  93.2G   144K  /usr/ports
mypool/usr/src         144K  93.2G   144K  /usr/src
mypool/var            1.21M  93.2G   614K  /var
mypool/var/crash       148K  93.2G   148K  /var/crash
mypool/var/log         178K  93.2G   178K  /var/log
mypool/var/mail        144K  93.2G   144K  /var/mail
mypool/var/tmp         152K  93.2G   152K  /var/tmp
<span class=c># zfs rename mypool/usr/mydataset mypool/var/newname</span>
<span class=c># zfs list</span>
NAME                  USED  AVAIL  REFER  MOUNTPOINT
mypool                780M  93.2G   144K  none
mypool/ROOT           777M  93.2G   144K  none
mypool/ROOT/default   777M  93.2G   777M  /
mypool/tmp            176K  93.2G   176K  /tmp
mypool/usr            616K  93.2G   144K  /usr
mypool/usr/home       184K  93.2G   184K  /usr/home
mypool/usr/ports      144K  93.2G   144K  /usr/ports
mypool/usr/src        144K  93.2G   144K  /usr/src
mypool/var           1.29M  93.2G   614K  /var
mypool/var/crash      148K  93.2G   148K  /var/crash
mypool/var/log        178K  93.2G   178K  /var/log
mypool/var/mail       144K  93.2G   144K  /var/mail
mypool/var/newname   87.5K  93.2G  87.5K  /var/newname
mypool/var/tmp        152K  93.2G   152K  /var/tmp</code></pre></div></div><div class=paragraph><p>Os snapshots também podem ser renomeados dessa maneira. Devido à natureza dos snapshots, eles não podem ser renomeados para um conjunto de dados pai diferente. Para renomear um snapshot recursivo, especifique <code>-r</code> e todos os snapshots com o mesmo nome nos conjuntos de dados filho também serão renomeados.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs list -t snapshot</span>
NAME                                USED  AVAIL  REFER  MOUNTPOINT
mypool/var/newname@first_snapshot      0      -  87.5K  -
<span class=c># zfs rename mypool/var/newname@first_snapshot new_snapshot_name</span>
<span class=c># zfs list -t snapshot</span>
NAME                                   USED  AVAIL  REFER  MOUNTPOINT
mypool/var/newname@new_snapshot_name      0      -  87.5K  -</code></pre></div></div></div><div class=sect2><h3 id=zfs-zfs-set>19.4.4. Configurando Propriedades do Conjunto de Dados<a class=anchor href=#zfs-zfs-set></a></h3><div class=paragraph><p>Cada conjunto de dados do ZFS possui várias propriedades que controlam seu comportamento. A maioria das propriedades é herdada automaticamente do conjunto de dados pai, mas pode ser substituída localmente. Defina uma propriedade em um conjunto de dados com <code>zfs set <em>property</em>=<em>value__dataset</em></code>. A maioria das propriedades tem um conjunto limitado de valores válidos, o <code>zfs get</code> exibirá cada propriedade e valor válido possível. A maioria das propriedades pode ser revertida para seus valores herdados usando <code>zfs inherit</code>.</p></div><div class=paragraph><p>Propriedades definidas pelo usuário também podem ser definidas. Eles se tornam parte da configuração do conjunto de dados e podem ser usados para fornecer informações adicionais sobre o conjunto de dados ou seu conteúdo. Para distinguir essas propriedades personalizadas daquelas fornecidas como parte do ZFS, dois pontos (<code>:</code>) são usados para criar um namespace personalizado para a propriedade.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set custom:costcenter=1234 tank</span>
<span class=c># zfs get custom:costcenter tank</span>
NAME PROPERTY           VALUE SOURCE
tank custom:costcenter  1234  <span class=nb>local</span></code></pre></div></div><div class=paragraph><p>Para remover uma propriedade customizada, use o <code>zfs inherit</code> com <code>-r</code>. Se a propriedade personalizada não estiver definida em nenhum dos conjuntos de dados pai, ela será removida completamente (embora as alterações ainda sejam registradas no histórico do pool).</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell> <span class=c>#   zfs inherit -r  customizado :  costcenter   tanque</span>
 <span class=c>#   zfs  customizado :  costcenter   tank</span>
NAME PROPERTY VALUE SOURCE
tanque personalizado: costcenter - -
 <span class=c>#   zfs obtém todos  tank  | grep  personalizado :  costcenter</span>
 <span class=c>#</span></code></pre></div></div><div class=sect3><h4 id=zfs-zfs-set-share>19.4.4.1. Obtendo e definindo propriedades de compartilhamento<a class=anchor href=#zfs-zfs-set-share></a></h4><div class=paragraph><p>Duas propriedades de conjunto de dados comumente usadas e úteis são as opções de compartilhamento NFS e SMB. Configurar estas define se e como os conjuntos de dados do ZFS podem ser compartilhados na rede. Atualmente, apenas o compartilhamento de configurações via NFS é suportado no FreeBSD. Para obter o status atual de um compartilhamento, insira:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs get sharenfs mypool/usr/home</span>
NAME             PROPERTY  VALUE    SOURCE
mypool/usr/home  sharenfs  on       <span class=nb>local</span>
<span class=c># zfs get sharesmb mypool/usr/home</span>
NAME             PROPERTY  VALUE    SOURCE
mypool/usr/home  sharesmb  off      <span class=nb>local</span></code></pre></div></div><div class=paragraph><p>Para ativar o compartilhamento de um conjunto de dados, insira:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set sharenfs=on mypool/usr/home</span></code></pre></div></div><div class=paragraph><p>Também é possível definir opções adicionais para compartilhar conjuntos de dados por meio do NFS, como <code>-alldirs</code>, <code>-maproot</code> e <code>-network</code>. Para definir opções adicionais para um conjunto de dados compartilhado por meio do NFS, insira:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set sharenfs=&#34;-alldirs,-maproot=root,-network=192.168.1.0/24&#34; mypool/usr/home</span></code></pre></div></div></div></div><div class=sect2><h3 id=zfs-zfs-snapshot>19.4.5. Gerenciando Snapshots<a class=anchor href=#zfs-zfs-snapshot></a></h3><div class=paragraph><p>Os <a href=#zfs-term-snapshot>snapshots</a> são um dos recursos mais poderosos do ZFS. Um snapshot fornece uma cópia point-in-time somente leitura do conjunto de dados. Com Copy-On-Write (COW), os snapshots podem ser criados rapidamente, preservando a versão mais antiga dos dados no disco. Se não houver snapshots, o espaço será recuperado para uso futuro quando os dados forem reconfigurados ou excluídos. Os snapshots preservam o espaço em disco gravando apenas as diferenças entre o conjunto de dados atual e uma versão anterior. Os snapshots são permitidos apenas em conjuntos de dados completos, não em arquivos ou diretórios individuais. Quando um snapshot é criado a partir de um conjunto de dados, tudo contido nele é duplicado. Isso inclui as propriedades do sistema de arquivos, arquivos, diretórios, permissões e assim por diante. Os snapshots não usam espaço adicional quando são criados pela primeira vez, consumindo espaço apenas quando os blocos de referência são alterados. Snapshots recursivos obtidos com <code>-r</code> criam um instantâneo com o mesmo nome no conjunto de dados e em todos os seus filhos, fornecendo um snapshot moment-in-time de todos os sistemas de arquivos no momento. Isso pode ser importante quando um aplicativo possui arquivos em vários conjuntos de dados relacionados ou dependentes um do outro. Sem snapshots, um backup teria cópias dos arquivos de diferentes pontos no tempo.</p></div><div class=paragraph><p>Os snapshots no ZFS fornecem uma variedade de recursos que até mesmo outros sistemas de arquivos com a funcionalidade de snapshots não têm. Um exemplo típico de uso de snapshots é ter uma maneira rápida de fazer backup do estado atual do sistema de arquivos quando uma ação arriscada, como uma instalação de software ou uma atualização do sistema, é executada. Se a ação falhar, o snapshot poderá ser revertido e o sistema terá o mesmo estado de quando o snapshot foi criado. Se a atualização foi bem sucedida, o instantâneo pode ser excluído para liberar espaço. Sem snapshots, uma atualização com falha geralmente requer uma restauração de backup, o que é tedioso, consome tempo e pode exigir tempo de inatividade durante o qual o sistema não pode ser usado. Os snapshots podem ser revertidos rapidamente, mesmo enquanto o sistema está sendo executado em operação normal, com pouco ou nenhum tempo de inatividade. A economia de tempo é enorme com sistemas de armazenamento de vários terabytes e o tempo necessário para copiar os dados a partir do backup. Os snapshots não substituem um backup completo de um pool, mas podem ser usados de maneira rápida e fácil para armazenar uma cópia do conjunto de dados em um momento específico.</p></div><div class=sect3><h4 id=zfs-zfs-snapshot-creation>19.4.5.1. Criando Snapshots<a class=anchor href=#zfs-zfs-snapshot-creation></a></h4><div class=paragraph><p>Os snapshots são criados com <code>zfs snapshot <em>dataset</em>@<em>snapshotname</em></code>. Adicionar a opção <code>-r</code> cria um snapshot recursivamente, com o mesmo nome em todos os conjuntos de dados filho.</p></div><div class=paragraph><p>Crie um Snapshot recursivo de todo o pool:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs list -t all</span>
NAME                                   USED  AVAIL  REFER  MOUNTPOINT
mypool                                 780M  93.2G   144K  none
mypool/ROOT                            777M  93.2G   144K  none
mypool/ROOT/default                    777M  93.2G   777M  /
mypool/tmp                             176K  93.2G   176K  /tmp
mypool/usr                             616K  93.2G   144K  /usr
mypool/usr/home                        184K  93.2G   184K  /usr/home
mypool/usr/ports                       144K  93.2G   144K  /usr/ports
mypool/usr/src                         144K  93.2G   144K  /usr/src
mypool/var                            1.29M  93.2G   616K  /var
mypool/var/crash                       148K  93.2G   148K  /var/crash
mypool/var/log                         178K  93.2G   178K  /var/log
mypool/var/mail                        144K  93.2G   144K  /var/mail
mypool/var/newname                    87.5K  93.2G  87.5K  /var/newname
mypool/var/newname@new_snapshot_name      0      -  87.5K  -
mypool/var/tmp                         152K  93.2G   152K  /var/tmp
<span class=c># zfs snapshot -r mypool@my_recursive_snapshot</span>
<span class=c># zfs list -t snapshot</span>
NAME                                        USED  AVAIL  REFER  MOUNTPOINT
mypool@my_recursive_snapshot                   0      -   144K  -
mypool/ROOT@my_recursive_snapshot              0      -   144K  -
mypool/ROOT/default@my_recursive_snapshot      0      -   777M  -
mypool/tmp@my_recursive_snapshot               0      -   176K  -
mypool/usr@my_recursive_snapshot               0      -   144K  -
mypool/usr/home@my_recursive_snapshot          0      -   184K  -
mypool/usr/ports@my_recursive_snapshot         0      -   144K  -
mypool/usr/src@my_recursive_snapshot           0      -   144K  -
mypool/var@my_recursive_snapshot               0      -   616K  -
mypool/var/crash@my_recursive_snapshot         0      -   148K  -
mypool/var/log@my_recursive_snapshot           0      -   178K  -
mypool/var/mail@my_recursive_snapshot          0      -   144K  -
mypool/var/newname@new_snapshot_name           0      -  87.5K  -
mypool/var/newname@my_recursive_snapshot       0      -  87.5K  -
mypool/var/tmp@my_recursive_snapshot           0      -   152K  -</code></pre></div></div><div class=paragraph><p>Os snapshots não são mostrados por uma operação normal do <code>zfs list</code>. Para listar snapshots , a opção <code>-t snapshot</code> é anexado ao <code>zfs list</code>. A opção <code>-t all</code> exibe os sistemas de arquivos e snapshots.</p></div><div class=paragraph><p>Os snapshots não são montados diretamente, portanto, nenhum caminho é mostrado na coluna <code>MOUNTPOINT</code>. Não há menção ao espaço disponível em disco na coluna <code>AVAIL</code>, já que os snapshots não podem ser gravados após serem criados. Compare o snapshot com o conjunto de dados original a partir do qual foi criado:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs list -rt all mypool/usr/home</span>
NAME                                    USED  AVAIL  REFER  MOUNTPOINT
mypool/usr/home                         184K  93.2G   184K  /usr/home
mypool/usr/home@my_recursive_snapshot      0      -   184K  -</code></pre></div></div><div class=paragraph><p>A exibição do conjunto de dados e dos snapshots juntos revela como os snapshots funcionam no modo <a href=#zfs-term-cow>COW</a>. Eles salvam apenas as alterações (<em>deltas</em>) que foram feitas e não o conteúdo completo do sistema de arquivos novamente. Isso significa que os snapshots ocupam pouco espaço quando poucas alterações são feitas. O uso do espaço pode se tornar ainda mais aparente copiando um arquivo para o conjunto de dados e fazendo um segundo snapshots:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># cp /etc/passwd /var/tmp</span>
<span class=c># zfs snapshot mypool/var/tmp@after_cp</span>
<span class=c># zfs list -rt all mypool/var/tmp</span>
NAME                                   USED  AVAIL  REFER  MOUNTPOINT
mypool/var/tmp                         206K  93.2G   118K  /var/tmp
mypool/var/tmp@my_recursive_snapshot    88K      -   152K  -
mypool/var/tmp@after_cp                   0      -   118K  -</code></pre></div></div><div class=paragraph><p>O segundo snapshot contém apenas as alterações feitas no conjunto de dados após a operação de cópia. Isso resulta numa enorme economia de espaço. Observe que o tamanho do snapshot <em>mypool/var/tmp@my_recursive_snapshot</em> também foi alterado na coluna <code>USED</code> para indicar as alterações entre ela mesma e o snapshot obtido posteriormente.</p></div></div><div class=sect3><h4 id=zfs-zfs-snapshot-diff>19.4.5.2. Comparando Snapshots<a class=anchor href=#zfs-zfs-snapshot-diff></a></h4><div class=paragraph><p>O ZFS fornece um comando interno para comparar as diferenças de conteúdo entre dois snapshots. Isso é útil quando muitos snapshots foram gerados com o passar do tempo e o usuário deseja ver como o sistema de arquivos mudou ao longo do tempo. Por exemplo, o <code>zfs diff</code> permite que um usuário localize o ultimo snapshot que ainda contém um arquivo que foi acidentalmente excluído. Fazer isso para os dois snapshots criados na seção anterior produz essa saída:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs list -rt all mypool/var/tmp</span>
NAME                                   USED  AVAIL  REFER  MOUNTPOINT
mypool/var/tmp                         206K  93.2G   118K  /var/tmp
mypool/var/tmp@my_recursive_snapshot    88K      -   152K  -
mypool/var/tmp@after_cp                   0      -   118K  -
<span class=c># zfs diff mypool/var/tmp@my_recursive_snapshot</span>
M       /var/tmp/
+       /var/tmp/passwd</code></pre></div></div><div class=paragraph><p>O comando lista as alterações entre o snapshot especificado (neste caso <code><em>mypool/var/tmp@my_recursive_snapshot</em></code>) e o sistema de arquivos ativo. A primeira coluna mostra o tipo de mudança:</p></div><table class="tableblock frame-all grid-all stretch informaltable"><col style=width:50%><col style=width:50%><tbody><tr><td class="tableblock halign-left valign-top"><p class=tableblock>+</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>O caminho ou arquivo foi adicionado.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock>-</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>O caminho ou arquivo foi excluído.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock>M</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>O caminho ou arquivo foi modificado.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock>R</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>O caminho ou arquivo foi renomeado.</p></td></tr></tbody></table><div class=paragraph><p>Comparando a saída com a tabela, fica claro que o <span class=filename>passwd</span> foi adicionado após o snapshot <code><em>mypool/var/tmp@my_recursive_snapshot</em></code> ter sido criado. Isso também resultou em uma modificação no diretório pai montado em <code><em>/var/tmp</em></code>.</p></div><div class=paragraph><p>A comparação de dois snapshots é útil ao usar o recurso de replicação do ZFS para transferir um conjunto de dados para um host diferente para fins de backup.</p></div><div class=paragraph><p>Compare dois snapshots fornecendo o nome completo do conjunto de dados e o nome do snapshot de ambos os conjuntos de dados:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># cp /var/tmp/passwd /var/tmp/passwd.copy</span>
<span class=c># zfs snapshot mypool/var/tmp@diff_snapshot</span>
<span class=c># zfs diff mypool/var/tmp@my_recursive_snapshot mypool/var/tmp@diff_snapshot</span>
M       /var/tmp/
+       /var/tmp/passwd
+       /var/tmp/passwd.copy
<span class=c># zfs diff mypool/var/tmp@my_recursive_snapshot mypool/var/tmp@after_cp</span>
M       /var/tmp/
+       /var/tmp/passwd</code></pre></div></div><div class=paragraph><p>Um administrador de backup pode comparar dois snapshots recebidos do host de envio e determinar as alterações reais no conjunto de dados. Consulte a seção <a href=#zfs-zfs-send>Replicação</a> para obter maiores informações.</p></div></div><div class=sect3><h4 id=zfs-zfs-snapshot-rollback>19.4.5.3. Reversão de um Snapshot<a class=anchor href=#zfs-zfs-snapshot-rollback></a></h4><div class=paragraph><p>Quando pelo menos um snapshot estiver disponível, ele poderá ser revertido a qualquer momento. Na maioria das vezes, esse é o caso quando o estado atual do conjunto de dados não é mais necessário e uma versão mais antiga é preferida. Cenários em que testes de desenvolvimento local deram errado, atualizações de sistemas com falhas que dificultam o funcionamento geral do sistema ou a necessidade de restaurar arquivos ou diretórios excluídos acidentalmente são ocorrências muito comuns. Felizmente, reverter um snapshot é tão fácil quanto digitar <code>zfs rollback <em>snapshotname</em></code>. Dependendo de quantas alterações estão envolvidas, a operação será concluída em um determinado período de tempo. Durante esse período, o conjunto de dados permanece sempre em um estado consistente, da mesma forma que um banco de dados em conformidade com os princípios do ACID ao realizar uma reversão. Isso está acontecendo enquanto o conjunto de dados está ativo e acessível, sem exigir um tempo de inatividade. Depois que o snapshot for revertido, o conjunto de dados terá o mesmo estado de quando o snapshot foi originalmente criado. Todos os outros dados nesse conjunto de dados que não faziam parte do snapshot são descartados. Criar um snapshot do estado atual do conjunto de dados antes de reverter para um anterior é uma boa ideia quando alguns dos dados são necessários mais tarde. Desta forma, o usuário pode alternar entre os snapshots sem perder dados que ainda são valiosos.</p></div><div class=paragraph><p>No primeiro exemplo, um snapshot é revertido por causa de uma operação descuidada com o comando <code>rm</code> que removeu muito mais dados do que o pretendido.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs list -rt all mypool/var/tmp</span>
NAME                                   USED  AVAIL  REFER  MOUNTPOINT
mypool/var/tmp                         262K  93.2G   120K  /var/tmp
mypool/var/tmp@my_recursive_snapshot    88K      -   152K  -
mypool/var/tmp@after_cp               53.5K      -   118K  -
mypool/var/tmp@diff_snapshot              0      -   120K  -
<span class=c># ls /var/tmp</span>
passwd          passwd.copy     vi.recover
<span class=c># rm /var/tmp/passwd*</span>
<span class=c># ls /var/tmp</span>
vi.recover</code></pre></div></div><div class=paragraph><p>Neste ponto, o usuário percebeu que muitos arquivos foram excluídos e os quer de volta. O ZFS fornece uma maneira fácil de recuperá-los usando reversões, mas somente quando os snapshots de dados importantes são executados regularmente. Para recuperar os arquivos e recomeçar a partir do último snapshot, emita o comando:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs rollback mypool/var/tmp@diff_snapshot</span>
<span class=c># ls /var/tmp</span>
passwd          passwd.copy     vi.recover</code></pre></div></div><div class=paragraph><p>A operação de reversão restaurou o conjunto de dados para o estado do último snapshot. Também é possível reverter para um snapshot que foi gerado muito antes e que possui outros snapshots criados após ele. Ao tentar fazer isso, o ZFS irá emitir este aviso:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs list -rt snapshot mypool/var/tmp</span>
AME                                   USED  AVAIL  REFER  MOUNTPOINT
mypool/var/tmp@my_recursive_snapshot    88K      -   152K  -
mypool/var/tmp@after_cp               53.5K      -   118K  -
mypool/var/tmp@diff_snapshot              0      -   120K  -
<span class=c># zfs rollback mypool/var/tmp@my_recursive_snapshot</span>
cannot rollback to <span class=s1>&#39;mypool/var/tmp@my_recursive_snapshot&#39;</span>: more recent snapshots exist
use <span class=s1>&#39;-r&#39;</span> to force deletion of the following snapshots:
mypool/var/tmp@after_cp
mypool/var/tmp@diff_snapshot</code></pre></div></div><div class=paragraph><p>Esse aviso significa que existem snapshots entre o estado atual do conjunto de dados e o snapshot para o qual o usuário deseja retroceder. Para concluir a reversão, esses snapshots devem ser excluídos. O ZFS não pode rastrear todas as alterações entre estados diferentes do conjunto de dados, porque os snapshots são somente de leitura. O ZFS não excluirá os snapshots afetados, a menos que o usuário especifique a opção <code>-r</code> para indicar que essa é a ação desejada. Se essa for a intenção e as consequências da perda de todos os snapshots intermediários forem compreendidas, o comando poderá ser emitido:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs rollback -r mypool/var/tmp@my_recursive_snapshot</span>
<span class=c># zfs list -rt snapshot mypool/var/tmp</span>
NAME                                   USED  AVAIL  REFER  MOUNTPOINT
mypool/var/tmp@my_recursive_snapshot     8K      -   152K  -
<span class=c># ls /var/tmp</span>
vi.recover</code></pre></div></div><div class=paragraph><p>A saída de <code>zfs list -t snapshot</code> confirma que os snapshots intermediários foram removidos como resultado do <code>zfs rollback -r</code>.</p></div></div><div class=sect3><h4 id=zfs-zfs-snapshot-snapdir>19.4.5.4. Restaurando arquivos individuais a partir de Snapshots<a class=anchor href=#zfs-zfs-snapshot-snapdir></a></h4><div class=paragraph><p>Os snapshots são montados em um diretório oculto no conjunto de dados pai: <span class=filename>.zfs/snapshots/snapshotname</span>. Por padrão, esses diretórios não serão exibidos mesmo quando um <code>ls -a</code> padrão for executado. Embora o diretório não seja exibido, ele está lá e pode ser acessado como qualquer diretório normal. A propriedade denominada <code>snapdir</code> controla se esses diretórios ocultos aparecem em uma listagem de diretórios. Definir a propriedade como <code>visible</code> permite que eles apareçam na saída do <code>ls</code> e de outros comandos que lidam com o conteúdo do diretório.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs get snapdir mypool/var/tmp</span>
NAME            PROPERTY  VALUE    SOURCE
mypool/var/tmp  snapdir   hidden   default
<span class=c># ls -a /var/tmp</span>
<span class=nb>.</span>               ..              passwd          vi.recover
<span class=c># zfs set snapdir=visible mypool/var/tmp</span>
<span class=c># ls -a /var/tmp</span>
<span class=nb>.</span>               ..              .zfs            passwd          vi.recover</code></pre></div></div><div class=paragraph><p>Arquivos individuais podem ser facilmente restaurados para um estado anterior, copiando-os do snapshot de volta para o conjunto de dados pai. A estrutura de diretórios abaixo de <span class=filename>.zfs/snapshot</span> tem um diretório nomeado exatamente como os instantâneos criados anteriormente para facilitar sua identificação. No próximo exemplo, presume-se que um arquivo deve ser restaurado a partir do diretório <span class=filename>.zfs</span> oculto, copiando-o do snapshot que continha a versão mais recente do arquivo:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># rm /var/tmp/passwd</span>
<span class=c># ls -a /var/tmp</span>
<span class=nb>.</span>               ..              .zfs            vi.recover
<span class=c># ls /var/tmp/.zfs/snapshot</span>
after_cp                my_recursive_snapshot
<span class=c># ls /var/tmp/.zfs/snapshot/after_cp</span>
passwd          vi.recover
<span class=c># cp /var/tmp/.zfs/snapshot/after_cp/passwd /var/tmp</span></code></pre></div></div><div class=paragraph><p>Quando o comando <code>ls .zfs/snapshot</code> foi emitido, a propriedade <code>snapdir</code> pode ter sido definida como oculta, mas ainda seria possível listar o conteúdo desse diretório. Cabe ao administrador decidir se esses diretórios serão exibidos. É possível exibi-los para determinados conjuntos de dados e impedi-los para outros. Copiar arquivos ou diretórios deste diretório <span class=filename>.zfs/snapshot</span> oculto é bastante simples. Tentar o contrário, resulta neste erro:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># cp /etc/rc.conf /var/tmp/.zfs/snapshot/after_cp/</span>
<span class=nb>cp</span>: /var/tmp/.zfs/snapshot/after_cp/rc.conf: Read-only file system</code></pre></div></div><div class=paragraph><p>O erro lembra ao usuário que os snapshots são somente de leitura e não podem ser alterados após a criação. Os arquivos não podem ser copiados para ou removidos dos diretórios de snapshot porque isso alteraria o estado do conjunto de dados que eles representam.</p></div><div class=paragraph><p>Os snapshots consomem espaço com base em quanto o sistema de arquivos pai foi alterado desde o momento da criação do snapshot. A propriedade <code>written</code> de um snapshot rastreia quanto espaço está sendo usado pelo snapshot.</p></div><div class=paragraph><p>Snapshots são destruídos e o espaço recuperado com o <code>zfs destroy <em>dataset</em>@<em>snapshot</em></code>. Adicionar <code>-r</code> remove recursivamente todos os snapshots com o mesmo nome sob o conjunto de dados pai. Adicionar <code>-n -v</code> ao comando exibe uma lista dos snapshots que seriam excluídos e uma estimativa de quanto espaço seria recuperado sem executar a operação de destruição real.</p></div></div></div><div class=sect2><h3 id=zfs-zfs-clones>19.4.6. Gerenciando Clones<a class=anchor href=#zfs-zfs-clones></a></h3><div class=paragraph><p>Um clone é uma cópia de um snapshot que é tratado mais como um conjunto de dados regular. Ao contrário de um snapshot, um clone não é somente de leitura, ele pode ser montado e pode ter suas próprias propriedades. Uma vez que um clone tenha sido criado usando <code>zfs clone</code>, o snapshot do qual ele foi criado não pode ser destruído. O relacionamento filho/pai entre o clone e o snapshot pode ser revertido usando <code>zfs promote</code>. Depois que um clone é promovido, o snapshot se torna um filho do clone, em vez de filho do conjunto de dados pai original. Isso mudará a maneira como o espaço é contabilizado, mas não mudará a quantidade de espaço consumida. O clone pode ser montado em qualquer ponto dentro da hierarquia do sistema de arquivos ZFS, não apenas abaixo do local original do snapshot.</p></div><div class=paragraph><p>Para demonstrar o recurso de clonagem, este conjunto de dados de exemplo é usado:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs list -rt all camino/home/joe</span>
NAME                    USED  AVAIL  REFER  MOUNTPOINT
camino/home/joe         108K   1.3G    87K  /usr/home/joe
camino/home/joe@plans    21K      -  85.5K  -
camino/home/joe@backup    0K      -    87K  -</code></pre></div></div><div class=paragraph><p>Um uso típico de clones é experimentar um conjunto de dados específico, mantendo o snapshot em volta, para o caso de algo dar errado. Como os snapshots não podem ser alterados, um clone de leitura/gravação de um snapshot é criado. Depois que o resultado desejado é alcançado no clone, o clone pode ser promovido para se tornar um conjunto de dados e o sistema de arquivos antigo é removido. Isso não é estritamente necessário, pois o clone e o conjunto de dados podem coexistir sem problemas.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs clone camino/home/joe@backup camino/home/joenew</span>
<span class=c># ls /usr/home/joe*</span>
/usr/home/joe:
backup.txz     plans.txt

/usr/home/joenew:
backup.txz     plans.txt
<span class=c># df -h /usr/home</span>
Filesystem          Size    Used   Avail Capacity  Mounted on
usr/home/joe        1.3G     31k    1.3G     0%    /usr/home/joe
usr/home/joenew     1.3G     31k    1.3G     0%    /usr/home/joenew</code></pre></div></div><div class=paragraph><p>Depois que um clone é criado, ele é uma cópia exata do estado em que o conjunto de dados estava quando o snapshot foi criado. O clone agora pode ser alterado independentemente de seu conjunto de dados de origem. A única conexão entre os dois é o snapshot. O ZFS registra essa conexão na propriedade <code>origin</code>. Uma vez que a dependência entre o snapshot e o clone foi removida promovendo-se o clone usando <code>zfs promote</code>, a <code>origem</code> do clone é removida, pois agora ele é um conjunto de dados independente. Este exemplo demonstra isso:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs get origin camino/home/joenew</span>
NAME                  PROPERTY  VALUE                     SOURCE
camino/home/joenew    origin    camino/home/joe@backup    -
<span class=c># zfs promote camino/home/joenew</span>
<span class=c># zfs get origin camino/home/joenew</span>
NAME                  PROPERTY  VALUE   SOURCE
camino/home/joenew    origin    -       -</code></pre></div></div><div class=paragraph><p>Depois de fazer algumas alterações, como copiar o <span class=filename>loader.conf</span> para o clone promovido, por exemplo, o diretório antigo torna-se obsoleto nesse caso. Em vez disso, o clone promovido pode substituí-lo. Isso pode ser conseguido por dois comandos consecutivos: <code>zfs destroy</code> no dataset antigo e <code>zfs rename</code> no clone para nomeá-lo como o conjunto de dados antigo (ele também poderia ter um nome totalmente diferente).</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># cp /boot/defaults/loader.conf /usr/home/joenew</span>
<span class=c># zfs destroy -f camino/home/joe</span>
<span class=c># zfs rename camino/home/joenew camino/home/joe</span>
<span class=c># ls /usr/home/joe</span>
backup.txz     loader.conf     plans.txt
<span class=c># df -h /usr/home</span>
Filesystem          Size    Used   Avail Capacity  Mounted on
usr/home/joe        1.3G    128k    1.3G     0%    /usr/home/joe</code></pre></div></div><div class=paragraph><p>O snapshot clonado agora é tratado como um conjunto de dados comum. Ele contém todos os dados do snapshot original mais os arquivos que foram adicionados a ele como o <span class=filename>loader.conf</span>. Os clones podem ser usados em diferentes cenários para fornecer recursos úteis aos usuários do ZFS. Por exemplo, os jails podem ser disponibilizados como snapshots contendo diferentes conjuntos de aplicativos instalados. Os usuários podem clonar esses snapshots e adicionar seus próprios aplicativos como acharem melhor. Uma vez satisfeitos com as alterações, os clones podem ser promovidos a conjuntos de dados completos e fornecidos aos usuários finais para que trabalhem como se estivessem com um conjunto de dados real. Fornecer estes jails economiza tempo e sobrecarga administrativa.</p></div></div><div class=sect2><h3 id=zfs-zfs-send>19.4.7. Replicação<a class=anchor href=#zfs-zfs-send></a></h3><div class=paragraph><p>Manter os dados em um único pool e em um único local o expõe a riscos como roubo e desastres naturais ou humanos. Fazer backups regulares de todo o pool é vital. O ZFS fornece um recurso de serialização integrado que pode enviar uma representação de fluxo dos dados para a saída padrão. Usando essa técnica, é possível não apenas armazenar os dados em outro pool conectado ao sistema local, mas também enviá-los por uma rede para outro sistema. Os snapshots são a base para essa replicação (consulte a seção sobre <a href=#zfs-zfs-snapshot>snapshots ZFS</a>). Os comandos usados para replicar dados são <code>zfs send</code> e <code>zfs receive</code>.</p></div><div class=paragraph><p>Estes exemplos demonstram a replicação do ZFS com estes dois pools:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool list</span>
NAME    SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG   CAP  DEDUP  HEALTH  ALTROOT
backup  960M    77K   896M         -         -     0%    0%  1.00x  ONLINE  -
mypool  984M  43.7M   940M         -         -     0%    4%  1.00x  ONLINE  -</code></pre></div></div><div class=paragraph><p>O pool chamado <em>mypool</em> é o pool principal no qual os dados são gravados e lidos regularmente. Um segundo pool, <em>backup</em> é usado como standby, caso o pool principal fique indisponível. Observe que esse failover não é feito automaticamente pelo ZFS, mas deve ser feito manualmente por um administrador do sistema, quando necessário. Um snapshot é usado para fornecer uma versão consistente do sistema de arquivos a ser replicado. Depois que um snapshot de <em>mypool</em> tiver sido criado, ele poderá ser copiado para o pool <em>backup</em>. Apenas snapshots podem ser replicados. As alterações feitas desde o snapshot mais recente não serão incluídas.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs snapshot mypool@backup1</span>
<span class=c># zfs list -t snapshot</span>
NAME                    USED  AVAIL  REFER  MOUNTPOINT
mypool@backup1             0      -  43.6M  -</code></pre></div></div><div class=paragraph><p>Agora que existe um snapshot, o <code>zfs send</code> pode ser usado para criar um fluxo representando o conteúdo do snapshot. Esse fluxo pode ser armazenado como um arquivo ou recebido por outro pool. O fluxo é gravado na saída padrão, mas deve ser redirecionado para um arquivo ou canal ou um erro será produzido:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs send mypool@backup1</span>
Error: Stream can not be written to a terminal.
You must redirect standard output.</code></pre></div></div><div class=paragraph><p>Para fazer backup de um conjunto de dados com o <code>zfs send</code>, redirecione para um arquivo localizado no pool de backup montado. Assegure-se de que o pool tenha espaço livre suficiente para acomodar o tamanho do snapshot que está sendo enviado, o que significa todos os dados contidos no snapshot, não apenas as mudanças do snapshot anterior.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs send mypool@backup1 &gt; /backup/backup1</span>
<span class=c># zpool list</span>
NAME    SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
backup  960M  63.7M   896M         -         -     0%     6%  1.00x  ONLINE  -
mypool  984M  43.7M   940M         -         -     0%     4%  1.00x  ONLINE  -</code></pre></div></div><div class=paragraph><p>O <code>zfs send</code> transferiu todos os dados do snapshot chamado <em>backup1</em> para o pool chamado <em>backup</em>. Criar e enviar esses snapshots pode ser feito automaticamente com uma tarefa agendada do <a href="https://man.freebsd.org/cgi/man.cgi?query=cron&amp;sektion=8&amp;format=html">cron(8)</a>.</p></div><div class=paragraph><p>Em vez de armazenar os backups como arquivos compactados, o ZFS pode recebê-los como um sistema de arquivos ativo, permitindo que os dados de backup sejam acessados diretamente. Para obter os dados reais contidos nesses fluxos, o <code>zfs receive</code> é usado para transformar os fluxos novamente em arquivos e diretórios. O exemplo a seguir combina o <code>zfs send</code> e o <code>zfs receive</code> usando um canal para copiar os dados de um pool para outro. Os dados podem ser usados diretamente no pool de recebimento após a conclusão da transferência. Um conjunto de dados só pode ser replicado para um conjunto de dados vazio.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs snapshot mypool@replica1</span>
<span class=c># zfs send -v mypool@replica1 | zfs receive backup/mypool</span>
send from @ to mypool@replica1 estimated size is 50.1M
total estimated size is 50.1M
TIME        SENT   SNAPSHOT

<span class=c># zpool list</span>
NAME    SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
backup  960M  63.7M   896M         -         -     0%     6%  1.00x  ONLINE  -
mypool  984M  43.7M   940M         -         -     0%     4%  1.00x  ONLINE  -</code></pre></div></div><div class=sect3><h4 id=zfs-send-incremental>19.4.7.1. Backups Incrementais<a class=anchor href=#zfs-send-incremental></a></h4><div class=paragraph><p>O <code>zfs send</code> também pode determinar a diferença entre dois snapshots e enviar apenas as diferenças entre os dois. Isso economiza espaço em disco e tempo de transferência. Por exemplo:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs snapshot mypool@replica2</span>
<span class=c># zfs list -t snapshot</span>
NAME                    USED  AVAIL  REFER  MOUNTPOINT
mypool@replica1         5.72M      -  43.6M  -
mypool@replica2             0      -  44.1M  -
<span class=c># zpool list</span>
NAME    SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG   CAP  DEDUP  HEALTH  ALTROOT
backup  960M  61.7M   898M         -         -     0%    6%  1.00x  ONLINE  -
mypool  960M  50.2M   910M         -         -     0%    5%  1.00x  ONLINE  -</code></pre></div></div><div class=paragraph><p>Um segundo snapshot chamado <em>replica2</em> foi criado. Este segundo snapshot contém apenas as alterações feitas no sistema de arquivos entre o snapshot atual e o anterior, <em>replica1</em>. O uso do <code>zfs send -i</code> e a indicação do par de snapshots gera um fluxo de réplica incremental contendo apenas os dados que foram alterados. Isso só será bem-sucedido se o snapshot inicial já existir no lado do recebimento.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs send -v -i mypool@replica1 mypool@replica2 | zfs receive /backup/mypool</span>
send from @replica1 to mypool@replica2 estimated size is 5.02M
total estimated size is 5.02M
TIME        SENT   SNAPSHOT

<span class=c># zpool list</span>
NAME    SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG  CAP  DEDUP  HEALTH  ALTROOT
backup  960M  80.8M   879M         -         -     0%   8%  1.00x  ONLINE  -
mypool  960M  50.2M   910M         -         -     0%   5%  1.00x  ONLINE  -

<span class=c># zfs list</span>
NAME                         USED  AVAIL  REFER  MOUNTPOINT
backup                      55.4M   240G   152K  /backup
backup/mypool               55.3M   240G  55.2M  /backup/mypool
mypool                      55.6M  11.6G  55.0M  /mypool

<span class=c># zfs list -t snapshot</span>
NAME                                         USED  AVAIL  REFER  MOUNTPOINT
backup/mypool@replica1                       104K      -  50.2M  -
backup/mypool@replica2                          0      -  55.2M  -
mypool@replica1                             29.9K      -  50.0M  -
mypool@replica2                                 0      -  55.0M  -</code></pre></div></div><div class=paragraph><p>O fluxo incremental foi transferido com sucesso. Apenas os dados que foram alterados foram replicados, em vez da totalidade da <em>replica1</em>. Somente as diferenças foram enviadas, o que levou muito menos tempo para transferir e economizou espaço em disco por não copiar o pool completo novamente. Isso é útil quando se precisa confiar em redes lentas ou quando os custos por byte transferido devem ser considerados.</p></div><div class=paragraph><p>Um novo sistema de arquivos, <em>backup/mypool</em>, está disponível com todos os arquivos e dados do pool <em>mypool</em>. Se <code>-P</code> for especificado, as propriedades do dataset serão copiadas, incluindo configurações de compactação, cotas e pontos de montagem. Quando <code>-R</code> é especificado, todos os conjuntos de dados filho do dataset indicado serão copiados, juntamente com todas as suas propriedades. O envio e o recebimento podem ser automatizados para que backups regulares sejam criados no segundo pool.</p></div></div><div class=sect3><h4 id=zfs-send-ssh>19.4.7.2. Envio de backups criptografados pelo SSH<a class=anchor href=#zfs-send-ssh></a></h4><div class=paragraph><p>O envio de fluxos pela rede é uma boa maneira de manter um backup remoto, mas apresenta uma desvantagem. Os dados enviados pelo link de rede não são criptografados, permitindo que qualquer pessoa intercepte e transforme os fluxos de volta em dados sem o conhecimento do usuário remetente. Isso é indesejável, especialmente ao enviar os fluxos pela Internet para um host remoto. O SSH pode ser usado para criptografar com segurança os dados enviados por uma conexão de rede. Como o ZFS requer apenas que o fluxo seja redirecionado da saída padrão, é relativamente fácil transmiti-lo através do SSH. Para manter o conteúdo do sistema de arquivos criptografado em trânsito e no sistema remoto, considere o uso do <a href=https://wiki.freebsd.org/PEFS>PEFS</a>.</p></div><div class=paragraph><p>Algumas configurações e precauções de segurança devem ser concluídas primeiro. Apenas as etapas necessárias para a operação do <code>zfs send</code> são mostradas aqui. Para mais informações sobre o SSH, consulte <a href=../security/#openssh>OpenSSH</a>.</p></div><div class=paragraph><p>Essa configuração é necessária:</p></div><div class=ulist><ul><li><p>Acesso SSH sem senha entre o host de envio e recebimento usando chaves SSH</p></li><li><p>Normalmente, os privilégios do usuário <code>root</code> são necessários para enviar e receber fluxos. Isso requer o login no sistema de recebimento como <code>root</code>. No entanto, o login como <code>root</code> vem desabilitado por padrão por motivos de segurança. O sistema <a href=#zfs-zfs-allow>ZFS Delegation</a> pode ser usado para permitir que um usuário não <code>root</code> em cada sistema execute as respectivas operações de envio e recebimento.</p></li><li><p>No sistema de envio:</p><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs allow -u someuser send,snapshot mypool</span></code></pre></div></div></li><li><p>Para montar o pool, o usuário não privilegiado deve ser o dono do diretório e os usuários regulares devem poder montar sistemas de arquivos. No sistema de recebimento:</p><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># sysctl vfs.usermount=1</span>
vfs.usermount: 0 -&gt; 1
<span class=c># echo vfs.usermount=1 &gt;&gt; /etc/sysctl.conf</span>
<span class=c># zfs create recvpool/backup</span>
<span class=c># zfs allow -u someuser create,mount,receive recvpool/backup</span>
<span class=c># chown someuser /recvpool/backup</span></code></pre></div></div></li></ul></div><div class=paragraph><p>O usuário sem privilégios agora tem a capacidade de receber e montar conjuntos de dados, e o conjunto de dados <em>home</em> pode ser replicado para o sistema remoto:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell>% zfs snapshot <span class=nt>-r</span> mypool/home@monday
% zfs send <span class=nt>-R</span> mypool/home@monday | ssh someuser@backuphost zfs recv <span class=nt>-dvu</span> recvpool/backup</code></pre></div></div><div class=paragraph><p>Um snapshot recursivo chamado <em>monday</em> é composto do conjunto de dados do sistema de arquivos <em>home</em> que reside no pool <em>mypool</em>. Em seguida, ele é enviado com o <code>zfs send -R</code> para incluir o conjunto de dados, todos os conjuntos de dados filho, snapshots, clones e configurações no fluxo. A saída é canalizada para o <code>zfs receive</code> em espera no host remoto <em>backuphost</em> através do SSH. Recomenda-se a utilização de um nome de domínio totalmente qualificado ou do endereço IP. A máquina receptora grava os dados no conjunto de dados <em>backup</em> no pool <em>recvpool</em>. Adicionar <code>-d</code> ao <code>zfs recv</code> sobrescreve o nome do pool no lado de recebimento com o nome do snapshot. A opção <code>-u</code> faz com que os sistemas de arquivos não sejam montados no lado do recebimento. Quando <code>-v</code> é incluído, mais detalhes sobre a transferência são mostrados, incluindo o tempo decorrido e a quantidade de dados transferidos.</p></div></div></div><div class=sect2><h3 id=zfs-zfs-quota>19.4.8. Cotas para Datasets, Usuários e Grupos<a class=anchor href=#zfs-zfs-quota></a></h3><div class=paragraph><p><a href=#zfs-term-quota>As cotas para dataset</a> são usadas para restringir a quantidade de espaço que pode ser consumida por um determinado conjunto de dados. As <a href=#zfs-term-refquota>cotas de referência</a> funcionam basicamente da mesma maneira, mas contam apenas o espaço usado pelo próprio conjunto de dados, excluindo snapshots e conjuntos de dados filho. Da mesma forma, as cotas para <a href=#zfs-term-userquota>usuário</a> e para <a href=#zfs-term-groupquota>grupo</a> podem ser usadas para impedir que usuários ou grupos usem todo o espaço do pool ou do conjunto de dados.</p></div><div class=paragraph><p>Os exemplos a seguir pressupõem que os usuários já existam no sistema. Antes de adicionar um usuário ao sistema, certifique-se de criar seu dataset antes e defina o seu <code>mountpoint</code> para <code>/home/<em>bob</em></code>. Em seguida, crie o usuário e faça com que o diretório inicial aponte para a localização do <code>mountpoint</code> do dataset. Isso definirá corretamente as permissões de proprietário e grupo sem obscurecer nenhum caminho de diretório inicial pré-existente que possa existir.</p></div><div class=paragraph><p>Para impor uma cota de dataser de 10 GB para o <span class=filename>storage/home/bob</span>:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set quota=10G storage/home/bob</span></code></pre></div></div><div class=paragraph><p>Para impor uma cota de referência de 10 GB para <span class=filename>storage/home/bob</span>:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set refquota=10G storage/home/bob</span></code></pre></div></div><div class=paragraph><p>Para remover uma cota de 10 GB do <span class=filename>storage/home/bob</span>:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set quota=none storage/home/bob</span></code></pre></div></div><div class=paragraph><p>O formato geral é <code>userquota@<em>user</em>=<em>size</em></code> e o nome do usuário deve estar em um destes formatos:</p></div><div class=ulist><ul><li><p>nome compatível com o POSIX, como <em>joe</em>.</p></li><li><p>ID numérico POSIX, como <em>789</em>.</p></li><li><p>nome SID, como <em>joe.bloggs@example.com</em>.</p></li><li><p>ID numérico SID , como <em>S-1-123-456-789</em>.</p></li></ul></div><div class=paragraph><p>Por exemplo, para impor uma cota de usuário de 50 GB para o usuário chamado <em>joe</em>:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set userquota@joe=50G</span></code></pre></div></div><div class=paragraph><p>Para remover qualquer cota:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set userquota@joe=none</span></code></pre></div></div><div class="admonitionblock note"><table><tbody><tr><td class=icon><i class="fa icon-note" title=Note></i></td><td class=content><div class=paragraph><p>As propriedades da cota do usuário não são exibidas pelo <code>zfs get all</code>. Os usuários que não são o <code>root</code> só podem ver suas próprias cotas, a menos que tenham recebido o privilégio <code>userquota</code>. Os usuários com esse privilégio podem visualizar e definir a cota de todos.</p></div></td></tr></tbody></table></div><div class=paragraph><p>O formato geral para definir uma cota de grupo é: <code>groupquota@<em>group</em>=<em>size</em></code>.</p></div><div class=paragraph><p>Para definir a cota do grupo <em>firstgroup</em> para 50 GB, use:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set groupquota@firstgroup=50G</span></code></pre></div></div><div class=paragraph><p>Para remover a cota do grupo <em>firstgroup</em> ou para certificar-se de que uma não está definida, use:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set groupquota@firstgroup=none</span></code></pre></div></div><div class=paragraph><p>Assim como a propriedade de cota do usuário, os usuários que não são <code>root</code> só podem ver as cotas associadas aos grupos aos quais eles pertencem. No entanto, o <code>root</code> ou um usuário com o privilégio <code>groupquota</code> pode visualizar e definir todas as cotas para todos os grupos.</p></div><div class=paragraph><p>Para exibir a quantidade de espaço utilizada por cada usuário em um sistema de arquivos ou snapshot junto com quaisquer cotas, use <code>zfs userspace</code>. Para informações de grupo, use <code>zfs groupspace</code>. Para obter maiores informações sobre opções suportadas ou sobre como exibir apenas opções específicas, consulte <a href="https://man.freebsd.org/cgi/man.cgi?query=zfs&amp;sektion=1&amp;format=html">zfs(1)</a>.</p></div><div class=paragraph><p>Usuários com privilégios suficientes, e o <code>root</code>, podem listar a cota para <span class=filename>storage/home/bob</span> usando:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs get quota storage/home/bob</span></code></pre></div></div></div><div class=sect2><h3 id=zfs-zfs-reservation>19.4.9. Reservas<a class=anchor href=#zfs-zfs-reservation></a></h3><div class=paragraph><p>As <a href=#zfs-term-reservation>reservas</a> garantem uma quantidade mínima de espaço sempre disponível em um conjunto de dados. O espaço reservado não estará disponível para nenhum outro conjunto de dados. Esse recurso pode ser especialmente útil para garantir que haja espaço livre disponível para um conjunto de dados ou arquivos de log importantes.</p></div><div class=paragraph><p>O formato geral da propriedade <code>reservation</code> é <code>reservation=<em>size</em></code>, portanto, para definir uma reserva de 10 GB em <span class=filename>storage/home/bob</span>, use:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set reservation=10G storage/home/bob</span></code></pre></div></div><div class=paragraph><p>Para cancelar qualquer reserva:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set reservation=none storage/home/bob</span></code></pre></div></div><div class=paragraph><p>O mesmo princípio pode ser aplicado à propriedade <code>refreservation</code> para definir uma <a href=#zfs-term-refreservation>Reserva de Referência</a>, com o formato geral <code>refreservation=<em>size</em></code>.</p></div><div class=paragraph><p>Este comando mostra todas as reservas ou atualizações existentes no <span class=filename>storage/home/bob</span>:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs get reservation storage/home/bob</span>
<span class=c># zfs get refreservation storage/home/bob</span></code></pre></div></div></div><div class=sect2><h3 id=zfs-zfs-compression>19.4.10. Compressão<a class=anchor href=#zfs-zfs-compression></a></h3><div class=paragraph><p>O ZFS fornece compactação transparente. A compactação de dados no nível do bloco a medida que ele é escrito, não apenas economiza espaço, mas também pode aumentar a performance do disco. Se os dados forem compactados em 25%, mas os dados compactados forem gravados no disco na mesma taxa da versão descompactada, resulta em uma velocidade efetiva de gravação de 125%. A compactação também pode ser uma ótima alternativa para <a href=#zfs-zfs-deduplication>Deduplicação</a> porque não requer memória adicional.</p></div><div class=paragraph><p>O ZFS oferece vários algoritmos de compactação diferentes, cada um com diferentes compensações. Com a introdução da compactação LZ4 no ZFS v5000, é possível ativar a compactação para todo o pool sem o trade-off de desempenho de outros algoritmos. A maior vantagem do LZ4 é o recurso <em>early abort</em>. Se o LZ4 não atingir pelo menos 12,5% de compactação na primeira parte dos dados, o bloco será gravado descompactado para evitar o desperdício de ciclos da CPU que tentam compactar dados já compactados ou não compactáveis. Para obter detalhes sobre os diferentes algoritmos de compactação disponíveis no ZFS, consulte a entrada <a href=#zfs-term-compression>Compactação</a> na seção de terminologia.</p></div><div class=paragraph><p>O administrador pode monitorar a eficácia da compactação usando várias propriedades do conjunto de dados.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs get used,compressratio,compression,logicalused mypool/compressed_dataset</span>
NAME        PROPERTY          VALUE     SOURCE
mypool/compressed_dataset  used              449G      -
mypool/compressed_dataset  compressratio     1.11x     -
mypool/compressed_dataset  compression       lz4       <span class=nb>local
</span>mypool/compressed_dataset  logicalused       496G      -</code></pre></div></div><div class=paragraph><p>O conjunto de dados está usando atualmente 449 GB de espaço (a propriedade used). Sem compressão, seriam necessários 496 GB de espaço (a propriedade <code>logicalused</code>). Isso resulta na taxa de compactação de 1,11: 1.</p></div><div class=paragraph><p>A compactação pode ter um efeito colateral inesperado quando combinada com <a href=#zfs-term-userquota>cotas de usuário</a>. As cotas de usuários restringem a quantidade de espaço que um usuário pode consumir em um conjunto de dados, mas as medidas são baseadas em quanto espaço é usado <em>após a compactação</em>. Portanto, se um usuário tiver uma cota de 10 GB e gravar 10 GB de dados compactáveis, eles ainda poderão armazenar dados adicionais. Se, posteriormente, atualizarem um arquivo, digamos um banco de dados, com dados mais ou menos compactáveis, a quantidade de espaço disponível para eles será alterada. Isso pode resultar na situação ímpar em que um usuário não aumentou a quantidade real de dados (a propriedade <code>logicalused</code>), mas a alteração na compactação fez com que eles atingissem seu limite de cota.</p></div><div class=paragraph><p>A compactação pode ter uma interação inesperada semelhante com backups. Muitas vezes, as cotas são usadas para limitar a quantidade de dados que podem ser armazenados para garantir que haja espaço de backup suficiente disponível. No entanto, uma vez que as cotas não consideram a compactação, mais dados podem ser gravados do que caberia com os backups descompactados.</p></div></div><div class=sect2><h3 id=zfs-zfs-deduplication>19.4.11. Desduplicação<a class=anchor href=#zfs-zfs-deduplication></a></h3><div class=paragraph><p>Quando ativado, a <a href=#zfs-term-deduplication>deduplicação</a> usa o checksum de cada bloco para detectar blocos duplicados. Quando um novo bloco é uma duplicata de um bloco existente, o ZFS grava uma referência adicional aos dados existentes, em vez de todo o bloco duplicado. Uma enorme economia de espaço é possível se os dados contiverem muitos arquivos duplicados ou informações repetidas. Esteja avisado: a desduplicação requer uma quantidade extremamente grande de memória, e a maior parte da economia de espaço pode ser obtida sem o custo extra, permitindo a compactação.</p></div><div class=paragraph><p>Para ativar a deduplicação, defina a propriedade <code>dedup</code> no pool de destino:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set dedup=on pool</span></code></pre></div></div><div class=paragraph><p>Somente novos dados sendo gravados no pool serão desduplicados. Os dados que já foram gravados no pool não serão desduplicados simplesmente ativando essa opção. Um pool com uma propriedade de desduplicação ativada recentemente será semelhante a este exemplo:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool list</span>
NAME  SIZE ALLOC  FREE   CKPOINT  EXPANDSZ   FRAG   CAP   DEDUP   HEALTH   ALTROOT
pool 2.84G 2.19M 2.83G         -         -     0%    0%   1.00x   ONLINE   -</code></pre></div></div><div class=paragraph><p>A coluna <code>DEDUP</code> mostra a taxa real de deduplicação para o pool. Um valor de <code>1.00x</code> mostra que os dados ainda não foram desduplicados. No próximo exemplo, a árvore de ports é copiada três vezes em diretórios diferentes no pool desduplicado criado acima.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># for d in dir1 dir2 dir3; do</span>
<span class=o>&gt;</span> <span class=nb>mkdir</span> <span class=nv>$d</span> <span class=o>&amp;&amp;</span> <span class=nb>cp</span> <span class=nt>-R</span> /usr/ports <span class=nv>$d</span> &amp;
<span class=o>&gt;</span> <span class=k>done</span></code></pre></div></div><div class=paragraph><p>Dados redundantes são detectados e desduplicados:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool list</span>
NAME SIZE  ALLOC  FREE   CKPOINT  EXPANDSZ   FRAG  CAP   DEDUP   HEALTH   ALTROOT
pool 2.84G 20.9M 2.82G         -         -     0%   0%   3.00x   ONLINE   -</code></pre></div></div><div class=paragraph><p>A coluna <code>DEDUP</code> mostra um fator de <code>3.00x</code>. Várias cópias dos dados da árvore de ports foram detectadas e desduplicadas, usando apenas um terço do espaço. O potencial de economia de espaço pode ser enorme, mas com o custo de ter memória suficiente para rastrear os blocos desduplicados.</p></div><div class=paragraph><p>A desduplicação nem sempre é benéfica, especialmente quando os dados em um pool não são redundantes. O ZFS pode mostrar uma possível economia de espaço ao simular a desduplicação em um pool existente:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zdb -S pool</span>
Simulated DDT histogram:

bucket              allocated                       referenced
______   ______________________________   ______________________________
refcnt   blocks   LSIZE   PSIZE   DSIZE   blocks   LSIZE   PSIZE   DSIZE
<span class=nt>------</span>   <span class=nt>------</span>   <span class=nt>-----</span>   <span class=nt>-----</span>   <span class=nt>-----</span>   <span class=nt>------</span>   <span class=nt>-----</span>   <span class=nt>-----</span>   <span class=nt>-----</span>
     1    2.58M    289G    264G    264G    2.58M    289G    264G    264G
     2     206K   12.6G   10.4G   10.4G     430K   26.4G   21.6G   21.6G
     4    37.6K    692M    276M    276M     170K   3.04G   1.26G   1.26G
     8    2.18K   45.2M   19.4M   19.4M    20.0K    425M    176M    176M
    16      174   2.83M   1.20M   1.20M    3.33K   48.4M   20.4M   20.4M
    32       40   2.17M    222K    222K    1.70K   97.2M   9.91M   9.91M
    64        9     56K   10.5K   10.5K      865   4.96M    948K    948K
   128        2   9.50K      2K      2K      419   2.11M    438K    438K
   256        5   61.5K     12K     12K    1.90K   23.0M   4.47M   4.47M
    1K        2      1K      1K      1K    2.98K   1.49M   1.49M   1.49M
 Total    2.82M    303G    275G    275G    3.20M    319G    287G    287G

dedup <span class=o>=</span> 1.05, compress <span class=o>=</span> 1.11, copies <span class=o>=</span> 1.00, dedup <span class=k>*</span> compress / copies <span class=o>=</span> 1.16</code></pre></div></div><div class=paragraph><p>Depois que o <code>zdb -S</code> termina de analisar o pool, ele mostra a taxa de redução de espaço que seria obtida ativando a deduplicação. Nesse caso, <code>1.16</code> é uma taxa de economia de espaço muito baixa e que poderia ser obtida apenas com a compactação. A ativação da deduplicação neste pool não salvaria uma quantidade significativa de espaço e não vale a quantidade de memória necessária para ativar a deduplicação. Usando a fórmula <em>ratio = dedup * compress / copies</em>, os administradores do sistema podem planejar a alocação de armazenamento, decidindo se a carga de trabalho conterá blocos duplicados suficientes para justificar os requisitos de memória. Se os dados forem razoavelmente compactáveis, a economia de espaço poderá ser muito boa. Recomenda-se ativar a compactação primeiro pois ela também pode aumentar significativamente a performance do sistema. Ative a deduplicação somente nos casos em que a economia adicional será considerável e se houver memória suficiente para o <a href=#zfs-term-deduplication>DDT</a>.</p></div></div><div class=sect2><h3 id=zfs-zfs-jail>19.4.12. ZFS e Jails<a class=anchor href=#zfs-zfs-jail></a></h3><div class=paragraph><p>O <code>zfs jail</code> e a propriedade <code>jailed</code> correspondente são usadas para delegar um conjunto de dados ZFS para uma <a href=../jails/#jails>Jail</a>. O <code>zfs jail <em>jailid</em></code> anexa um dataset à jail especificada, e o <code>zfs unjail</code> o desanexa. Para que o conjunto de dados seja controlado de dentro de um jail, a propriedade <code>jailed</code> deve ser configurada. Depois que um conjunto de dados é anexado a um jail, ele não pode mais ser montado no host porque ele poderá ter pontos de montagem que comprometam a segurança do host.</p></div></div></div></div><div class=sect1><h2 id=zfs-zfs-allow>19.5. Administração Delegada<a class=anchor href=#zfs-zfs-allow></a></h2><div class=sectionbody><div class=paragraph><p>Um sistema abrangente de delegação de permissão permite que usuários sem privilégios realizem funções de administração do ZFS. Por exemplo, se o diretório pessoal de cada usuário for um conjunto de dados, os usuários poderão receber permissão para criar e destruir snapshots de seus diretórios pessoais. Um usuário de backup pode receber permissão para usar recursos de replicação. Um script de estatísticas de uso pode ter permissão para ser executado com acesso apenas aos dados de utilização de espaço para todos os usuários. É ainda possível delegar a capacidade de delegar permissões. A delegação de permissão é possível para cada subcomando e para a maioria das propriedades.</p></div><div class=sect2><h3 id=zfs-zfs-allow-create>19.5.1. Delegando a criação de conjunto de dados<a class=anchor href=#zfs-zfs-allow-create></a></h3><div class=paragraph><p>O <code>zfs allow <em>someuser</em> create <em>mydataset</em></code> concede ao usuário especificado permissão para criar conjuntos de dados filho sob o conjunto de dados pai selecionado. Há uma ressalva: criar um novo conjunto de dados envolve montá-lo. Isso requer configurar o <code>vfs.usermount</code> <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a> do FreeBSD para <code>1</code> para permitir que usuários não-root montem um sistema de arquivos. Existe outra restrição que visa impedir o abuso: os usuários que não são <code>root</code> devem ser donos do ponto de montagem onde o sistema de arquivos deve ser montado.</p></div></div><div class=sect2><h3 id=zfs-zfs-allow-allow>19.5.2. Delegando a delegação de permissão<a class=anchor href=#zfs-zfs-allow-allow></a></h3><div class=paragraph><p>O <code>zfs allow <em>someuser</em> allow <em>mydataset</em></code> permite ao usuário especificado atribuir qualquer permissão que tenha no conjunto de dados de destino, ou nos seus filhos, para outros usuários . Se um usuário tiver a permissão <code>snapshot</code> e a permissão <code>allow</code>, esse usuário poderá conceder a permissão <code>snapshot</code> para outros usuários.</p></div></div></div></div><div class=sect1><h2 id=zfs-advanced>19.6. Tópicos Avançados<a class=anchor href=#zfs-advanced></a></h2><div class=sectionbody><div class=sect2><h3 id=zfs-advanced-tuning>19.6.1. Otimizações<a class=anchor href=#zfs-advanced-tuning></a></h3><div class=paragraph><p>Existem vários parametros que podem ser ajustados para tornar o ZFS melhor para diferentes cargas de trabalho.</p></div><div class=ulist><ul><li><p><a id=zfs-advanced-tuning-arc_max></a><code><em>vfs.zfs.arc_max</em></code> - Tamanho máximo do <a href=#zfs-term-arc>ARC</a>. O padrão é toda a memória RAM menos 1 GB, ou metade da RAM, o que for maior. No entanto, um valor menor deve ser usado se o sistema estiver executando quaisquer outros daemons ou processos que possam requerer memória. Este valor pode ser ajustado em tempo de execução com <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a> e pode ser configurado no <span class=filename>/boot/loader.conf</span> ou <span class=filename>/etc/sysctl.conf</span>.</p></li><li><p><a id=zfs-advanced-tuning-arc_meta_limit></a><code><em>vfs.zfs.arc_meta_limit</em></code> - Limita a parte do <a href=#zfs-term-arc>ARC</a> que pode ser usado para armazenar metadados. O padrão é um quarto de <code>vfs.zfs.arc_max</code>. Aumentar esse valor melhorará o desempenho se a carga de trabalho envolver operações em um grande número de arquivos e diretórios ou operações de metadados frequentes, ao custo de caber menos dados de arquivo no <a href=#zfs-term-arc>ARC</a>. Este valor pode ser ajustado em tempo de execução com <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a> e pode ser configurado em <span class=filename>/boot/loader.conf</span> ou <span class=filename>/etc/sysctl.conf</span>.</p></li><li><p><a id=zfs-advanced-tuning-arc_min></a><code><em>vfs.zfs.arc_min</em></code> - Tamanho mínimo do <a href=#zfs-term-arc>ARC</a>. O padrão é metade de <code>vfs.zfs.arc_meta_limit</code>. Ajuste esse valor para evitar que outros aplicativos pressionem o <a href=#zfs-term-arc>ARC</a> inteiro. Este valor pode ser ajustado em tempo de execução com <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a> e pode ser configurado em <span class=filename>/boot/loader.conf</span> ou <span class=filename>/etc/sysctl.conf</span>.</p></li><li><p><a id=zfs-advanced-tuning-vdev-cache-size></a><code><em>vfs.zfs.vdev.cache.size</em></code> - Uma quantidade pré-alocada de memória reservada como um cache para cada dispositivo no pool. A quantidade total de memória usada será esse valor multiplicado pelo número de dispositivos. Este valor só pode ser ajustado no momento da inicialização e é definido em <span class=filename>/boot/loader.conf</span>.</p></li><li><p><a id=zfs-advanced-tuning-min-auto-ashift></a><code><em>vfs.zfs.min_auto_ashift</em></code> - Mínimo <code>ashift</code> (tamanho do setor) que será usado automaticamente no momento da criação do pool. O valor é uma potência de dois. O valor padrão de <code>9</code> representa <code>2^9 = 512</code>, um tamanho de setor de 512 bytes. Para evitar <em>amplificação de escrita</em> e para obter o melhor desempenho, defina esse valor para o maior tamanho de setor usado por um dispositivo no pool.</p><div class=paragraph><p>Muitas unidades possuem setores de 4 KB. Usar o <code>ashift</code> padrão <code>9</code> com esses drives resulta em amplificação de gravação nesses dispositivos. Os dados que podem estar contidos em uma única gravação de 4 KB devem, em vez disso, ser gravados em oito gravações de 512 bytes. O ZFS tenta ler o tamanho do setor nativo de todos os dispositivos ao criar um pool, mas muitas unidades com setores de 4 KB relatam que seus setores têm 512 bytes para compatibilidade. Configure o <code>vfs.zfs.min_auto_ashift</code> para <code>12</code> (<code>2^12=4096</code>) antes de criar um pool irá forçar o ZFS a usar blocos de 4 KB para melhor desempenho nessas unidades.</p></div><div class=paragraph><p>Forçar blocos de 4 KB também é útil em pools em que as atualizações de disco são planejadas. Os discos futuros provavelmente usarão setores de 4 KB, e os valores de <code>ashift</code> não poderão ser alterados depois que um pool for criado.</p></div><div class=paragraph><p>Em alguns casos específicos, o menor tamanho de bloco de 512 bytes pode ser preferível. Quando usado com discos de 512 bytes para bancos de dados, ou como armazenamento para máquinas virtuais, menos dados são transferidos durante pequenas leituras aleatórias. Isso pode fornecer melhor desempenho, especialmente ao usar um tamanho de registro ZFS menor.</p></div></li><li><p><a id=zfs-advanced-tuning-prefetch_disable></a><code><em>vfs.zfs.prefetch_disable</em></code> - Desabilita a pré-busca. Um valor de <code>0</code> está ativado e <code>1</code> está desativado. O padrão é <code>0</code>, a menos que o sistema tenha menos de 4 GB de RAM. A pré-busca funciona lendo blocos maiores do que os que foram solicitados no <a href=#zfs-term-arc>ARC</a> na esperança de que os dados sejam necessários em breve. Se a carga de trabalho tiver um grande número de leituras aleatórias, a desativação da pré-busca poderá melhorar o desempenho reduzindo leituras desnecessárias. Este valor pode ser ajustado a qualquer momento com <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a>.</p></li><li><p><a id=zfs-advanced-tuning-vdev-trim_on_init></a><code><em>vfs.zfs.vdev.trim_on_init</em></code> - Controla se os novos dispositivos adicionados ao pool têm o comando <code>TRIM</code> executado neles. Isso garante o melhor desempenho e a longevidade dos SSDs, mas leva um tempo extra. Se o dispositivo já tiver sido apagado de forma segura, a desativação dessa configuração tornará o acréscimo do novo dispositivo mais rápido. Este valor pode ser ajustado a qualquer momento com <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a>.</p></li><li><p><a id=zfs-advanced-tuning-vdev-max_pending></a><code><em>vfs.zfs.vdev.max_pending</em></code> - Limita o número de solicitações de I/O pendentes por dispositivo. Um valor mais alto manterá a fila de comandos do dispositivo cheia e poderá resultar em maior rendimento. Um valor menor reduzirá a latência. Este valor pode ser ajustado a qualquer momento com o <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a>.</p></li><li><p><a id=zfs-advanced-tuning-top_maxinflight></a><code><em>vfs.zfs.top_maxinflight</em></code> - Número máximo de I/Os pendentes por <a href=#zfs-term-vdev>vdev</a> de nível superior. Limita a profundidade da fila de comandos para evitar alta latência. O limite é por vdev de nível superior, o que significa que o limite se aplica a cada <a href=#zfs-term-vdev-mirror>Mirror</a>, <a href=#zfs-term-vdev-raidz>RAID-Z</a>, ou outro vdev independentemente. Este valor pode ser ajustado a qualquer momento com <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a>.</p></li><li><p><a id=zfs-advanced-tuning-l2arc_write_max></a><code><em>vfs.zfs.l2arc_write_max</em></code> - Limita a quantidade de dados gravados no <a href=#zfs-term-l2arc>L2ARC</a> por segundo. Este ajuste foi projetado para estender a longevidade de SSDs limitando a quantidade de dados gravados no dispositivo. Este valor pode ser ajustado a qualquer momento com <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a>.</p></li><li><p><a id=zfs-advanced-tuning-l2arc_write_boost></a><code><em>vfs.zfs.l2arc_write_boost</em></code> - O valor deste ajuste é adicionado ao <a href=#zfs-advanced-tuning-l2arc_write_max><code>vfs.zfs.l2arc_write_max</code></a> e aumenta a velocidade de gravação para o SSD até que o primeiro bloco seja removido do <a href=#zfs-term-l2arc>L2ARC</a>. Esta "Turbo Warmup Phase" é projetada para reduzir a perda de desempenho de um <a href=#zfs-term-l2arc>L2ARC</a> vazio após uma reinicialização. Este valor pode ser ajustado a qualquer momento com <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a>.</p></li><li><p><a id=zfs-advanced-tuning-scrub_delay></a><code><em>vfs.zfs.scrub_delay</em></code> - Número de ticks a serem atrasados entre cada operação de I/O durante um <a href=#zfs-term-scrub><code>scrub</code></a>. Para garantir que um <code>scrub</code> não interfira com a operação normal do pool, se qualquer outra I/O estiver acontecendo, o <code>scrub</code> será atrasado entre cada comando. Esse valor controla o limite no total de IOPS (I/Os por segundo) gerados pelo <code>scrub</code>. A granularidade da configuração é determinada pelo valor de <code>kern.hz</code>, cujo padrão é de 1.000 ticks por segundo. Essa configuração pode ser alterada, resultando em um limite efetivo de IOPS diferente. O valor padrão é <code>4</code>, resultando em um limite de: 1000 ticks/seg/4 = 250 IOPS. Usar um valor de <em>20</em> daria um limite de: 1000 ticks/seg/20 = 50 IOPS. A velocidade de <code>scrub</code> é limitada apenas quando houver atividade recente no pool, conforme determinado por <a href=#zfs-advanced-tuning-scan_idle><code>vfs.zfs.scan_idle</code></a>. Esse valor pode ser ajustado a qualquer momento com <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a>.</p></li><li><p><a id=zfs-advanced-tuning-resilver_delay></a><code><em>vfs.zfs.resilver_delay</em></code> - Número de milissegundos de atraso inserido entre cada I/O durante um <a href=#zfs-term-resilver>resilver</a> . Para garantir que um resilver não interfira com a operação normal do pool, se qualquer outro I/O estiver acontecendo, o resilver irá atrasar entre cada comando. Esse valor controla o limite de total de IOPS (I/Os por segundo) gerados pelo resilver. A granularidade da configuração é determinada pelo valor de <code>kern.hz</code>, cujo padrão é de 1.000 marcações por segundo. Essa configuração pode ser alterada, resultando em um limite efetivo de IOPS diferente. O valor padrão é 2, resultando em um limite de: 1000 ticks / seg / 2 = 500 IOPS. Retornar o pool a um estado <a href=#zfs-term-online>Online</a> pode ser mais importante se a falha outro dispositivo levar o pool ao estado de <a href=#zfs-term-faulted>Fault</a>, causando perda de dados. Um valor de 0 dará à operação de resilver a mesma prioridade que outras operações, acelerando o processo de recuperação. A velocidade do resilver é limitada apenas quando houver outra atividade recente no pool, conforme determinado por <a href=#zfs-advanced-tuning-scan_idle><code>vfs.zfs.scan_idle</code></a>. Este valor pode ser ajustado a qualquer momento com <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a>.</p></li><li><p><a id=zfs-advanced-tuning-scan_idle></a><code><em>vfs.zfs.scan_idle</em></code> - Número de milissegundos desde a última operação antes do pool ser considerado ocioso. Quando o pool estiver ocioso, a taxa limite para <a href=#zfs-term-scrub><code>scrub</code></a> e <a href=#zfs-term-resilver>resilver</a> fica desativada. Este valor pode ser ajustado a qualquer momento com <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a>.</p></li><li><p><a id=zfs-advanced-tuning-txg-timeout></a><code><em>vfs.zfs.txg.timeout</em></code> - Número máximo de segundos entre os <a href=#zfs-term-txg>grupos de transações</a>. O grupo de transações atual será gravado no pool e um novo grupo de transações será iniciado se esse período de tempo tiver decorrido desde o grupo de transações anterior. Um grupo de transações pode ser acionado antes se dados suficientes forem gravados. O valor padrão é de 5 segundos. Um valor maior pode melhorar o desempenho de leitura atrasando gravações assíncronas, mas isso pode causar um desempenho irregular quando o grupo de transações é gravado. Este valor pode ser ajustado a qualquer momento com <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a>.</p></li></ul></div></div><div class=sect2><h3 id=zfs-advanced-i386>19.6.2. ZFS em i386<a class=anchor href=#zfs-advanced-i386></a></h3><div class=paragraph><p>Alguns dos recursos fornecidos pelo ZFS consomem muita memória, e podem exigir ajuste para máxima eficiência em sistemas com RAM limitada.</p></div><div class=sect3><h4 id=_memória>19.6.2.1. Memória<a class=anchor href=#_memória></a></h4><div class=paragraph><p>Como mínimo, a memória total do sistema deve ter pelo menos um gigabyte. A quantidade de RAM recomendada depende do tamanho do pool e dos recursos do ZFS usados. Uma regra geral é 1 GB de RAM para cada 1 TB de armazenamento. Se o recurso de deduplicação for usado, uma regra geral é 5 GB de RAM por TB de armazenamento para ser desduplicado. Enquanto alguns usuários usam com sucesso o ZFS com menos RAM, os sistemas sob carga pesada podem entrar em panic devido ao esgotamento da memória. Outros ajustes podem ser necessários para sistemas com uma quantia de memória RAM inferior ao recomendado.</p></div></div><div class=sect3><h4 id=_configuração_do_kernel>19.6.2.2. Configuração do Kernel<a class=anchor href=#_configuração_do_kernel></a></h4><div class=paragraph><p>Devido às limitações de espaço de endereço da plataforma i386™, os usuários do ZFS na arquitetura i386™ devem adicionar essa opção a um arquivo de configuração de kernel personalizado, reconstruir o kernel e reiniciar:</p></div><div class="literalblock programlisting"><div class=content><pre>options        KVA_PAGES=512</pre></div></div><div class=paragraph><p>Isso expande o espaço de endereço do kernel, permitindo que o parametro <code>vm.kvm_size</code> seja ajustado além do limite imposto atualmente de 1 GB ou o limite de 2 GB para PAE. Para encontrar o valor mais adequado para essa opção, divida o espaço de endereço desejado em megabytes por quatro. Neste exemplo, é <code>512</code> para 2 GB.</p></div></div><div class=sect3><h4 id=_ajustes_do_carregador>19.6.2.3. Ajustes do Carregador<a class=anchor href=#_ajustes_do_carregador></a></h4><div class=paragraph><p>O espaço de endereçamento <span class=filename>kmem</span> pode ser aumentado em todas as arquiteturas do FreeBSD. Em um sistema de teste com 1 GB de memória física, o sucesso foi alcançado com essas opções abaixo adicionadas ao <span class=filename>/boot/loader.conf</span>, e o sistema reiniciado:</p></div><div class="literalblock programlisting"><div class=content><pre>vm.kmem_size=&#34;330M&#34;
vm.kmem_size_max=&#34;330M&#34;
vfs.zfs.arc_max=&#34;40M&#34;
vfs.zfs.vdev.cache.size=&#34;5M&#34;</pre></div></div><div class=paragraph><p>Para obter uma lista mais detalhada de recomendações para otimizações relacionadas ao ZFS, consulte <a href=https://wiki.freebsd.org/ZFSTuningGuide class=bare>https://wiki.freebsd.org/ZFSTuningGuide</a>.</p></div></div></div></div></div><div class=sect1><h2 id=zfs-links>19.7. Recursos adicionais<a class=anchor href=#zfs-links></a></h2><div class=sectionbody><div class=ulist><ul><li><p><a href=http://open-zfs.org>OpenZFS</a></p></li><li><p><a href=https://wiki.freebsd.org/ZFSTuningGuide>FreeBSD Wiki - ZFS Tuning</a></p></li><li><p><a href=http://docs.oracle.com/cd/E19253-01/819-5461/index.html>Oracle Solaris ZFS Administration Guide</a></p></li><li><p><a href=https://calomel.org/zfs_raid_speed_capacity.html>Calomel Blog - ZFS Raidz Performance, Capacity and Integrity</a></p></li></ul></div></div></div><div class=sect1><h2 id=zfs-term>19.8. Recursos e terminologia do ZFS<a class=anchor href=#zfs-term></a></h2><div class=sectionbody><div class=paragraph><p>O ZFS é um sistema de arquivos fundamentalmente diferente, porque é mais do que apenas um sistema de arquivos. O ZFS combina as funções do sistema de arquivos e do gerenciador de volume, permitindo que dispositivos de armazenamento adicionais sejam adicionados a um sistema ativo e torne o novo espaço disponível em todos os sistemas de arquivos existentes nesse pool imediatamente. Combinando os papéis tradicionalmente separados, o ZFS é capaz de superar limitações anteriores que impediam o crescimento de grupos RAID. Cada dispositivo de nível superior em um pool é chamado de <em>vdev</em>, que pode ser um disco simples ou uma transformação RAID como um espelho ou array RAID-Z. Os sistemas de arquivos ZFS (chamados <em>datasets</em>) têm acesso ao espaço livre combinado de todo o pool. À medida que os blocos são alocados do pool, o espaço disponível para cada sistema de arquivos diminui. Essa abordagem evita a armadilha comum com o particionamento extensivo onde o espaço livre se torna fragmentado nas partições.</p></div><table class="tableblock frame-all grid-all stretch informaltable"><col style=width:20%><col style=width:80%><tbody><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-pool></a>pool</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Um <em>pool</em> de armazenamento é o bloco de construção mais básico do ZFS. Um pool é composto de um ou mais vdevs, os dispositivos subjacentes que armazenam os dados. Um pool é então usado para criar um ou mais sistemas de arquivos (datasets) ou dispositivos de bloco (volumes). Esses conjuntos de dados e volumes compartilham o espaço livre restante do pool. Cada pool é identificado exclusivamente por um nome e um GUID. Os recursos disponíveis são determinados pelo número da versão do ZFS no pool.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-vdev></a>vdev Types</p></td><td class="tableblock halign-left valign-top"><div class=content><div class=paragraph><p>Um pool é composto de um ou mais vdevs, que podem ser um único disco ou um grupo de discos, no caso de uma transformação RAID. Quando vários vdevs são usados, o ZFS propaga dados entre os vdevs para aumentar o desempenho e maximizar o espaço utilizável.</p></div><div class=ulist><ul><li><p><a id=zfs-term-vdev-disk></a><em>Disk</em> - O tipo mais básico de vdev é um dispositivo de bloco padrão. Isso pode ser um disco inteiro (como <span class=filename>/dev/ada0</span> ou <span class=filename>/dev/da0</span>) ou uma partição (<span class=filename>/dev/ada0p3</span>). No FreeBSD, não há penalidade de desempenho por usar uma partição em vez de todo o disco. Isso difere das recomendações feitas pela documentação do Solaris.</p><div class="admonitionblock caution"><table><tbody><tr><td class=icon><i class="fa icon-caution" title=Caution></i></td><td class=content><div class=paragraph><p>Usar um disco inteiro como parte de um pool inicializável é altamente desencorajado, pois isso pode tornar o pool não inicializável. Da mesma forma, você não deve usar um disco inteiro como parte de um mirror ou um RAID-Z vdev. Isso ocorre porque é impossível determinar com segurança o tamanho de um disco não particionado no momento da inicialização e porque não há lugar para inserir código de inicialização.</p></div></td></tr></tbody></table></div></li><li><p><a id=zfs-term-vdev-file></a><em>File</em>- Além dos discos, os pools do ZFS podem ser suportados por arquivos regulares, o que é especialmente útil para testes e experimentação. Use o caminho completo para o arquivo como o caminho do dispositivo no <code>zpool create</code>. Todos os vdevs devem ter pelo menos 128 MB de tamanho.</p></li><li><p><a id=zfs-term-vdev-mirror></a><em>Mirror</em> - Ao criar um espelho, especifique a palavra-chave <code>mirror</code> seguida pela lista de dispositivos membros para o espelho. Um espelho consiste em dois ou mais dispositivos, todos os dados serão gravados em todos os dispositivos membros. Um espelho vdev só armazenará tantos dados quanto seu menor membro. Um espelho vdev pode suportar a falha de todos, exceto um de seus membros, sem perder nenhum dado.</p><div class="admonitionblock note"><table><tbody><tr><td class=icon><i class="fa icon-note" title=Note></i></td><td class=content><div class=paragraph><p>Um vdev de disco único regular pode ser atualizado para um vdev de espelho a qualquer momento com <code>zpool <a href=#zfs-zpool-attach>attach</a></code>.</p></div></td></tr></tbody></table></div></li><li><p><a id=zfs-term-vdev-raidz></a><em>RAID-Z</em> - O ZFS implementa o RAID-Z, uma variação do padrão RAID-5 que oferece uma melhor distribuição de paridade e elimina o furo de escrita do "RAID-5" no qual os dados e informações de paridade tornam-se inconsistentes após um reinício inesperado. O ZFS suporta três níveis de RAID-Z, que fornecem vários níveis de redundância em troca de níveis decrescentes de armazenamento utilizável. Os tipos são nomeados de RAID-Z1 até RAID-Z3 com base no número de dispositivos de paridade na matriz e no número de discos que podem falhar enquanto o pool permanece operacional.</p><div class=paragraph><p>Em uma configuração de RAID-Z1 com quatro discos, cada um com 1 TB, resultará em um volume com armazenamento utilizável de 3 TB e o pool ainda poderá operar em modo degradado com um disco com falha. Se um disco adicional ficar off-line antes que o disco com falha seja substituído e que o resilver tenha sido executado, todos os dados no pool poderão ser perdidos.</p></div><div class=paragraph><p>Em uma configuração de RAID-Z3 com oito discos de 1 TB, o volume fornecerá 5 TB de espaço utilizável e ainda poderá operar com três discos com falha. A Sun™ recomenda no máximo nove discos em um único vdev. Se a configuração tiver mais discos, é recomendável dividi-los em vdevs separados e os dados do conjunto serão divididos entre eles.</p></div><div class=paragraph><p>Uma configuração de dois vdevs RAID-Z2 consistindo de 8 discos cada criaria algo similar a um array RAID-60. A capacidade de armazenamento do grupo RAID-Z é aproximadamente o tamanho do menor disco multiplicado pelo número de discos sem paridade. Quatro discos de 1 TB em RAID-Z1 têm um tamanho efetivo de aproximadamente 3 TB, e uma matriz de oito discos de 1 TB em RAID-Z3 produzirá 5 TB de espaço utilizável .</p></div></li><li><p><a id=zfs-term-vdev-spare></a><em>Spare</em>- O ZFS tem um tipo especial de pseudo-vdev para controlar os discos hot spares disponíveis. Observe que as peças de reposição instaladas não são implantadas automaticamente; eles devem ser configurados manualmente para substituir o dispositivo com falha usando o comando <code>zfs replace</code>.</p></li><li><p><a id=zfs-term-vdev-log></a><em>Log</em> - ZFS Dispositivos de log, também conhecidos como ZFS Intent Log (<a href=#zfs-term-zil>ZIL</a>) move o log de intenção dos dispositivos comuns do pool para um dispositivo dedicado, normalmente um SSD. Ter um dispositivo de log dedicado pode melhorar significativamente o desempenho de aplicativos com um alto volume de gravações síncronas, especialmente bancos de dados. Os dispositivos de log podem ser espelhados, mas o RAID-Z não é suportado. Se vários dispositivos de log forem usados, as gravações serão balanceadas entre eles.</p></li><li><p><a id=zfs-term-vdev-cache></a><em>Cache</em> - Adicionar um cache vdev a um pool adicionará o armazenamento do cache ao <a href=#zfs-term-l2arc>L2ARC</a>. Dispositivos de cache não podem ser espelhados. Como um dispositivo de cache armazena apenas cópias adicionais de dados existentes, não há risco de perda de dados.</p></li></ul></div></div></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-txg></a>Transaction Group (TXG)</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Grupos de transações são a forma como os blocos alterados são agrupados e eventualmente gravados no pool. Grupos de transação são a unidade atômica que o ZFS usa para garantir a consistência. Cada grupo de transações recebe um identificador consecutivo exclusivo de 64 bits. Pode haver até três grupos de transações ativos por vez, um em cada um desses três estados:</p><p class=tableblock>* <em>Open</em> - Quando um novo grupo de transações é criado, ele está no estado aberto e aceita novas gravações. Há sempre um grupo de transações no estado aberto, no entanto, o grupo de transações pode recusar novas gravações se tiver atingido um limite. Quando o grupo de transações abertas tiver atingido um limite ou o <a href=#zfs-advanced-tuning-txg-timeout><code>vfs.zfs.txg.timeout</code></a> tiver sido alcançado, o grupo de transações avança para o próximo estado.
* <em>Quiescing</em> - Um estado curto que permite que qualquer operação pendente termine sem bloquear a criação de um novo grupo de transações abertas. Depois que todas as transações no grupo forem concluídas, o grupo de transações avançará para o estado final.
* <em>Syncing</em> - Todos os dados no grupo de transações são gravados no armazenamento estável. Esse processo, por sua vez, modificará outros dados, como metadados e mapas de espaço, que também precisarão ser gravados no armazenamento estável. O processo de sincronização envolve vários passos. O primeiro é o maior, e trata de todos os blocos de dados alterados, seguido pelos metadados, que podem levar vários passos para serem concluídos. Como a alocação de espaço para os blocos de dados gera novos metadados, o estado de sincronização não pode ser concluído até que um passo seja concluído e não aloque espaço adicional. O estado de sincronização também é onde as <em>synctasks</em> são concluídas. As operações de sincronização são operações administrativas, como criar ou destruir snapshots e datasets, que modificam o uberblock. Quando o estado de sincronização estiver concluído, o grupo de transações no estado de quiesce é avançado para o estado de sincronização.
Todas as funções administrativas, tal como <a href=#zfs-term-snapshot><code>snapshot</code></a> são gravados como parte do grupo de transações. Quando uma tarefa de sincronização é criada, ela é adicionada ao grupo de transações atualmente aberto e esse grupo é avançado o mais rápido possível para o estado de sincronização para reduzir a latência de comandos administrativos.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-arc></a>Adaptive Replacement Cache (ARC)</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>O ZFS usa um Cache Adaptativo de Substituição (ARC), em vez de um mais tradicional como o Least Recently Used (LRU). Um cache LRU é uma lista simples de itens no cache, classificados por quando cada objeto foi usado mais recentemente. Novos itens são adicionados ao topo da lista. Quando o cache está cheio, os itens da parte inferior da lista são despejados para liberar espaço para mais objetos ativos. Um ARC consiste em quatro listas; os objetos Mais Recentes Utilizados (MRU) e Mais Frequentemente Usados (MFU), além de uma lista fantasma para cada um. Essas listas fantasmas rastreiam objetos recentemente despejados para evitar que sejam adicionados de volta ao cache. Isso aumenta a taxa de acertos do cache evitando objetos que têm um histórico de serem usados apenas ocasionalmente. Outra vantagem de usar um MRU e um MFU é que a verificação de um sistema de arquivos inteiro normalmente despejaria todos os dados de um MRU ou LRU do cache em favor deste conteúdo recém-acessado. Com o ZFS, há também um MFU que rastreia apenas os objetos usados com mais freqüência, e o cache dos blocos acessados com mais frequência permanece.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-l2arc></a>L2ARC</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>O L2ARC é o segundo nível do sistema de armazenamento em cache do ZFS. O ARC principal é armazenado em RAM. Como a quantidade de RAM disponível é limitada, o ZFS também pode usar <a href=#zfs-term-vdev-cache>cache vdevs</a>. Discos de estado sólido (SSDs) geralmente são usados como esses dispositivos de cache devido à sua maior velocidade e menor latência em comparação aos discos mecânicos tradicionais. O L2ARC é totalmente opcional, mas um deles aumentará significativamente a velocidade de leitura dos arquivos armazenados em cache no SSD em vez de precisar ser lido nos discos normais. O L2ARC também pode acelerar a <a href=#zfs-term-deduplication>desduplicação</a> porque um DDT que não cabe na RAM mas cabe no L2ARC será muito mais rápido que um DDT que deve ser lido do disco. A taxa na qual os dados são adicionados aos dispositivos de cache é limitada para evitar o desgaste prematuro dos SSDs com muitas gravações. Até que o cache esteja cheio (o primeiro bloco foi removido para liberar espaço), a gravação no L2ARC é limitada à soma do limite de gravação e do limite de aumento e depois limitada ao limite de gravação. Um par de valores <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a> controla esses limites de taxa. A <a href=#zfs-advanced-tuning-l2arc_write_max><code>vfs.zfs.l2arc_write_max</code></a> controla quantos bytes são gravados no cache por segundo, enquanto <a href=#zfs-advanced-tuning-l2arc_write_boost><code>vfs.zfs.l2arc_write_boost</code></a> adiciona a este limite durante a "Turbo Warmup Phase " (aumento de gravação).</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-zil></a>ZIL</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>O ZIL acelera as transações síncronas usando dispositivos de armazenamento como SSDs mais rápidos do que os usados no pool de armazenamento principal. Quando um aplicativo solicita uma gravação síncrona (uma garantia de que os dados foram armazenados com segurança no disco, em vez de simplesmente serem gravados posteriormente), os dados são gravados no armazenamento mais rápido de ZIL e, depois, liberados aos discos regulares. Isso reduz enormemente a latência e melhora o desempenho. Apenas cargas de trabalho síncronas, como bancos de dados, serão beneficiadas com um ZIL. Gravações assíncronas regulares, como copiar arquivos, não usarão o ZIL.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-cow></a>Copy-On-Write</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Ao contrário de um sistema de arquivos tradicional, quando os dados são sobrescritos no ZFS, os novos dados são gravados em um bloco diferente, em vez de sobrescrever os dados antigos no lugar. Somente quando essa gravação for concluída, os metadados serão atualizados para apontar para o novo local. No caso de uma gravação simplificada (uma falha do sistema ou perda de energia no meio da gravação de um arquivo), todo o conteúdo original do arquivo ainda estará disponível e a gravação incompleta será descartada. Isso também significa que o ZFS não requer um <a href="https://man.freebsd.org/cgi/man.cgi?query=fsck&amp;sektion=8&amp;format=html">fsck(8)</a> após um desligamento inesperado.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-dataset></a>Dataset</p></td><td class="tableblock halign-left valign-top"><p class=tableblock><em>Dataset</em> é o termo genérico para um sistema de arquivos ZFS, volume, snapshot ou clone. Cada dataset tem um nome exclusivo no formato <em>poolname/path@snapshot</em>. A raiz do pool é tecnicamente um dataset também. Dataset filhos são nomeados hierarquicamente como diretórios. Por exemplo, <em>mypool/home</em>, o dataset inicial, é um filho de <em>mypool</em> e herda propriedades dele. Isso pode ser expandido ainda mais criando o <em>mypool/home/user</em>. Este dataset neto herdará propriedades do pai e do avô. As propriedades de um filho podem ser definidas para substituir os padrões herdados dos pais e avós. A administração de datasets e seus filhos pode ser <a href=#zfs-zfs-allow>delegada</a>.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-filesystem></a>File system</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Um dataset ZFS é mais frequentemente usado como um sistema de arquivos. Como a maioria dos outros sistemas de arquivos, um sistema de arquivos ZFS é montado em algum lugar na hierarquia de diretórios do sistema e contém arquivos e diretórios próprios com permissões, sinalizadores e outros metadados.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-volume></a>Volume</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Além dos datasets regulares do sistema de arquivos, o ZFS também pode criar volumes, que são dispositivos de bloco. Os volumes têm muitos dos mesmos recursos, incluindo copy-on-write, snapshots, clones e checksum. Os volumes podem ser úteis para executar outros formatos de sistema de arquivos sobre o ZFS, tal como a virtualização do UFS ou a exportação de extensões iSCSI.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-snapshot></a>Snapshot</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>O design <a href=#zfs-term-cow>copy-on-write</a> (COW) do ZFS permite snapshots quase instantâneos e consistentes com nomes arbitrários. Depois de obter um snapshot de um dataset ou um snapshot recursivo de um dataset pai que incluirá todos os datasets filho, novos dados serão gravados em novos blocos, mas os blocos antigos não serão recuperados como espaço livre. O snapshot contém a versão original do sistema de arquivos e o sistema de arquivos em tempo real contém as alterações feitas desde que o snapshot foi feito. Nenhum espaço adicional é usado. Conforme novos dados são gravados no sistema de arquivos ao vivo, novos blocos são alocados para armazenar esses dados. O tamanho aparente do snapshot aumentará à medida que os blocos não forem mais usados no sistema de arquivos ativo, mas apenas no snapshot. Estes snapshots podem ser montados somente como leitura para permitir a recuperação de versões anteriores de arquivos. Também é possível <a href=#zfs-zfs-snapshot>reverter</a> um sistema de arquivos ativo para um snapshot específico, desfazendo quaisquer alterações que ocorreram depois que o snapshot foi tirado. Cada bloco no pool tem um contador de referência que registra quantos snapshots, clones, datasets ou volumes fazem uso desse bloco. À medida que arquivos e snapshots são excluídos, a contagem de referência é diminuída. Quando um bloco não é mais referenciado, ele é recuperado como espaço livre. Os snapshots também podem ser marcados com um <a href=#zfs-zfs-snapshot>hold</a>. Quando um snapshot é mantido, qualquer tentativa de destruí-lo retornará um erro <code>EBUSY</code>. Cada snapshot pode ter várias retenções, cada uma com um nome exclusivo. O comando <a href=#zfs-zfs-snapshot>release</a> remove a retenção para que o snapshot possa ser excluído. Snapshots podem ser obtidos de volumes, mas eles só podem ser clonados ou revertidos, não montados independentemente.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-clone></a>Clone</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Os snapshots também podem ser clonados. Um clone é uma versão gravável de um snapshot, permitindo que o sistema de arquivos seja bifurcado como um novo dataset. Como com um snapshot, um clone inicialmente não consome espaço adicional. Conforme novos dados são gravados em um clone e novos blocos são alocados, o tamanho aparente do clone aumenta. Quando os blocos são sobrescritos no sistema de arquivos ou no volume clonado, a contagem de referência no bloco anterior é diminuída. O snapshot no qual um clone é baseado não pode ser excluído porque o clone depende dele. O snapshot é o pai e o clone é o filho. Os clones podem ser <em>promovidos</em>, invertendo essa dependência e tornando o clone o pai e o pai anterior, o filho. Esta operação não requer espaço adicional. Como a quantidade de espaço usada pelo pai e pelo filho é revertida, as cotas e reservas existentes podem ser afetadas.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-checksum></a>Checksum</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Cada bloco alocado também é verificado. O algoritmo de checksum usado é uma propriedade por dataset, consulte <a href=#zfs-zfs-set><code>set</code></a>. O checksum de cada bloco é validado de forma transparente à medida que é lido, permitindo que o ZFS detecte a corrupção silenciosa. Se os dados lidos não corresponderem à checksum esperada, o ZFS tentará recuperar os dados de qualquer redundância disponível, como espelhos ou RAID-Z). A validação de todos os checksums pode ser acionada com o <a href=#zfs-term-scrub><code>scrub</code></a>. Os algoritmos de checksum incluem:</p><p class=tableblock>* <code>fletcher2</code>
* <code>fletcher4</code>
* <code>sha256</code>
Os algoritmos <code>fletcher</code> são mais rápidos, mas o <code>sha256</code> é um hash criptográfico forte e tem uma chance muito menor de colisões ao custo de algum desempenho. Checksums podem ser desativados, mas isso não é recomendado.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-compression></a>Compression</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Cada dataset tem uma propriedade de compactação, cujo padrão é off. Essa propriedade pode ser definida como um dos vários algoritmos de compactação. Isso fará com que todos os novos dados gravados no dataset sejam compactados. Além de uma redução no espaço usado, a taxa de leitura e gravação geralmente aumenta porque menos blocos são lidos ou gravados.</p><p class=tableblock><a id=zfs-term-compression-lz4></a>* <em>LZ4</em> - Adicionado na versão 5000 do pool do ZFS (feature flags), o LZ4 é agora o algoritmo de compressão recomendado. O LZ4 compacta aproximadamente 50% mais rápido do que o LZJB ao operar em dados compactáveis e é três vezes mais rápido ao operar em dados não compactáveis. O LZ4 também descompacta aproximadamente 80% mais rápido que o LZJB. Nas CPUs modernas, o LZ4 pode frequentemente comprimir a mais de 500 MB/s e descompactar a mais de 1,5 GB/s (por núcleo de CPU).
<a id=zfs-term-compression-lzjb></a>* <em>LZJB</em> - O algoritmo de compressão padrão. Criado por Jeff Bonwick (um dos criadores originais do ZFS). O LZJB oferece boa compactação com menos sobrecarga de CPU em comparação com o GZIP. No futuro, o algoritmo de compactação padrão provavelmente será alterado para LZ4.
<a id=zfs-term-compression-gzip></a>* <em>GZIP</em> - Um algoritmo popular de compressão de fluxo disponível no ZFS. Uma das principais vantagens de usar o GZIP é seu nível configurável de compactação. Ao definir a propriedade <code>compress</code>, o administrador pode escolher o nível de compactação, desde <code>gzip1</code>, o nível mais baixo de compactação, até <code>gzip9</code>, o maior nível de compressão. Isso dá ao administrador o controle sobre quanto tempo CPU será dedicado para economizar espaço em disco.
<a id=zfs-term-compression-zle></a>* <em>ZLE</em> - A codificação de comprimento zero é um algoritmo de compressão especial que apenas comprime sequencias contínuas de zeros. Esse algoritmo de compactação é útil apenas quando o dataset contém grandes blocos de zeros.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-copies></a>Copies</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Quando configurada para um valor maior que 1, a propriedade <code>copies</code> instrui o ZFS a manter várias cópias de cada bloco no <a href=#zfs-term-filesystem>sistema de arquivos </a>ou <a href=#zfs-term-volume>volume</a>. Definir essa propriedade em datasets importantes fornece redundância adicional a partir da qual recuperar um bloco que não corresponde ao seu checksum. Em pools sem redundância, o recurso de cópias é a única forma de redundância. O recurso de cópias pode se recuperar de um único setor defeituoso ou de outras formas de corrupção menor, mas não protege o pool da perda de um disco inteiro.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-deduplication></a>Deduplication</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Os checksums permitem detectar blocos duplicados de dados à medida que são escritos. Com a deduplicação, a contagem de referência de um bloco existente e idêntico é aumentada, economizando espaço de armazenamento. Para detectar blocos duplicados, uma tabela de deduplicação (DDT) é mantida na memória. A tabela contém uma lista de checksums exclusivas, a localização desses blocos e uma contagem de referência. Quando novos dados são gravados, o checksum é calculado e comparado à lista. Se uma correspondência for encontrada, o bloco existente será usado. O algoritmo de checksum SHA256 é usado com deduplicação para fornecer um hash criptográfico seguro. A desduplicação é configurável. Se <code>dedup</code> for <code>on</code>, um checksum correspondente será considerado como significando que os dados são idênticos. Se <code>dedup</code> for definido como <code>verify</code>, os dados nos dois blocos serão verificados byte por byte para garantir que sejam realmente idênticos. Se os dados não forem idênticos, a colisão de hash será anotada e os dois blocos serão armazenados separadamente. Como o DDT deve armazenar o hash de cada bloco único, ele consome uma quantidade muito grande de memória. Uma regra geral é 5-6 GB de RAM por 1 TB de dados desduplicados). Em situações em que não é prático ter RAM suficiente para manter todo o DDT na memória, o desempenho sofrerá muito, pois o DDT deve ser lido do disco antes que cada novo bloco seja gravado. A desduplicação pode usar o L2ARC para armazenar o DDT, fornecendo um meio termo entre a memória rápida do sistema e os discos mais lentos. Considere a possibilidade de usar a compactação, que geralmente oferece quase a mesma economia de espaço sem o requisito de memória adicional.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-scrub></a>Scrub</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Em vez de uma verificação de consistência como o <a href="https://man.freebsd.org/cgi/man.cgi?query=fsck&amp;sektion=8&amp;format=html">fsck(8)</a>, o ZFS tem o <code>scrub</code>. O <code>scrub</code> lê todos os blocos de dados armazenados no pool e verifica seus checksums em relação checksums bons conhecidos armazenados nos metadados. Uma verificação periódica de todos os dados armazenados no pool garante a recuperação de quaisquer blocos corrompidos antes que eles sejam necessários. Um scrub não é necessário após um desligamento inadequado do sistema, mas é recomendado pelo menos uma vez a cada três meses. O checksum de cada bloco é verificado à medida que os blocos são lidos durante o uso normal, mas um scrub garante que mesmo os blocos usados com pouca freqüência sejam verificados quanto a sua corrupção silenciosa. A segurança dos dados é aprimorada, especialmente em situações de armazenamento de arquivos. A prioridade relativa do <code>scrub</code> pode ser ajustada por meio da variável <a href=#zfs-advanced-tuning-scrub_delay><code>vfs.zfs.scrub_delay</code></a> para evitar que o scrub degrade o desempenho de outras cargas de trabalho no pool.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-quota></a>Dataset Quota</p></td><td class="tableblock halign-left valign-top"><div class=content><div class=paragraph><p>O ZFS fornece datasets rápidos e precisos, contabilidade de espaço de usuários e grupos, além de cotas e reservas de espaço. Isso dá ao administrador um controle refinado sobre como o espaço é alocado e permite que o espaço seja reservado para sistemas de arquivos críticos.</p></div><div class=paragraph><p>ZFS supports different types of quotas: the dataset quota, the <a href=#zfs-term-refquota>reference quota (refquota)</a>, the <a href=#zfs-term-userquota>user quota</a>, and the <a href=#zfs-term-groupquota>group quota</a>.</p></div><div class=paragraph><p>As cotas limitam a quantidade de espaço que um dataset e todos os seus descendentes, incluindo snapshots do dataset, datasets filhos e snapshots desses datasets, podem consumir.</p></div><div class="admonitionblock note"><table><tbody><tr><td class=icon><i class="fa icon-note" title=Note></i></td><td class=content><div class=paragraph><p>Cotas não podem ser definidas em volumes, pois a propriedade <code>volsize</code> atua como uma cota implícita.</p></div></td></tr></tbody></table></div></div></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-refquota></a>Reference Quota</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Uma cota de referência limita a quantidade de espaço que um dataset pode consumir impondo um limite rígido. No entanto, esse limite rígido inclui apenas o espaço ao qual o dataset faz referência e não inclui o espaço usado pelos descendentes, como sistemas de arquivos ou snapshots.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-userquota></a>User Quota</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Cotas de usuários são úteis para limitar a quantidade de espaço que pode ser usada pelo usuário especificado.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-groupquota></a>Group Quota</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>A cota de grupo limita a quantidade de espaço que um grupo especificado pode consumir.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-reservation></a>Dataset Reservation</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>A propriedade <code>reservation</code> torna possível garantir uma quantidade mínima de espaço para um dataset específico e seus descendentes. Se uma reserva de 10 GB estiver definida em <span class=filename>storage/home/bob</span>, e outro dataset tentar usar todo o espaço livre, pelo menos 10 GB de espaço serão reservados para este dataset. Se um snapshot for criado de <span class=filename>storage/home/bob</span>, o espaço usado por esse snapshot será contabilizado contra a reserva. A propriedade <a href=#zfs-term-refreservation><code>refreservation</code></a> funciona de maneira semelhante, mas <em>exclui</em> os descendentes como os snapshots.</p><p class=tableblock>Reservas de qualquer tipo são úteis em muitas situações, como planejar e testar a adequação da alocação de espaço em disco em um novo sistema ou garantindo espaço suficiente nos sistemas de arquivos para logs de áudio ou procedimentos e arquivos de recuperação do sistema.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-refreservation></a>Reference Reservation</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>A propriedade <code>refreservation</code> torna possível garantir uma quantidade mínima de espaço para o uso de dataset específico <em>excluindo</em> seus descendentes. Isso significa que, se uma reserva de 10 GB estiver definida em <span class=filename>storage/home/bob</span>, e outro dataset tentar usar todo o espaço livre, pelo menos 10 GB de espaço serão reservados para este dataset. Em contraste com uma <a href=#zfs-term-reservation>reserva</a> regular, o espaço usado por snapshots e datasets descendentes não é contado contra a reserva. Por exemplo, se um snapshot for criado do <span class=filename>storage/home/bob</span>, deve haver espaço em disco suficiente fora da quantia de <code>refreservation</code> para que a operação seja bem-sucedida. Descendentes do dataset principal não são contados na quantia de <code>refreservation</code> e, portanto, não invadem o espaço definido.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-resilver></a>Resilver</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Quando um disco falha e é substituído, o novo disco deve ser preenchido com os dados perdidos. O processo de usar as informações de paridade distribuídas entre as unidades restantes para calcular e gravar os dados ausentes na nova unidade é chamado de <em>resilvering</em>.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-online></a>Online</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Um pool ou vdev no estado <code>Online</code> tem todos os seus dispositivos membros conectados e totalmente operacionais. Dispositivos individuais no estado <code>Online</code> estão funcionando normalmente.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-offline></a>Offline</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Dispositivos individuais podem ser colocados em um estado <code>Offline</code> pelo administrador se houver redundância suficiente para evitar colocar o pool ou vdev em um estado <a href=#zfs-term-faulted>Faulted</a>. Um administrador pode optar por colocar um disco off-line como preparação para substituí-lo ou para facilitar sua identificação.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-degraded></a>Degraded</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Um pool ou vdev no estado <code>Degraded</code> possui um ou mais discos que foram desconectados ou falharam. O pool ainda é utilizável, mas se dispositivos adicionais falharem, o pool poderá se tornar irrecuperável. Reconectar os dispositivos ausentes ou substituir os discos com falha retornará o pool a um estado <a href=#zfs-term-online>Online</a> depois que o dispositivo reconectado ou novo tiver concluído o processo de <a href=#zfs-term-resilver>Resilver</a>.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-faulted></a>Faulted</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Um pool ou vdev no estado <code>Faulted</code> não está mais operacional. Os dados nele não podem mais ser acessados. Um pool ou vdev entra no estado <code>Faulted</code> quando o número de dispositivos ausentes ou com falha excede o nível de redundância no vdev. Se os dispositivos ausentes puderem ser reconectados, o pool retornará ao estado <a href=#zfs-term-online>Online</a>. Se houver redundância insuficiente para compensar o número de discos com falha, o conteúdo do pool será perdido e deverá ser restaurado a partir de um backup.</p></td></tr></tbody></table></div></div></div><hr><div class=last-modified><p><strong>Última alteração em</strong>: 9 de março de 2024 por <a href="https://cgit.freebsd.org/doc/commit/?id=6199af92e7" target=_blank>Danilo G. Baio</a></p></div><div class=buttons><div class=prev><i class="fa fa-angle-left" aria-hidden=true title=Anterior></i><div class=container><a href=http://172.16.201.134:1313/pt-br/books/handbook/geom class=direction>Anterior</a></div></div><div class=home><i class="fa fa-home" aria-hidden=true title=Início></i><div class=container><a href=../ class=direction>Início</a></div></div><div class=next><div class=container><a href=http://172.16.201.134:1313/pt-br/books/handbook/filesystems class=direction>Próximo</a></div><i class="fa fa-angle-right" aria-hidden=true title=Próximo></i></div></div><label class="hidden book-menu-overlay" for=menu-control></label></div><aside class=toc><div class=toc-content><h3>Índice</h3><nav id=TableOfContents><ul><li><a href=#zfs-differences>19.1. O que torna o ZFS diferente</a></li><li><a href=#zfs-quickstart>19.2. Guia de Início Rápido</a></li><li><a href=#zfs-zpool>19.3. Administração <code>zpool</code></a></li><li><a href=#zfs-zfs>19.4. Administração do <code>zfs</code></a></li><li><a href=#zfs-zfs-allow>19.5. Administração Delegada</a></li><li><a href=#zfs-advanced>19.6. Tópicos Avançados</a></li><li><a href=#zfs-links>19.7. Recursos adicionais</a></li><li><a href=#zfs-term>19.8. Recursos e terminologia do ZFS</a></li></ul></nav><hr><div class=resources><h3>Recursos</h3><ul class=contents><li><i class="fa fa-file-pdf-o" aria-hidden=true title="Download PDF"></i><a href=https://download.freebsd.org/doc/pt-br/books/handbook/handbook_pt-br.pdf>Download PDF</a></li><li><i class="fa fa-pencil-square-o" aria-hidden=true title="Edite essa página"></i><a href=https://github.com/freebsd/freebsd-doc/blob/main/documentation/content/pt-br/_index target=_blank>Edite essa página</a></li></ul></div></div></aside><a class=to-top href=#top><i class="fa fa-arrow-circle-up" aria-hidden=true></i></a></main><footer><div class=footer-container><section class=logo-column><img src=http://172.16.201.134:1313/images/FreeBSD-colors.svg width=160 height=50 alt="FreeBSD logo"><div class=options-container><div class=language-container><a id=languages href=http://172.16.201.134:1313/pt-br/languages><img src=http://172.16.201.134:1313/images/language.png class=language-image alt="Escolha o idioma">
<span>Brazilian Portuguese</span></a></div><div class=theme-container><select id=theme-chooser><option value=theme-system>System</option><option value=theme-light>Light</option><option value=theme-dark>Dark</option><option value=theme-high-contrast>Alto contraste</option></select></div></div></section><section class=about-column><h3 class=column-title>About</h3><ul class=column-elements-container><li><a href=https://www.freebsd.org/about/ target=_blank class=column-element>FreeBSD</a></li><li><a href=https://freebsdfoundation.org/ target=_blank class=column-element>FreeBSD Foundation</a></li><li><a href=https://www.freebsd.org/where/ target=_blank class=column-element>Get FreeBSD</a></li><li><a href=https://www.freebsd.org/internal/code-of-conduct target=_blank class=column-element>Code of Conduct</a></li><li><a href=https://www.freebsd.org/security/ target=_blank class=column-element>Security Advisories</a></li></ul></section><section class=documentation-column><h3 class=column-title>Documentation</h3><ul class=column-elements-container><li><a href=/pt-br class=column-element>Documentation portal</a></li><li><a href=https://man.FreeBSD.org target=_blank class=column-element>Manual pages</a></li><li><a href=https://papers.FreeBSD.org target=_blank class=column-element>Presentations and papers</a></li><li><a href=https://docs-archive.freebsd.org/doc/ target=_blank class=column-element>Previous versions</a></li><li><a href=https://docs-archive.freebsd.org/44doc/ target=_blank class=column-element>4.4BSD Documents</a></li><li><a href=https://wiki.freebsd.org/ target=_blank class=column-element>Wiki</a></li></ul></section><section class=community-column><h3 class=column-title>Community</h3><ul class=column-elements-container><li><a href=http://172.16.201.134:1313/pt-br/articles/contributing class=column-element>Get involved</a></li><li><a href=https://forums.freebsd.org/ target=_blank class=column-element>Community forum</a></li><li><a href=https://lists.freebsd.org/ target=_blank class=column-element>Mailing lists</a></li><li><a href=https://wiki.freebsd.org/IRC/Channels target=_blank class=column-element>IRC Channels</a></li><li><a href=https://bugs.freebsd.org/bugzilla/ target=_blank class=column-element>Bug Tracker</a></li></ul></section><section class=legal-column><h3 class=column-title>Legal</h3><ul class=column-elements-container><li><a href=https://freebsdfoundation.org/donate/ target=_blank class=column-element>Donations</a></li><li><a href=https://www.freebsd.org/copyright/freebsd-license/ target=_blank class=column-element>Licensing</a></li><li><a href=https://www.freebsd.org/privacy/ target=_blank class=column-element>Privacy Policy</a></li><li><a href=https://www.freebsd.org/copyright/ target=_blank class=column-element>Legal notices</a></li></ul></section><section class=copyright-column><p>&copy; 1994-2024 The FreeBSD Project. All rights reserved</p><span>Made with <span class=heart>♥</span> by the FreeBSD Community</span></section></div></footer></body></html>