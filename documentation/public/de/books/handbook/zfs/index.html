<!doctype html><html class=theme-light lang=de><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content><meta name=keywords content><meta name=copyright content="1995-2024 The FreeBSD Foundation"><link rel=canonical href=http://172.16.201.134:1313/de/books/handbook/zfs/><title>Kapitel 19. Das Z-Dateisystem (ZFS) | FreeBSD Documentation Portal</title>
<meta name=theme-color content="#790000"><meta name=color-scheme content="system light dark high-contrast"><link rel="shortcut icon" href=http://172.16.201.134:1313/favicon.ico><link rel=stylesheet href=http://172.16.201.134:1313/styles/main.min.css><link rel=stylesheet href=http://172.16.201.134:1313/css/font-awesome-min.css><script defer src=/js/theme-chooser.min.js></script><script defer src=/js/copy-clipboard.min.js></script><script defer src=/js/search.min.js></script><meta name=twitter:card content="summary"><meta name=twitter:domain content="docs.FreeBSD.org"><meta name=twitter:site content="@freebsd"><meta name=twitter:url content="https://twitter.com/freebsd"><meta property="og:title" content="Kapitel 19. Das Z-Dateisystem (ZFS)"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:image" content="http://172.16.201.134:1313/favicon.ico"><meta property="og:image:alt" content="FreeBSD Logo"><meta property="og:locale" content="de"><meta property="og:url" content="http://172.16.201.134:1313/de/books/handbook/zfs/"><meta property="og:site_name" content="FreeBSD Documentation Portal"><script type=application/ld+json>{"@context":"http://schema.org","@type":"Article","url":"http:\/\/172.16.201.134:1313\/de\/books\/handbook\/zfs\/","name":"FreeBSD Documentation Portal","headline":"FreeBSD Documentation Portal","description":"FreeBSD Documentation Portal"}</script></head><body><header><div class=header-container><div class=logo-menu-bars-container><a href=https://www.FreeBSD.org class=logo><img src=http://172.16.201.134:1313/images/FreeBSD-monochromatic.svg width=160 height=50 alt="FreeBSD logo">
</a><label class=menu-bars for=menu-bars><i class="fa fa-bars" aria-hidden=true></i></label></div><input id=menu-bars type=checkbox><nav><ul class=menu><li class=menu-item><input id=about type=checkbox>
<a href=# aria-label="Navigate to About section"><label class=menu-item-description for=about>About
<i class="fa fa-angle-down fa-lg" aria-hidden=true></i></label></a><ul class=sub-menu><li class=title><a href=https://www.freebsd.org/about/ target=_blank>About</a></li><li><a href=https://www.freebsd.org/about/ target=_blank>FreeBSD</a></li><li><a href=https://freebsdfoundation.org/about-us/about-the-foundation/ target=_blank>FreeBSD Foundation</a></li><li><a href=https://www.freebsd.org/internal/code-of-conduct/ target=_blank>Code of Conduct</a></li></ul></li><li class=menu-item><input id=download type=checkbox>
<a href=# aria-label="Navigate to get FreeBSD section"><label class=menu-item-description for=download>Get FreeBSD
<i class="fa fa-angle-down fa-lg" aria-hidden=true></i></label></a><ul class=sub-menu><li class=title><a href=https://www.freebsd.org/where/ target=_blank>Get FreeBSD</a></li><li><a href=https://www.freebsd.org/releases/ target=_blank>Release Information</a></li><li><a href=https://www.freebsd.org/releng/ target=_blank>Release Engineering</a></li><li><a href=https://www.freebsd.org/security/ target=_blank>Security Advisories</a></li></ul></li><li class=menu-item><input id=documentation type=checkbox>
<a href=# aria-label="Navigate to get Documentation section"><label class=menu-item-description for=documentation>Documentation
<i class="fa fa-angle-down fa-lg" aria-hidden=true></i></label></a><ul class=sub-menu><li class=title><a href=/de>Documentation portal</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook>FreeBSD Handbook</a></li><li><a href=http://172.16.201.134:1313/de/books/porters-handbook>Porter's Handbook</a></li><li><a href=https://docs.FreeBSD.org/en/books/fdp-primer>Documentation Project Handbook</a></li><li><a href=https://man.FreeBSD.org target=_blank>Manual pages</a></li><li><a href=https://papers.FreeBSD.org target=_blank>Presentations and papers</a></li><li><a href=https://wiki.FreeBSD.org target=_blank>Wiki</a></li><li><a href=http://172.16.201.134:1313/de/books>Books</a></li><li><a href=http://172.16.201.134:1313/de/articles>Articles</a></li></ul></li><li class=menu-item><input id=community type=checkbox>
<a href=# aria-label="Navigate to get Community section"><label class=menu-item-description for=community>Community
<i class="fa fa-angle-down fa-lg" aria-hidden=true></i></label></a><ul class=sub-menu><li class=title><a href=https://www.freebsd.org/community/>Community</a></li><li><a href=http://172.16.201.134:1313/de/articles/contributing>Get involved</a></li><li><a href=https://forums.freebsd.org/ target=_blank>Forum</a></li><li><a href=https://lists.freebsd.org/ target=_blank>Mailing lists</a></li><li><a href=https://wiki.freebsd.org/IRC/Channels target=_blank>IRC Channels</a></li><li><a href=https://bugs.freebsd.org/bugzilla/ target=_blank>Bug Tracker</a></li><li><a href=https://www.freebsd.org/support/ target=_blank>Support</a></li></ul></li></ul></nav><div class=search-donate-container><form class=search method=get id=search-header-form action=https://docs.freebsd.org/search name=search-header-form><input type=hidden name=DB value=de>
<input id=words name=P type=text size=20 maxlength=255>
<button>
<i class="fa fa-search" aria-hidden=true></i></button></form><div class=donate><a href=https://freebsdfoundation.org/donate/ target=_blank><span class=heart>♥</span>
Donate</a></div></div></div></header><input type=checkbox class="hidden toggle" id=menu-control><main class=main-wrapper-book><a id=top></a><aside class=book-menu><div class=book-menu-content><input id=search-book type=text placeholder=Search aria-label=Search maxlength=128><nav id=MenuContents><ul><li><input type=checkbox id=chapter-eb2a98ce203d8afd517726e6d8776be3 class=toggle>
<label class="icon cursor" for=chapter-eb2a98ce203d8afd517726e6d8776be3><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/preface/>Vorwort</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/preface/#preface-audience>Über dieses Buch</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/preface/#preface-changes-from3>Änderungen gegenüber der dritten Auflage</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/preface/#preface-changes-from2>Änderungen gegenüber der zweiten Auflage (2004)</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/preface/#preface-changes>Änderungen gegenüber der ersten Auflage (2001)</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/preface/#preface-overview>Gliederung</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/preface/#preface-conv>Konventionen in diesem Buch</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/preface/#preface-acknowledgements>Danksagung</a></li></ul></li><li><input type=checkbox id=chapter-14a525fce014b90b8a458a894818255a class=toggle>
<label for=chapter-14a525fce014b90b8a458a894818255a><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/parti/>Teil I. Erste Schritte</a></li><li><input type=checkbox id=chapter-f9c9f3451644df30d224350da97d5da6 class=toggle>
<label class="icon cursor" for=chapter-f9c9f3451644df30d224350da97d5da6><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/introduction/>Kapitel 1. Einleitung</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/introduction/#introduction-synopsis>1.1. Überblick</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/introduction/#nutshell>1.2. Willkommen zu FreeBSD!</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/introduction/#history>1.3. Über das FreeBSD Projekt</a></li></ul></li><li><input type=checkbox id=chapter-f693a3fa687a72d63ec8129ee302d664 class=toggle>
<label class="icon cursor" for=chapter-f693a3fa687a72d63ec8129ee302d664><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/bsdinstall/>Kapitel 2. FreeBSD installieren</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/bsdinstall/#bsdinstall-synopsis>2.1. Übersicht</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/bsdinstall/#bsdinstall-hardware>2.2. Minimale Hardwareanforderungen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/bsdinstall/#bsdinstall-pre>2.3. Vor der Installation</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/bsdinstall/#bsdinstall-start>2.4. Die Installation starten</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/bsdinstall/#using-bsdinstall>2.5. Verwendung von bsdinstall</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/bsdinstall/#bsdinstall-partitioning>2.6. Plattenplatz bereitstellen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/bsdinstall/#bsdinstall-fetching-distribution>2.7. Abrufen der Distributionen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/bsdinstall/#bsdinstall-post>2.8. Benutzerkonten, Zeitzone, Dienste und Sicherheitsoptionen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/bsdinstall/#bsdinstall-network>2.9. Netzwerkschnittstellen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/bsdinstall/#bsdinstall-install-trouble>2.10. Fehlerbehebung</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/bsdinstall/#using-live-cd>2.11. Verwendung der Live-CD</a></li></ul></li><li><input type=checkbox id=chapter-9f6db261075f578742036fcc6000eecd class=toggle>
<label class="icon cursor" for=chapter-9f6db261075f578742036fcc6000eecd><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/basics/>Kapitel 3. Grundlagen des FreeBSD Betriebssystems</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/basics/#basics-synopsis>3.1. Übersicht</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/basics/#consoles>3.2. Virtuelle Konsolen und Terminals</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/basics/#users-synopsis>3.3. Benutzer und grundlegende Account-Verwaltung</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/basics/#permissions>3.4. Zugriffsrechte</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/basics/#dirstructure>3.5. Verzeichnis-Strukturen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/basics/#disk-organization>3.6. Festplatten, Slices und Partitionen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/basics/#mount-unmount>3.7. Anhängen und Abhängen von Dateisystemen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/basics/#basics-processes>3.8. Prozesse und Dämonen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/basics/#shells>3.9. Shells</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/basics/#editors>3.10. Text-Editoren</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/basics/#basics-devices>3.11. Geräte und Gerätedateien</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/basics/#basics-more-information>3.12. Manualpages</a></li></ul></li><li><input type=checkbox id=chapter-01c5707e95d14c0ff84bf62600c958d1 class=toggle>
<label class="icon cursor" for=chapter-01c5707e95d14c0ff84bf62600c958d1><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/ports/>Kapitel 4. Installieren von Anwendungen: Pakete und Ports</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/ports/#ports-synopsis>4.1. Übersicht</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/ports/#ports-overview>4.2. Installation von Software</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/ports/#ports-finding-applications>4.3. Suchen einer Anwendung</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/ports/#pkgng-intro>4.4. Benutzen von pkg zur Verwaltung von Binärpaketen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/ports/#ports-using>4.5. Benutzen der Ports-Sammlung</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/ports/#ports-poudriere>4.6. Pakete mit Poudriere bauen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/ports/#ports-nextsteps>4.7. Nach der Installation</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/ports/#ports-broken>4.8. Kaputte Ports</a></li></ul></li><li><input type=checkbox id=chapter-3405c00581365a8b5d16af70fe4d1b72 class=toggle>
<label class="icon cursor" for=chapter-3405c00581365a8b5d16af70fe4d1b72><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/x11/>Kapitel 5. Das X-Window-System</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/x11/#x11-synopsis>5.1. Übersicht</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/x11/#x-understanding>5.2. Terminologie</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/x11/#x-install>5.3. Xorg installieren</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/x11/#x-config>5.4. Xorg konfigurieren</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/x11/#x-fonts>5.5. Schriftarten in Xorg benutzen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/x11/#x-xdm>5.6. Der X-Display-Manager</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/x11/#x11-wm>5.7. Grafische Oberflächen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/x11/#x-compiz-fusion>5.8. Compiz Fusion installieren</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/x11/#x11-troubleshooting>5.9. Fehlersuche</a></li></ul></li><li><input type=checkbox id=chapter-0eab3565e8f59f5a8a896dfba7eb3680 class=toggle>
<label for=chapter-0eab3565e8f59f5a8a896dfba7eb3680><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/partii/>Teil II. Oft benutzte Funktionen</a></li><li><input type=checkbox id=chapter-b33cf28993f3f7bf5baf036e79da0f39 class=toggle>
<label class="icon cursor" for=chapter-b33cf28993f3f7bf5baf036e79da0f39><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/desktop/>Kapitel 6. Desktop-Anwendungen</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/desktop/#desktop-synopsis>6.1. Übersicht</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/desktop/#desktop-browsers>6.2. Browser</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/desktop/#desktop-productivity>6.3. Büroanwendungen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/desktop/#desktop-viewers>6.4. Anzeigen von Dokumenten</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/desktop/#desktop-finance>6.5. Finanzsoftware</a></li></ul></li><li><input type=checkbox id=chapter-152f694a19312ad72ec7bb4e1c3c33b2 class=toggle>
<label class="icon cursor" for=chapter-152f694a19312ad72ec7bb4e1c3c33b2><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/multimedia/>Kapitel 7. Multimedia</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/multimedia/#multimedia-synopsis>7.1. Übersicht</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/multimedia/#sound-setup>7.2. Soundkarten einrichten</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/multimedia/#sound-mp3>7.3. MP3-Audio</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/multimedia/#video-playback>7.4. Videos wiedergeben</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/multimedia/#tvcard>7.5. TV-Karten</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/multimedia/#mythtv>7.6. MythTV</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/multimedia/#scanners>7.7. Scanner</a></li></ul></li><li><input type=checkbox id=chapter-80888b4ee02e3e409e5f71cf97a36450 class=toggle>
<label class="icon cursor" for=chapter-80888b4ee02e3e409e5f71cf97a36450><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/kernelconfig/>Kapitel 8. Konfiguration des FreeBSD-Kernels</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/kernelconfig/#kernelconfig-synopsis>8.1. Übersicht</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/kernelconfig/#kernelconfig-custom-kernel>8.2. Wieso einen eigenen Kernel bauen?</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/kernelconfig/#kernelconfig-devices>8.3. Informationen über die vorhandene Hardware beschaffen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/kernelconfig/#kernelconfig-config>8.4. Die Kernelkonfigurationsdatei</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/kernelconfig/#kernelconfig-building>8.5. Einen angepassten Kernel bauen und installieren</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/kernelconfig/#kernelconfig-trouble>8.6. Wenn etwas schiefgeht</a></li></ul></li><li><input type=checkbox id=chapter-cb174c55879b17ab955f2f16989a79e0 class=toggle>
<label class="icon cursor" for=chapter-cb174c55879b17ab955f2f16989a79e0><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/printing/>Kapitel 9. Drucken</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/printing/#printing-quick-start>9.1. Schnellstart</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/printing/#printing-connections>9.2. Druckerverbindungen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/printing/#printing-pdls>9.3. Gebräuchliche Seitenbeschreibungssprachen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/printing/#printing-direct>9.4. Direktes Drucken</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/printing/#printing-lpd>9.5. LPD (Line Printer Daemon)</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/printing/#printing-other>9.6. Andere Drucksysteme</a></li></ul></li><li><input type=checkbox id=chapter-c12b8c3f2a8fcefce87087241f695c83 class=toggle>
<label class="icon cursor" for=chapter-c12b8c3f2a8fcefce87087241f695c83><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/linuxemu/>Kapitel 10. Linux®-Binärkompatibilität</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/linuxemu/#linuxemu-synopsis>10.1. Übersicht</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/linuxemu/#linuxemu-lbc-install>10.2. Konfiguration der Linux®-Binärkompatibilität</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/linuxemu/#linuxemu-advanced>10.3. Weiterführende Themen</a></li></ul></li><li><input type=checkbox id=chapter-03b11ba627b9a0c85b247f5641bde272 class=toggle>
<label for=chapter-03b11ba627b9a0c85b247f5641bde272><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/partiii/>Teil III. Systemadministration</a></li><li><input type=checkbox id=chapter-6c31587f8d736319f099cd4dc1961301 class=toggle>
<label class="icon cursor" for=chapter-6c31587f8d736319f099cd4dc1961301><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/config/>Kapitel 11. Konfiguration und Tuning</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/config/#config-synopsis>11.1. Übersicht</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/config/#configtuning-starting-services>11.2. Start von Diensten</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/config/#configtuning-cron>11.3. cron(8) konfigurieren</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/config/#configtuning-rcd>11.4. Dienste unter FreeBSD verwalten</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/config/#config-network-setup>11.5. Einrichten von Netzwerkkarten</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/config/#configtuning-virtual-hosts>11.6. Virtual Hosts</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/config/#configtuning-syslog>11.7. Konfiguration der Systemprotokollierung</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/config/#configtuning-configfiles>11.8. Konfigurationsdateien</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/config/#configtuning-sysctl>11.9. Einstellungen mit sysctl(8)</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/config/#configtuning-disk>11.10. Tuning von Laufwerken</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/config/#configtuning-kernel-limits>11.11. Einstellungen von Kernel Limits</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/config/#adding-swap-space>11.12. Hinzufügen von Swap-Bereichen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/config/#acpi-overview>11.13. Energie- und Ressourcenverwaltung</a></li></ul></li><li><input type=checkbox id=chapter-459f0012b3b4f0b6b123010f029da5e4 class=toggle>
<label class="icon cursor" for=chapter-459f0012b3b4f0b6b123010f029da5e4><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/boot/>Kapitel 12. FreeBSDs Bootvorgang</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/boot/#boot-synopsis>12.1. Übersicht</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/boot/#boot-introduction>12.2. FreeBSDs Bootvorgang</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/boot/#boot-splash>12.3. Willkommensbildschirme während des Bootvorgangs konfigurieren</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/boot/#device-hints>12.4. Konfiguration von Geräten</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/boot/#boot-shutdown>12.5. Der Shutdown-Vorgang</a></li></ul></li><li><input type=checkbox id=chapter-917c75fcffbb14d48ed6d0a48e7028f2 class=toggle>
<label class="icon cursor" for=chapter-917c75fcffbb14d48ed6d0a48e7028f2><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/security/>Kapitel 13. Sicherheit</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/security/#security-synopsis>13.1. Übersicht</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/security/#security-intro>13.2. Einführung</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/security/#one-time-passwords>13.3. Einmalpasswörter</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/security/#tcpwrappers>13.4. TCP Wrapper</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/security/#kerberos5>13.5. Kerberos</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/security/#openssl>13.6. OpenSSL</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/security/#ipsec>13.7. VPN mit IPsec</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/security/#openssh>13.8. OpenSSH</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/security/#fs-acl>13.9. Zugriffskontrolllisten für Dateisysteme (ACL)</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/security/#security-pkg>13.10. Sicherheitsprobleme in Software von Drittanbietern überwachen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/security/#security-advisories>13.11. FreeBSD Sicherheitshinweise</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/security/#security-accounting>13.12. Prozess-Überwachung</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/security/#security-resourcelimits>13.13. Einschränkung von Ressourcen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/security/#security-sudo>13.14. Gemeinsame Administration mit Sudo</a></li></ul></li><li><input type=checkbox id=chapter-1a2a8e719703649c2c66d99aa7a25fd4 class=toggle>
<label class="icon cursor" for=chapter-1a2a8e719703649c2c66d99aa7a25fd4><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/jails/>Kapitel 14. Jails</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/jails/#jails-synopsis>14.1. Übersicht</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/jails/#jails-terms>14.2. Jails - Definitionen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/jails/#jails-build>14.3. Einrichtung und Verwaltung von Jails</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/jails/#jails-tuning>14.4. Feinabstimmung und Administration</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/jails/#jails-application>14.5. Mehrere Jails aktualisieren</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/jails/#jails-ezjail>14.6. Verwaltung von Jails mit ezjail</a></li></ul></li><li><input type=checkbox id=chapter-8f4620c77e572cbb58917911a33c73cf class=toggle>
<label class="icon cursor" for=chapter-8f4620c77e572cbb58917911a33c73cf><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/mac/>Kapitel 15. Verbindliche Zugriffskontrolle</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/mac/#mac-synopsis>15.1. Übersicht</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mac/#mac-inline-glossary>15.2. Schlüsselbegriffe</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mac/#mac-initial>15.3. Erläuterung</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mac/#mac-understandlabel>15.4. MAC Labels verstehen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mac/#mac-planning>15.5. Planung eines Sicherheitsmodells</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mac/#mac-modules>15.6. Modulkonfiguration</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mac/#mac-seeotheruids>15.7. Das MAC Modul seeotheruids</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mac/#mac-bsdextended>15.8. Das MAC Modul bsdextended</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mac/#mac-ifoff>15.9. Das MAC Modul ifoff</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mac/#mac-portacl>15.10. Das MAC Modul portacl</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mac/#mac-partition>15.11. Das MAC Modul partition</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mac/#mac-mls>15.12. Das MAC Modul Multi-Level Security</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mac/#mac-biba>15.13. Das MAC Modul Biba</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mac/#mac-lomac>15.14. Das MAC Modul LOMAC</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mac/#mac-implementing>15.15. Beispiel 1: Nagios in einer MAC Jail</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mac/#mac-userlocked>15.16. Beispiel 2: User Lock Down</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mac/#mac-troubleshoot>15.17. Fehler im MAC beheben</a></li></ul></li><li><input type=checkbox id=chapter-9598d66a76cb3182057b6bcd775149a0 class=toggle>
<label class="icon cursor" for=chapter-9598d66a76cb3182057b6bcd775149a0><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/audit/>Kapitel 16. Security Event Auditing</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/audit/#audit-synopsis>16.1. Einleitung</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/audit/#audit-inline-glossary>16.2. Schlüsselbegriffe</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/audit/#audit-config>16.3. Audit Konfiguration</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/audit/#audit-administration>16.4. Audit-Trails</a></li></ul></li><li><input type=checkbox id=chapter-e1edcad13d9db6e8e4cb645d378ecfaf class=toggle>
<label class="icon cursor" for=chapter-e1edcad13d9db6e8e4cb645d378ecfaf><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/disks/>Kapitel 17. Speichermedien</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/disks/#disks-synopsis>17.1. Übersicht</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/disks/#disks-adding>17.2. Hinzufügen von Laufwerken</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/disks/#disks-growing>17.3. Partitionen vergrößern</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/disks/#usb-disks>17.4. USB Speichermedien</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/disks/#creating-cds>17.5. Erstellen und Verwenden von CDs</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/disks/#creating-dvds>17.6. DVDs benutzen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/disks/#floppies>17.7. Disketten benutzen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/disks/#backup-basics>17.8. Datensicherung</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/disks/#disks-virtual>17.9. Speicherbasierte Laufwerke</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/disks/#snapshots>17.10. Schnappschüsse von Dateisystemen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/disks/#quotas>17.11. Disk Quotas</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/disks/#disks-encrypting>17.12. Partitionen verschlüsseln</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/disks/#swap-encrypting>17.13. Den Auslagerungsspeicher verschlüsseln</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/disks/#disks-hast>17.14. Highly Available Storage (HAST)</a></li></ul></li><li><input type=checkbox id=chapter-dde37901a0e0ea32745b67607854900f class=toggle>
<label class="icon cursor" for=chapter-dde37901a0e0ea32745b67607854900f><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/geom/>Kapitel 18. GEOM: Modulares Framework zur Plattentransformation</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/geom/#geom-synopsis>18.1. Übersicht</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/geom/#geom-striping>18.2. RAID0 - Striping</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/geom/#geom-mirror>18.3. RAID1 - Spiegelung</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/geom/#geom-raid3>18.4. RAID3 - Byte-Level Striping mit dedizierter Parität</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/geom/#geom-graid>18.5. Software RAID</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/geom/#geom-ggate>18.6. GEOM Gate Netzwerk</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/geom/#geom-glabel>18.7. Das Labeln von Laufwerken</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/geom/#geom-gjournal>18.8. UFS Journaling in GEOM</a></li></ul></li><li><input type=checkbox id=chapter-73e82560fcb7145b7c0e2ec47af8fc04 class=toggle checked>
<label class="icon cursor" for=chapter-73e82560fcb7145b7c0e2ec47af8fc04><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/zfs/>Kapitel 19. Das Z-Dateisystem (ZFS)</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/zfs/#zfs-differences>19.1. Was ZFS anders macht</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/zfs/#zfs-quickstart>19.2. Schnellstartanleitung</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/zfs/#zfs-zpool>19.3. <code>zpool</code> Administration</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/zfs/#zfs-zfs>19.4. <code>zfs</code> Administration</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/zfs/#zfs-zfs-allow>19.5. Delegierbare Administration</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/zfs/#zfs-advanced>19.6. Themen für Fortgeschrittene</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/zfs/#zfs-links>19.7. Zusätzliche Informationen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/zfs/#zfs-term>19.8. ZFS-Eigenschaften und Terminologie</a></li></ul></li><li><input type=checkbox id=chapter-7af71270807eb7b70cd3eedc6577b254 class=toggle>
<label class="icon cursor" for=chapter-7af71270807eb7b70cd3eedc6577b254><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/filesystems/>Kapitel 20. Dateisystemunterstützung</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/filesystems/#filesystems-synopsis>20.1. Übersicht</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/filesystems/#filesystems-linux>20.2. Linux® Dateisysteme</a></li></ul></li><li><input type=checkbox id=chapter-375257268d95faaf87faf4f7a2e6aa67 class=toggle>
<label class="icon cursor" for=chapter-375257268d95faaf87faf4f7a2e6aa67><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/virtualization/>Kapitel 21. Virtualisierung</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/virtualization/#virtualization-synopsis>21.1. Übersicht</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/virtualization/#virtualization-guest-parallels>21.2. FreeBSD als Gast-Betriebssystem unter Parallels für Mac OS® X</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/virtualization/#virtualization-guest-virtualpc>21.3. FreeBSD als Gast-Betriebssystem unter Virtual PC für Windows®</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/virtualization/#virtualization-guest-vmware>21.4. FreeBSD als Gast-Betriebssystem unter VMware Fusion für Mac OS®</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/virtualization/#virtualization-guest-virtualbox-guest-additions>21.5. FreeBSD als Gast mit VirtualBox™</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/virtualization/#virtualization-host-virtualbox>21.6. FreeBSD als Host mit Virtualbox</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/virtualization/#virtualization-host-bhyve>21.7. FreeBSD als Host mit bhyve</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/virtualization/#virtualization-host-xen>21.8. FreeBSD als Xen™-Host</a></li></ul></li><li><input type=checkbox id=chapter-49f1e96591c090304ea532012257f4ef class=toggle>
<label class="icon cursor" for=chapter-49f1e96591c090304ea532012257f4ef><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/l10n/>Kapitel 22. Lokalisierung – I18N/L10N einrichten und benutzen</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/l10n/#l10n-synopsis>22.1. Übersicht</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/l10n/#using-localization>22.2. Lokale Anpassungen benutzen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/l10n/#l10n-compiling>22.3. I18N-Programme</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/l10n/#lang-setup>22.4. Lokalisierung für einzelne Sprachen</a></li></ul></li><li><input type=checkbox id=chapter-dead2b4c5ea325dd390a9b0dccd8f763 class=toggle>
<label class="icon cursor" for=chapter-dead2b4c5ea325dd390a9b0dccd8f763><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/cutting-edge/>Kapitel 23. FreeBSD aktualisieren</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/cutting-edge/#updating-upgrading-synopsis>23.1. Übersicht</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/cutting-edge/#updating-upgrading-freebsdupdate>23.2. FreeBSD-Update</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/cutting-edge/#updating-upgrading-documentation>23.3. Aktualisieren der Dokumentationssammlung</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/cutting-edge/#current-stable>23.4. Einem Entwicklungszweig folgen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/cutting-edge/#makeworld>23.5. FreeBSD aus den Quellen aktualisieren</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/cutting-edge/#small-lan>23.6. Installation mehrerer Maschinen</a></li></ul></li><li><input type=checkbox id=chapter-29c1eeb0e9dedc487a98399e2737ee8a class=toggle>
<label class="icon cursor" for=chapter-29c1eeb0e9dedc487a98399e2737ee8a><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/dtrace/>Kapitel 24. DTrace</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/dtrace/#dtrace-synopsis>24.1. Überblick</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/dtrace/#dtrace-implementation>24.2. Unterschiede in der Implementierung</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/dtrace/#dtrace-enable>24.3. Die DTrace Unterstützung aktivieren</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/dtrace/#dtrace-using>24.4. DTrace verwenden</a></li></ul></li><li><input type=checkbox id=chapter-913e72bfb3d6947b2869d3e9447a6eaa class=toggle>
<label class="icon cursor" for=chapter-913e72bfb3d6947b2869d3e9447a6eaa><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/usb-device-mode/>Kapitel 25. USB Gerätemodus</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/usb-device-mode/#usb-device-mode-synopsis>25.1. Übersicht</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/usb-device-mode/#usb-device-mode-terminals>25.2. Virtuelle serielle USB-Ports</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/usb-device-mode/#usb-device-mode-network>25.3. Netzwerkkarten im USB-Gerätemodus</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/usb-device-mode/#usb-device-mode-storage>25.4. Virtuelle USB-Speichergeräte</a></li></ul></li><li><input type=checkbox id=chapter-bbd25f9a194f9c39ca2d658c75767db5 class=toggle>
<label for=chapter-bbd25f9a194f9c39ca2d658c75767db5><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/partiv/>Teil IV. Netzwerke</a></li><li><input type=checkbox id=chapter-499dab596afd7ddac77e80295314e0dd class=toggle>
<label class="icon cursor" for=chapter-499dab596afd7ddac77e80295314e0dd><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/serialcomms/>Kapitel 26. Serielle Datenübertragung</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/serialcomms/#serial-synopsis>26.1. Übersicht</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/serialcomms/#serial>26.2. Begriffe und Hardware</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/serialcomms/#term>26.3. Terminals</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/serialcomms/#dialup>26.4. Einwählverbindungen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/serialcomms/#dialout>26.5. Verbindungen nach Außen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/serialcomms/#serialconsole-setup>26.6. Einrichten der seriellen Konsole</a></li></ul></li><li><input type=checkbox id=chapter-95e4571c48bee1cced5e84a538d302e3 class=toggle>
<label class="icon cursor" for=chapter-95e4571c48bee1cced5e84a538d302e3><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/ppp-and-slip/>Kapitel 27. PPP</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/ppp-and-slip/#ppp-and-slip-synopsis>27.1. Übersicht</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/ppp-and-slip/#userppp>27.2. PPP konfigurieren</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/ppp-and-slip/#ppp-troubleshoot>27.3. Probleme bei PPP-Verbindungen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/ppp-and-slip/#pppoe>27.4. PPP over Ethernet (PPPoE)</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/ppp-and-slip/#pppoa>27.5. PPP over ATM (PPPoA)</a></li></ul></li><li><input type=checkbox id=chapter-f089ac726c401c9b4bd5c34a295e11bb class=toggle>
<label class="icon cursor" for=chapter-f089ac726c401c9b4bd5c34a295e11bb><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/mail/>Kapitel 28. Elektronische Post (E-Mail)</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/mail/#mail-de-term>28.1. Terminologie</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mail/#mail-synopsis>28.2. Übersicht</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mail/#mail-using>28.3. E-Mail Komponenten</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mail/#sendmail>28.4. Sendmail-Konfigurationsdateien</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mail/#mail-changingmta>28.5. Wechseln des Mailübertragungs-Agenten</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mail/#mail-trouble>28.6. Fehlerbehebung</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mail/#mail-advanced>28.7. Weiterführende Themen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mail/#outgoing-only>28.8. Ausgehende E-Mail über einen Relay versenden</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mail/#SMTP-dialup>28.9. E-Mail über Einwahl-Verbindungen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mail/#SMTP-Auth>28.10. SMTP-Authentifizierung</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mail/#mail-agents>28.11. E-Mail-Programme</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mail/#mail-fetchmail>28.12. E-Mails mit fetchmail abholen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mail/#mail-procmail>28.13. E-Mails mit procmail filtern</a></li></ul></li><li><input type=checkbox id=chapter-6de4de3fe925639d4175ce4b6f8c1829 class=toggle>
<label class="icon cursor" for=chapter-6de4de3fe925639d4175ce4b6f8c1829><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/network-servers/>Kapitel 29. Netzwerkserver</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/network-servers/#network-servers-synopsis>29.1. Übersicht</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/network-servers/#network-inetd>29.2. Der inetd"Super-Server"</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/network-servers/#network-nfs>29.3. Network File System (NFS)</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/network-servers/#network-nis>29.4. Network Information System (NIS)</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/network-servers/#network-ldap>29.5. Lightweight Access Directory Protocol (LDAP)</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/network-servers/#network-dhcp>29.6. Dynamic Host Configuration Protocol (DHCP)</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/network-servers/#network-dns>29.7. Domain Name System (DNS)</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/network-servers/#network-apache>29.8. Apache HTTP-Server</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/network-servers/#network-ftp>29.9. File Transfer Protocol (FTP)</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/network-servers/#network-samba>29.10. Datei- und Druckserver für Microsoft® Windows®-Clients (Samba)</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/network-servers/#network-ntp>29.11. Die Uhrzeit mit NTP synchronisieren</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/network-servers/#network-iscsi>29.12. iSCSI Initiator und Target Konfiguration</a></li></ul></li><li><input type=checkbox id=chapter-776d855c7b75e048f90b5c2c9b35ffe0 class=toggle>
<label class="icon cursor" for=chapter-776d855c7b75e048f90b5c2c9b35ffe0><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/firewalls/>Kapitel 30. Firewalls</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/firewalls/#firewalls-intro>30.1. Einführung</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/firewalls/#firewalls-concepts>30.2. Firewallkonzepte</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/firewalls/#firewalls-pf>30.3. PF</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/firewalls/#firewalls-ipfw>30.4. IPFW</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/firewalls/#firewalls-ipf>30.5. IPFILTER (IPF)</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/firewalls/#firewalls-blacklistd>30.6. Blacklistd</a></li></ul></li><li><input type=checkbox id=chapter-5b07f776a0e6155c1c89aa0d15610380 class=toggle>
<label class="icon cursor" for=chapter-5b07f776a0e6155c1c89aa0d15610380><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/advanced-networking/>Kapitel 31. Weiterführende Netzwerkthemen</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/advanced-networking/#advanced-networking-synopsis>31.1. Übersicht</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/advanced-networking/#network-routing>31.2. Gateways und Routen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/advanced-networking/#network-wireless>31.3. Drahtlose Netzwerke</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/advanced-networking/#network-usb-tethering>31.4. USB Tethering</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/advanced-networking/#network-bluetooth>31.5. Bluetooth</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/advanced-networking/#network-bridging>31.6. LAN-Kopplung mit einer Bridge</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/advanced-networking/#network-aggregation>31.7. Link-Aggregation und Failover</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/advanced-networking/#network-diskless>31.8. Plattenloser Betrieb mit PXE</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/advanced-networking/#network-ipv6>31.9. IPv6</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/advanced-networking/#carp>31.10. Common Address Redundancy Protocol (CARP)</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/advanced-networking/#network-vlan>31.11. VLANs</a></li></ul></li><li><input type=checkbox id=chapter-171a77aa9d067a1024f849470e1f33e8 class=toggle>
<label for=chapter-171a77aa9d067a1024f849470e1f33e8><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/partv/>Teil V. Anhang</a></li><li><input type=checkbox id=chapter-8050f436a0a7986a4aaded93d8e49469 class=toggle>
<label class="icon cursor" for=chapter-8050f436a0a7986a4aaded93d8e49469><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/mirrors/>Anhang A. Bezugsquellen für FreeBSD</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/mirrors/#mirrors-cdrom>A.1. CD and DVD Sets</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mirrors/#mirrors-ftp>A.2. FTP-Server</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mirrors/#svn>A.3. Benutzen von Subversion</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/mirrors/#mirrors-rsync>A.4. Benutzen von rsync</a></li></ul></li><li><input type=checkbox id=chapter-128b630a8f88f158e7027fe6c2184d21 class=toggle>
<label class="icon cursor" for=chapter-128b630a8f88f158e7027fe6c2184d21><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/bibliography/>Anhang B. Bibliografie</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/bibliography/#bibliography-freebsd>B.1. Bücher speziell für FreeBSD</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/bibliography/#bibliography-userguides>B.2. Handbücher</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/bibliography/#bibliography-adminguides>B.3. Administrations-Anleitungen</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/bibliography/#bibliography-programmers>B.4. Programmierhandbücher</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/bibliography/#bibliography-osinternals>B.5. Betriebssystem-Interna</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/bibliography/#bibliography-security>B.6. Sicherheits-Anleitung</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/bibliography/#bibliography-hardware>B.7. Hardware-Anleitung</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/bibliography/#bibliography-history>B.8. UNIX® Geschichte</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/bibliography/#bibliography-journals>B.9. Zeitschriften, Magazine und Journale</a></li></ul></li><li><input type=checkbox id=chapter-8bbb8867c46dac315e2253945d8c18a8 class=toggle>
<label class="icon cursor" for=chapter-8bbb8867c46dac315e2253945d8c18a8><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/eresources/>Anhang C. Ressourcen im Internet</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/eresources/#eresources-www>C.1. Webseiten</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/eresources/#eresources-mail>C.2. Mailinglisten</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/eresources/#eresources-news>C.3. Usenet-News</a></li><li><a href=http://172.16.201.134:1313/de/books/handbook/eresources/#eresources-web>C.4. Offizielle Spiegel</a></li></ul></li><li><input type=checkbox id=chapter-a80ea4f5a4480b8725422710f954ef36 class=toggle>
<label class="icon cursor" for=chapter-a80ea4f5a4480b8725422710f954ef36><a role=button></a></label><a href=http://172.16.201.134:1313/de/books/handbook/pgpkeys/>Anhang D. OpenPGP-Schlüssel</a><ul><li><a href=http://172.16.201.134:1313/de/books/handbook/pgpkeys/#pgpkeys-officers>D.1. Ansprechpartner</a></li></ul></li><li></li></ul></nav></div></aside><div class=book><div class=book-menu-mobile><label for=menu-control><span class=menu-control-button><i class="fa fa-list" aria-hidden=true title="Book menu"></i>
Book menu</span></label></div><h1 class=title>Kapitel 19. Das Z-Dateisystem (ZFS)</h1><div class="admonitionblock note"><p><i class="fa fa-exclamation-circle" aria-hidden=true></i>
This translation may be out of date. To help with the translations please access the <a href=https://translate-dev.freebsd.org/ target=_blank>FreeBSD translations instance</a>.</p></div><div class=toc-mobile><h3>Table of Contents</h3><nav id=TableOfContents><ul><li><a href=#zfs-differences>19.1. Was ZFS anders macht</a></li><li><a href=#zfs-quickstart>19.2. Schnellstartanleitung</a></li><li><a href=#zfs-zpool>19.3. <code>zpool</code> Administration</a></li><li><a href=#zfs-zfs>19.4. <code>zfs</code> Administration</a></li><li><a href=#zfs-zfs-allow>19.5. Delegierbare Administration</a></li><li><a href=#zfs-advanced>19.6. Themen für Fortgeschrittene</a></li><li><a href=#zfs-links>19.7. Zusätzliche Informationen</a></li><li><a href=#zfs-term>19.8. ZFS-Eigenschaften und Terminologie</a></li></ul></nav></div><div class=book-content><div id=preamble><div class=sectionbody><div class=paragraph><p>Das <em>Z-Dateisystem</em>, oder kurz ZFS, ist ein fortgeschrittenes Dateisystem, das entwickelt wurde, um viele der großen Probleme in vorherigen Entwicklungen zu überwinden.</p></div><div class=paragraph><p>Ursprünglich von Sun™ entworfen, wird die weitere Entwicklung von ZFS heutzutage als Open Source vom <a href=http://open-zfs.org>OpenZFS Projekt</a> vorangetrieben.</p></div><div class=paragraph><p>ZFS hat drei große Entwurfsziele:</p></div><div class=ulist><ul><li><p>Datenintegrität: Alle Daten enthalten eine Prüfsumme (<a href=#zfs-term-checksum>checksum</a>) der Daten. Wenn Daten geschrieben werden, wird die Prüfsumme berechnet und zusammen mit den Daten gespeichert. Wenn diese Daten später wieder eingelesen werden, wird diese Prüfsumme erneut berechnet. Falls die Prüfsummen nicht übereinstimmen, wurde ein Datenfehler festgestellt. ZFS wird versuchen, diesen Fehler automatisch zu korrigieren, falls genug Datenredundanz vorhanden ist.</p></li><li><p>Gepoolter Speicher: physikalische Speichermedien werden zu einem Pool zusammengefasst und der Speicherplatz wird von diesem gemeinsam genutzten Pool allokiert. Der Speicherplatz steht allen Dateisystemen zur Verfügung und kann durch das Hinzufügen von neuen Speichermedien vergrößert werden.</p></li><li><p>Geschwindigkeit: mehrere Zwischenspeichermechanismen sorgen für erhöhte Geschwindigkeit. Der <a href=#zfs-term-arc>ARC</a> ist ein weiterentwickelter, hauptspeicherbasierter Zwischenspeicher für Leseanfragen. Auf einer zweiten Stufe kann ein plattenbasierter <a href=#zfs-term-l2arc>L2ARC</a>-Lesezwischenspeicher hinzugefügt werden. Zusätzlich ist auch noch ein plattenbasierter, synchroner Schreibzwischenspeicher verfügbar, der sog. <a href=#zfs-term-zil>ZIL</a>.</p></li></ul></div><div class=paragraph><p>Eine vollständige Liste aller Eigenschaften und der dazugehörigen Terminologie ist in <a href=#zfs-term>ZFS-Eigenschaften und Terminologie</a> zu sehen.</p></div></div></div><div class=sect1><h2 id=zfs-differences>19.1. Was ZFS anders macht<a class=anchor href=#zfs-differences></a></h2><div class=sectionbody><div class=paragraph><p>ZFS ist signifikant unterschiedlich zu allen bisherigen Dateisystemen, weil es mehr als nur ein Dateisystem ist. Durch die Kombination von traditionell getrennten Rollen von Volumenmanager und Dateisystem ist ZFS mit einzigartigen Vorteilen ausgestattet. Das Dateisystem besitzt jetzt Kenntnis von der zugrundeliegenden Struktur der Speichermedien. Traditionelle Dateisysteme konnten nur auf einer einzigen Platte gleichzeitig angelegt werden. Falls es zwei Festplatten gab, mussten auch zwei getrennte Dateisysteme erstellt werden. In einer traditionellen Hardware-RAID-Konfiguration wurde dieses Problem umgangen, indem dem Betriebssystem nur eine einzige logische Platte angezeigt wurde, die sich aus dem Speicherplatz von der Anzahl an physischen Platten zusammensetzte, auf dem dann das Betriebssystem ein Dateisystem erstellte. Sogar im Fall von Software-RAID-Lösungen, wie die, die von GEOM bereitgestellt werden, war das UFS-Dateisystem der Ansicht, dass es auf nur einem einzigen Gerät angelegt wurde. ZFS’s Kombination eines Volumenmanagers und eines Dateisystems löst dies und erlaubt das Erstellen von vielen Dateisystemen, die sich alle den darunterliegenden Pool aus verfügbarem Speicher teilen. Einer der größten Vorteile von ZFS’s Kenntnis des physikalischen Layouts der Platten ist, dass existierende Dateisysteme automatisch wachsen können, wenn zusätzliche Platten zum Pool hinzugefügt werden. Dieser neue Speicherplatz wird dann allen Dateisystemen zur Verfügung gestellt. ZFS besitzt ebenfalls eine Menge an unterschiedlichen Eigenschaften, die für jedes Dateisystem angepasst werden können, was viele Vorteile bringt, wenn man unterschiedliche Dateisysteme und Datasets anlegt, anstatt ein einziges, monolithisches Dateisystem zu erzeugen.</p></div></div></div><div class=sect1><h2 id=zfs-quickstart>19.2. Schnellstartanleitung<a class=anchor href=#zfs-quickstart></a></h2><div class=sectionbody><div class=paragraph><p>Es existiert ein Startmechanismus, der es FreeBSD erlaubt, ZFS-Pools während der Systeminitialisierung einzubinden. Um diesen zu aktivieren, fügen Sie diese Zeile in <span class=filename>/etc/rc.conf</span> ein:</p></div><div class="literalblock programlisting"><div class=content><pre>zfs_enable=&#34;YES&#34;</pre></div></div><div class=paragraph><p>Starten Sie dann den Dienst:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># service zfs start</span></code></pre></div></div><div class=paragraph><p>Die Beispiele in diesem Abschnitt gehen von drei SCSI-Platten mit den Gerätenamen <span class=filename>da0</span>, <span class=filename>da1</span> und <span class=filename>da2</span> aus. Nutzer von SATA-Hardware sollten stattdessen die Bezeichnung <span class=filename>ada</span> als Gerätenamen verwenden.</p></div><div class=sect2><h3 id=zfs-quickstart-single-disk-pool>19.2.1. Pools mit einer Platte<a class=anchor href=#zfs-quickstart-single-disk-pool></a></h3><div class=paragraph><p>Um einen einfachen, nicht-redundanten Pool mit einem einzigen Gerät anzulegen, geben Sie folgendes ein:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool create example /dev/da0</span></code></pre></div></div><div class=paragraph><p>Um den neuen Pool anzuzeigen, prüfen Sie die Ausgabe von <code>df</code>:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># df</span>
Filesystem  1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a   2026030  235230  1628718    13%    /
devfs               1       1        0   100%    /dev
/dev/ad0s1d  54098308 1032846 48737598     2%    /usr
example      17547136       0 17547136     0%    /example</code></pre></div></div><div class=paragraph><p>Diese Ausgabe zeigt, dass der <code>example</code>-Pool erstellt und eingehängt wurde. Er ist nun als Dateisystem verfügbar. Dateien können darauf angelegt werden und Anwender können sich den Inhalt ansehen:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># cd /example</span>
<span class=c># ls</span>
<span class=c># touch testfile</span>
<span class=c># ls -al</span>
total 4
drwxr-xr-x   2 root  wheel    3 Aug 29 23:15 <span class=nb>.</span>
drwxr-xr-x  21 root  wheel  512 Aug 29 23:12 ..
<span class=nt>-rw-r--r--</span>   1 root  wheel    0 Aug 29 23:15 testfile</code></pre></div></div><div class=paragraph><p>Allerdings nutzt dieser Pool noch keine der Vorteile von ZFS. Um ein Dataset auf diesem Pool mit aktivierter Komprimierung zu erzeugen, geben Sie ein:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs create example/compressed</span>
<span class=c># zfs set compression=gzip example/compressed</span></code></pre></div></div><div class=paragraph><p>Das <code>example/compressed</code>-Dataset ist nun ein komprimiertes ZFS-Dateisystem. Versuchen Sie, ein paar große Dateien auf <span class=filename>/example/compressed</span> zu kopieren.</p></div><div class=paragraph><p>Deaktivieren lässt sich die Komprimierung durch:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set compression=off example/compressed</span></code></pre></div></div><div class=paragraph><p>Um ein Dateisystem abzuhängen, verwenden Sie <code>zfs umount</code> und überprüfen Sie dies anschließend mit <code>df</code>:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs umount example/compressed</span>
<span class=c># df</span>
Filesystem  1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a   2026030  235232  1628716    13%    /
devfs               1       1        0   100%    /dev
/dev/ad0s1d  54098308 1032864 48737580     2%    /usr
example      17547008       0 17547008     0%    /example</code></pre></div></div><div class=paragraph><p>Um das Dateisystem wieder einzubinden und erneut verfügbar zu machen, verwenden Sie <code>zfs mount</code> und prüfen Sie erneut mit <code>df</code>:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs mount example/compressed</span>
<span class=c># df</span>
Filesystem         1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a          2026030  235234  1628714    13%    /
devfs                      1       1        0   100%    /dev
/dev/ad0s1d         54098308 1032864 48737580     2%    /usr
example             17547008       0 17547008     0%    /example
example/compressed  17547008       0 17547008     0%    /example/compressed</code></pre></div></div><div class=paragraph><p>Den Pool und die Dateisysteme können Sie auch über die Ausgabe von <code>mount</code> prüfen:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># mount</span>
/dev/ad0s1a on / <span class=o>(</span>ufs, <span class=nb>local</span><span class=o>)</span>
devfs on /dev <span class=o>(</span>devfs, <span class=nb>local</span><span class=o>)</span>
/dev/ad0s1d on /usr <span class=o>(</span>ufs, <span class=nb>local</span>, soft-updates<span class=o>)</span>
example on /example <span class=o>(</span>zfs, <span class=nb>local</span><span class=o>)</span>
example/compressed on /example/compressed <span class=o>(</span>zfs, <span class=nb>local</span><span class=o>)</span></code></pre></div></div><div class=paragraph><p>Nach der Erstellung können ZFS-Datasets wie jedes andere Dateisystem verwendet werden. Jedoch sind jede Menge andere Besonderheiten verfügbar, die individuell auf Dataset-Basis eingestellt sein können. Im Beispiel unten wird ein neues Dateisystem namens <code>data</code> angelegt. Wichtige Dateien werden dort abgespeichert, deshalb wird es so konfiguriert, dass zwei Kopien jedes Datenblocks vorgehalten werden.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs create example/data</span>
<span class=c># zfs set copies=2 example/data</span></code></pre></div></div><div class=paragraph><p>Es ist jetzt möglich, den Speicherplatzverbrauch der Daten durch die Eingabe von <code>df</code> zu sehen:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># df</span>
Filesystem         1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a          2026030  235234  1628714    13%    /
devfs                      1       1        0   100%    /dev
/dev/ad0s1d         54098308 1032864 48737580     2%    /usr
example             17547008       0 17547008     0%    /example
example/compressed  17547008       0 17547008     0%    /example/compressed
example/data        17547008       0 17547008     0%    /example/data</code></pre></div></div><div class=paragraph><p>Sie haben vermutlich bemerkt, dass jedes Dateisystem auf dem Pool die gleiche Menge an verfügbarem Speicherplatz besitzt. Das ist der Grund dafür, dass in diesen Beispielen <code>df</code> verwendet wird, um zu zeigen, dass die Dateisysteme nur die Menge an Speicher verbrauchen, den sie benötigen und alle den gleichen Pool verwenden. ZFS eliminiert Konzepte wie Volumen und Partitionen und erlaubt es mehreren Dateisystemen den gleichen Pool zu belegen.</p></div><div class=paragraph><p>Um das Dateisystem und anschließend den Pool zu zerstören, wenn dieser nicht mehr benötigt wird, geben Sie ein:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs destroy example/compressed</span>
<span class=c># zfs destroy example/data</span>
<span class=c># zpool destroy example</span></code></pre></div></div></div><div class=sect2><h3 id=zfs-quickstart-raid-z>19.2.2. RAID-Z<a class=anchor href=#zfs-quickstart-raid-z></a></h3><div class=paragraph><p>Platten fallen aus. Eine Methode, um Datenverlust durch Festplattenausfall zu vermeiden, ist die Verwendung von RAID. ZFS unterstützt dies in seiner Poolgestaltung. Pools mit RAID-Z benötigen drei oder mehr Platten, bieten aber auch mehr nutzbaren Speicher als gespiegelte Pools.</p></div><div class=paragraph><p>Dieses Beispiel erstellt einen RAID-Z-Pool, indem es die Platten angibt, die dem Pool hinzugefügt werden sollen:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool create storage raidz da0 da1 da2</span></code></pre></div></div><div class="admonitionblock note"><table><tbody><tr><td class=icon><i class="fa icon-note" title=Note></i></td><td class=content><div class=paragraph><p>Sun™ empfiehlt, dass die Anzahl der Geräte in einer RAID-Z Konfiguration zwischen drei und neun beträgt. Für Umgebungen, die einen einzelnen Pool benötigen, der aus 10 oder mehr Platten besteht, sollten Sie in Erwägung ziehen, diesen in kleinere RAID-Z-Gruppen aufzuteilen. Falls nur zwei Platten verfügbar sind und Redundanz benötigt wird, ziehen Sie die Verwendung eines ZFS-Spiegels (mirror) in Betracht. Lesen Sie dazu <a href="https://man.freebsd.org/cgi/man.cgi?query=zpool&amp;sektion=8&amp;format=html">zpool(8)</a>, um weitere Details zu erhalten.</p></div></td></tr></tbody></table></div><div class=paragraph><p>Das vorherige Beispiel erstellte einen ZPool namens <code>storage</code>. Dieses Beispiel erzeugt ein neues Dateisystem, genannt <code>home</code>, in diesem Pool:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs create storage/home</span></code></pre></div></div><div class=paragraph><p>Komprimierung und das Vorhalten von mehreren Kopien von Dateien und Verzeichnissen kann aktiviert werden:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set copies=2 storage/home</span>
<span class=c># zfs set compression=gzip storage/home</span></code></pre></div></div><div class=paragraph><p>Um dies als das neue Heimatverzeichnis für Anwender zu setzen, kopieren Sie die Benutzerdaten in dieses Verzeichnis und erstellen passende symbolische Verknüpfungen:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># cp -rp /home/* /storage/home</span>
<span class=c># rm -rf /home /usr/home</span>
<span class=c># ln -s /storage/home /home</span>
<span class=c># ln -s /storage/home /usr/home</span></code></pre></div></div><div class=paragraph><p>Daten von Anwendern werden nun auf dem frisch erstellten <span class=filename>/storage/home</span> abgelegt. Überprüfen Sie dies durch das Anlegen eines neuen Benutzers und das anschließende Anmelden als dieser Benutzer.</p></div><div class=paragraph><p>Versuchen Sie, einen Dateisystemschnappschuss anzulegen, den Sie später wieder zurückrollen können:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs snapshot storage/home@08-30-08</span></code></pre></div></div><div class=paragraph><p>Schnappschüsse können nur auf einem Dateisystem angelegt werden, nicht auf einem einzelnen Verzeichnis oder einer Datei.</p></div><div class=paragraph><p>Das Zeichen <code>@</code> ist der Trenner zwischen dem Dateisystem- oder dem Volumennamen. Wenn ein wichtiges Verzeichnis aus Versehen gelöscht wurde, kann das Dateisystem gesichert und dann zu einem früheren Schnappschuss zurückgerollt werden, in welchem das Verzeichnis noch existiert:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs rollback storage/home@08-30-08</span></code></pre></div></div><div class=paragraph><p>Um all verfügbaren Schnappschüsse aufzulisten, geben Sie <code>ls</code> im Verzeichnis <span class=filename>.zfs/snapshot</span> dieses Dateisystems ein. Beispielsweise lässt sich der zuvor angelegte Schnappschuss wie folgt anzeigen:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># ls /storage/home/.zfs/snapshot</span></code></pre></div></div><div class=paragraph><p>Es ist möglich, ein Skript zu schreiben, um regelmäßig Schnappschüsse von Benutzerdaten anzufertigen. Allerdings verbrauchen Schnappschüsse über lange Zeit eine große Menge an Speicherplatz. Der zuvor angelegte Schnappschuss kann durch folgendes Kommando wieder entfernt werden:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs destroy storage/home@08-30-08</span></code></pre></div></div><div class=paragraph><p>Nach erfolgreichen Tests kann <span class=filename>/storage/home</span> zum echten <span class=filename>/home</span>-Verzeichnis werden, mittels:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set mountpoint=/home storage/home</span></code></pre></div></div><div class=paragraph><p>Prüfen Sie mit <code>df</code> und <code>mount</code>, um zu bestätigen, dass das System das Dateisystem nun als <span class=filename>/home</span> verwendet:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># mount</span>
/dev/ad0s1a on / <span class=o>(</span>ufs, <span class=nb>local</span><span class=o>)</span>
devfs on /dev <span class=o>(</span>devfs, <span class=nb>local</span><span class=o>)</span>
/dev/ad0s1d on /usr <span class=o>(</span>ufs, <span class=nb>local</span>, soft-updates<span class=o>)</span>
storage on /storage <span class=o>(</span>zfs, <span class=nb>local</span><span class=o>)</span>
storage/home on /home <span class=o>(</span>zfs, <span class=nb>local</span><span class=o>)</span>
<span class=c># df</span>
Filesystem   1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a    2026030  235240  1628708    13%    /
devfs                1       1        0   100%    /dev
/dev/ad0s1d   54098308 1032826 48737618     2%    /usr
storage       26320512       0 26320512     0%    /storage
storage/home  26320512       0 26320512     0%    /home</code></pre></div></div><div class=paragraph><p>Damit ist die RAID-Z Konfiguration abgeschlossen. Tägliche Informationen über den Status der erstellten Dateisysteme können als Teil des nächtlichen <a href="https://man.freebsd.org/cgi/man.cgi?query=periodic&amp;sektion=8&amp;format=html">periodic(8)</a>-Berichts generiert werden. Fügen Sie dazu die folgende Zeile in <span class=filename>/etc/periodic.conf</span> ein:</p></div><div class="literalblock programlisting"><div class=content><pre>daily_status_zfs_enable=&#34;YES&#34;</pre></div></div></div><div class=sect2><h3 id=zfs-quickstart-recovering-raid-z>19.2.3. RAID-Z wiederherstellen<a class=anchor href=#zfs-quickstart-recovering-raid-z></a></h3><div class=paragraph><p>Jedes Software-RAID besitzt eine Methode, um den Zustand (<code>state</code>) zu überprüfen. Der Status von RAID-Z Geräten wird mit diesem Befehl angezeigt:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool status -x</span></code></pre></div></div><div class=paragraph><p>Wenn alle Pools <a href=#zfs-term-online>Online</a> sind und alles normal ist, zeigt die Meldung folgendes an:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell>all pools are healthy</code></pre></div></div><div class=paragraph><p>Wenn es ein Problem gibt, womöglich ist eine Platte im Zustand <a href=#zfs-term-offline>Offline</a>, dann wird der Poolzustand ähnlich wie dieser aussehen:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell>  pool: storage
 state: DEGRADED
status: One or more devices has been taken offline by the administrator.
	Sufficient replicas exist <span class=k>for </span>the pool to <span class=k>continue </span>functioning <span class=k>in </span>a
	degraded state.
action: Online the device using <span class=s1>&#39;zpool online&#39;</span> or replace the device with
	<span class=s1>&#39;zpool replace&#39;</span><span class=nb>.</span>
 scrub: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	storage     DEGRADED     0     0     0
	  raidz1    DEGRADED     0     0     0
	    da0     ONLINE       0     0     0
	    da1     OFFLINE      0     0     0
	    da2     ONLINE       0     0     0

errors: No known data errors</code></pre></div></div><div class=paragraph><p>Dies zeigt an, dass das Gerät zuvor vom Administrator mit diesem Befehl abgeschaltet wurde:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool offline storage da1</span></code></pre></div></div><div class=paragraph><p>Jetzt kann das System heruntergefahren werden, um <span class=filename>da1</span> zu ersetzen. Wenn das System wieder eingeschaltet wird, kann die fehlerhafte Platte im Pool ersetzt werden:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool replace storage da1</span></code></pre></div></div><div class=paragraph><p>Von diesem Punkt an kann der Status erneut geprüft werden. Dieses Mal ohne die Option <code>-x</code>, damit alle Pools angezeigt werden:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool status storage</span>
 pool: storage
 state: ONLINE
 scrub: resilver completed with 0 errors on Sat Aug 30 19:44:11 2008
config:

	NAME        STATE     READ WRITE CKSUM
	storage     ONLINE       0     0     0
	  raidz1    ONLINE       0     0     0
	    da0     ONLINE       0     0     0
	    da1     ONLINE       0     0     0
	    da2     ONLINE       0     0     0

errors: No known data errors</code></pre></div></div><div class=paragraph><p>In diesem Beispiel ist alles normal.</p></div></div><div class=sect2><h3 id=zfs-quickstart-data-verification>19.2.4. Daten verifizieren<a class=anchor href=#zfs-quickstart-data-verification></a></h3><div class=paragraph><p>ZFS verwendet Prüfsummen, um die Integrität der gespeicherten Daten zu gewährleisten. Dies wird automatisch beim Erstellen von Dateisystemen aktiviert.</p></div><div class="admonitionblock warning"><table><tbody><tr><td class=icon><i class="fa icon-warning" title=Warning></i></td><td class=content><div class=paragraph><p>Prüfsummen können deaktiviert werden, dies wird jedoch <em>nicht</em> empfohlen! Prüfsummen verbrauchen nur sehr wenig Speicherplatz und sichern die Integrität der Daten. Viele Eigenschaften vom ZFS werden nicht richtig funktionieren, wenn Prüfsummen deaktiviert sind. Es gibt keinen merklichen Geschwindigkeitsunterschied durch das Deaktivieren dieser Prüfsummen.</p></div></td></tr></tbody></table></div><div class=paragraph><p>Prüfsummenverifikation ist unter der Bezeichnung <em>scrubbing</em> bekannt. Verifizieren Sie die Integrität der Daten des <code>storage</code>-Pools mit diesem Befehl:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool scrub storage</span></code></pre></div></div><div class=paragraph><p>Die Laufzeit einer Überprüfung hängt ab von der Menge an Daten, die gespeichert sind. Größere Mengen an Daten benötigen proportional mehr Zeit zum überprüfen. Diese Überprüfungen sind sehr I/O-intensiv und es kann auch nur eine Überprüfung zur gleichen Zeit durchgeführt werden. Nachdem eine Prüfung beendet ist, kann der Status mit dem Unterkommando <code>status</code> angezeigt werden:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool status storage</span>
 pool: storage
 state: ONLINE
 scrub: scrub completed with 0 errors on Sat Jan 26 19:57:37 2013
config:

	NAME        STATE     READ WRITE CKSUM
	storage     ONLINE       0     0     0
	  raidz1    ONLINE       0     0     0
	    da0     ONLINE       0     0     0
	    da1     ONLINE       0     0     0
	    da2     ONLINE       0     0     0

errors: No known data errors</code></pre></div></div><div class=paragraph><p>Das Datum der letzten Prüfoperation wird angezeigt, um zu verfolgen, wann die nächste Prüfung benötigt wird. Routinemässige Überprüfungen helfen dabei, Daten vor stiller Korrumpierung zu schützen und die Integrität des Pools sicher zu stellen.</p></div><div class=paragraph><p>Lesen Sie <a href="https://man.freebsd.org/cgi/man.cgi?query=zfs&amp;sektion=8&amp;format=html">zfs(8)</a> und <a href="https://man.freebsd.org/cgi/man.cgi?query=zpool&amp;sektion=8&amp;format=html">zpool(8)</a>, um über weitere ZFS-Optionen zu erfahren.</p></div></div></div></div><div class=sect1><h2 id=zfs-zpool>19.3. <code>zpool</code> Administration<a class=anchor href=#zfs-zpool></a></h2><div class=sectionbody><div class=paragraph><p>Administration von ZFS ist unterteilt zwischen zwei Hauptkommandos. Das <code>zpool</code>-Werkzeug steuert die Operationen des Pools und kümmert sich um das Hinzufügen, entfernen, ersetzen und verwalten von Platten. Mit dem <a href=#zfs-zfs><code>zfs</code></a>-Befehl können Datasets erstellt, zerstört und verwaltet werden, sowohl <a href=#zfs-term-filesystem>Dateisysteme</a> als auch <a href=#zfs-term-volume>Volumes</a>.</p></div><div class=sect2><h3 id=zfs-zpool-create>19.3.1. Pools anlegen und zerstören<a class=anchor href=#zfs-zpool-create></a></h3><div class=paragraph><p>Einen ZFS-Pool (<em>zpool</em>) anzulegen beinhaltet das Treffen von einer Reihe von Entscheidungen, die relativ dauerhaft sind, weil die Struktur des Pools nachdem er angelegt wurde, nicht mehr geändert werden kann. Die wichtigste Entscheidung ist, welche Arten von vdevs als physische Platten zusammengefasst werden soll. Sehen Sie sich dazu die Liste von <a href=#zfs-term-vdev>vdev-Arten</a> an, um Details zu möglichen Optionen zu bekommen. Nachdem der Pool angelegt wurde, erlauben die meisten vdev-Arten es nicht mehr, weitere Geräte zu diesem vdev hinzuzufügen. Die Ausnahme sind Spiegel, die das Hinzufügen von weiteren Platten zum vdev gestatten, sowie stripes, die zu Spiegeln umgewandelt werden können, indem man zusätzliche Platten zum vdev anhängt. Obwohl weitere vdevs eingefügt werden können, um einen Pool zu vergrößern, kann das Layout des Pools nach dem Anlegen nicht mehr verändert werden. Stattdessen müssen die Daten gesichert, der Pool zerstört und danach neu erstellt werden.</p></div><div class=paragraph><p>Erstellen eines einfachen gespiegelten Pools:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool create mypool mirror /dev/ada1 /dev/ada2</span>
<span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada1    ONLINE       0     0     0
            ada2    ONLINE       0     0     0

errors: No known data errors</code></pre></div></div><div class=paragraph><p>Mehrere vdevs können gleichzeitig angelegt werden. Geben Sie zusätzliche Gruppen von Platten, getrennt durch das vdev-Typ Schlüsselwort, in diesem Beispiel <code>mirror</code>, an:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool create mypool mirror /dev/ada1 /dev/ada2 mirror /dev/ada3 /dev/ada4</span>
  pool: mypool
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada1    ONLINE       0     0     0
            ada2    ONLINE       0     0     0
          mirror-1  ONLINE       0     0     0
            ada3    ONLINE       0     0     0
            ada4    ONLINE       0     0     0

errors: No known data errors</code></pre></div></div><div class=paragraph><p>Pools lassen sich auch durch die Angabe von Partitionen anstatt von ganzen Platten erzeugen. Durch die Verwendung von ZFS in einer separaten Partition ist es möglich, dass die gleiche Platte andere Partitionen für andere Zwecke besitzen kann. Dies ist besonders von Interesse, wenn Partitionen mit Bootcode und Dateisysteme, die zum starten benötigt werden, hinzugefügt werden können. Das erlaubt es, von Platten zu booten, die auch Teil eines Pools sind. Es gibt keinen Geschwindigkeitsnachteil unter FreeBSD wenn eine Partition anstatt einer ganzen Platte verwendet wird. Durch den Einsatz von Partitionen kann der Administrator die Platten <em>unter provisionieren</em>, indem weniger als die volle Kapazität Verwendung findet. Wenn in Zukunft eine Ersatzfestplatte mit der gleichen Größe als die Originalplatte eine kleinere Kapazität aufweist, passt die kleinere Partition immer noch und die Ersatzplatte kann immer noch verwendet werden.</p></div><div class=paragraph><p>Erstellen eines <a href=#zfs-term-vdev-raidz>RAID-Z2</a>-Pools mit Partitionen:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool create mypool raidz2 /dev/ada0p3 /dev/ada1p3 /dev/ada2p3 /dev/ada3p3 /dev/ada4p3 /dev/ada5p3</span>
<span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          raidz2-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0
            ada3p3  ONLINE       0     0     0
            ada4p3  ONLINE       0     0     0
            ada5p3  ONLINE       0     0     0

errors: No known data errors</code></pre></div></div><div class=paragraph><p>Ein Pool, der nicht länger benötigt wird, kann zerstört werden, so dass die Platten für einen anderen Einsatzzweck Verwendung finden können. Um einen Pool zu zerstören, müssen zuerst alle Datasets in diesem Pool abgehängt werden. Wenn die Datasets verwendet werden, wird das Abhängen fehlschlagen und der Pool nicht zerstört. Die Zerstörung des Pools kann erzwungen werden durch die Angabe der Option <code>-f</code>, jedoch kann dies undefiniertes Verhalten in den Anwendungen auslösen, die noch offene Dateien auf diesen Datasets hatten.</p></div></div><div class=sect2><h3 id=zfs-zpool-attach>19.3.2. Hinzufügen und Löschen von Geräten<a class=anchor href=#zfs-zpool-attach></a></h3><div class=paragraph><p>Es gibt zwei Fälle für das Hinzufügen von Platten zu einem Pool: einhängen einer Platte zu einem existierenden vdev mit <code>zpool attach</code> oder einbinden von vdevs zum Pool mit <code>zpool add</code>. Nur manche <a href=#zfs-term-vdev>vdev-Arten</a> gestatten es, Platten zum vdev hinzuzufügen, nachdem diese angelegt wurden.</p></div><div class=paragraph><p>Ein Pool mit nur einer einzigen Platte besitzt keine Redundanz. Datenverfälschung kann erkannt, aber nicht repariert werden, weil es keine weiteren Kopien der Daten gibt. Die Eigenschaft <a href=#zfs-term-copies>copies</a> kann genutzt werden, um einen geringen Fehler wie einen beschädigtem Sektor auszumerzen, enthält aber nicht die gleiche Art von Schutz, die Spiegelung oder RAID-Z bieten. Wenn man mit einem Pool startet, der nur aus einer einzigen vdev-Platte besteht, kann mit dem Kommando <code>zpool attach</code> eine zustätzliche Platte dem vdev hinzugefügt werden, um einen Spiegel zu erzeugen. Mit <code>zpool attach</code> können auch zusätzliche Platten zu einer Spiegelgruppe eingefügt werden, was die Redundanz und Lesegeschwindigkeit steigert. Wenn die Platten, aus denen der Pool besteht, partitioniert sind, replizieren Sie das Layout der ersten Platte auf die Zweite. Verwenden Sie dazu <code>gpart backup</code> und <code>gpart restore</code>, um diesen Vorgang einfacher zu gestalten.</p></div><div class=paragraph><p>Umwandeln eines (stripe) vdevs namens <em>ada0p3</em> mit einer einzelnen Platte zu einem Spiegel durch das Einhängen von <em>ada1p3</em>:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          ada0p3    ONLINE       0     0     0

errors: No known data errors
<span class=c># zpool attach mypool ada0p3 ada1p3</span>
Make sure to <span class=nb>wait </span><span class=k>until </span>resilver is <span class=k>done </span>before rebooting.

If you boot from pool <span class=s1>&#39;mypool&#39;</span>, you may need to update
boot code on newly attached disk <span class=s1>&#39;ada1p3&#39;</span><span class=nb>.</span>

Assuming you use GPT partitioning und <span class=s1>&#39;da0&#39;</span> is your new boot disk
you may use the following <span class=nb>command</span>:

        gpart bootcode <span class=nt>-b</span> /boot/pmbr <span class=nt>-p</span> /boot/gptzfsboot <span class=nt>-i</span> 1 da0
<span class=c># gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 ada1</span>
bootcode written to ada1
<span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
status: One or more devices is currently being resilvered.  The pool will
        <span class=k>continue </span>to <span class=k>function</span>, possibly <span class=k>in </span>a degraded state.
action: Wait <span class=k>for </span>the resilver to complete.
  scan: resilver <span class=k>in </span>progress since Fri May 30 08:19:19 2014
        527M scanned out of 781M at 47.9M/s, 0h0m to go
        527M resilvered, 67.53% <span class=k>done
</span>config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0  <span class=o>(</span>resilvering<span class=o>)</span>

errors: No known data errors
<span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: resilvered 781M <span class=k>in </span>0h0m with 0 errors on Fri May 30 08:15:58 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0

errors: No known data errors</code></pre></div></div><div class=paragraph><p>Wenn das Hinzufügen von Platten zu einem vdev keine Option wie für RAID-Z ist, gibt es eine Alternative, nämlich einen anderen vdev zum Pool hinzuzufügen. Zusätzliche vdevs bieten höhere Geschwindigkeit, indem Schreibvorgänge über die vdevs verteilt werden. Jedes vdev ist dafür verantwortlich, seine eigene Redundanz sicherzustellen. Es ist möglich, aber nicht empfehlenswert, vdev-Arten zu mischen, wie zum Beispiel <code>mirror</code> und <code>RAID-Z</code>. Durch das Einfügen eines nicht-redundanten vdev zu einem gespiegelten Pool oder einem RAID-Z vdev riskiert man die Daten des gesamten Pools. Schreibvorgänge werden verteilt, deshalb ist der Ausfall einer nicht-redundanten Platte mit dem Verlust eines Teils von jedem Block verbunden, der auf den Pool geschrieben wird.</p></div><div class=paragraph><p>Daten werden über jedes vdev gestriped. Beispielsweise sind zwei Spiegel-vdevs effektiv ein RAID 10, dass über zwei Sets von Spiegeln die Daten schreibt. Speicherplatz wird so allokiert, dass jedes vdev zur gleichen Zeit vollgeschrieben wird. Es gibt einen Geschwindigkeitsnachteil wenn die vdevs unterschiedliche Menge von freiem Speicher aufweisen, wenn eine unproportionale Menge an Daten auf das weniger volle vdev geschrieben wird.</p></div><div class=paragraph><p>Wenn zusätzliche Geräte zu einem Pool, von dem gebootet wird, hinzugefügt werden, muss der Bootcode aktualisiert werden.</p></div><div class=paragraph><p>Einbinden einer zweiten Spiegelgruppe (<span class=filename>ada2p3</span> und <span class=filename>ada3p3</span>) zu einem bestehenden Spiegel:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: resilvered 781M <span class=k>in </span>0h0m with 0 errors on Fri May 30 08:19:35 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0

errors: No known data errors
<span class=c># zpool add mypool mirror ada2p3 ada3p3</span>
<span class=c># gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 ada2</span>
bootcode written to ada2
<span class=c># gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 ada3</span>
bootcode written to ada3
<span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: scrub repaired 0 <span class=k>in </span>0h0m with 0 errors on Fri May 30 08:29:51 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0
          mirror-1  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0
            ada3p3  ONLINE       0     0     0

errors: No known data errors</code></pre></div></div><div class=paragraph><p>Momentan können vdevs nicht von einem Pool entfernt und Platten nur von einem Spiegel ausgehängt werden, wenn genug Redundanz übrig bleibt. Wenn auch nur eine Platte in einer Spiegelgruppe bestehen bleibt, hört der Spiegel auf zu existieren und wird zu einem stripe, was den gesamten Pool riskiert, falls diese letzte Platte ausfällt.</p></div><div class=paragraph><p>Entfernen einer Platte aus einem Spiegel mit drei Platten:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: scrub repaired 0 <span class=k>in </span>0h0m with 0 errors on Fri May 30 08:29:51 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0

errors: No known data errors
<span class=c># zpool detach mypool ada2p3</span>
<span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: scrub repaired 0 <span class=k>in </span>0h0m with 0 errors on Fri May 30 08:29:51 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0

errors: No known data errors</code></pre></div></div></div><div class=sect2><h3 id=zfs-zpool-status>19.3.3. Den Status eines Pools überprüfen<a class=anchor href=#zfs-zpool-status></a></h3><div class=paragraph><p>Der Status eines Pools ist wichtig. Wenn ein Gerät sich abschaltet oder ein Lese-, Schreib- oder Prüfsummenfehler festgestellt wird, wird der dazugehörige Fehlerzähler erhöht. Die <code>status</code>-Ausgabe zeigt die Konfiguration und den Status von jedem Gerät im Pool und den Gesamtstatus des Pools. Aktionen, die durchgeführt werden sollten und Details zum letzten <a href=#zfs-zpool-scrub><code>scrub</code></a> werden ebenfalls angezeigt.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: scrub repaired 0 <span class=k>in </span>2h25m with 0 errors on Sat Sep 14 04:25:50 2013
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          raidz2-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0
            ada3p3  ONLINE       0     0     0
            ada4p3  ONLINE       0     0     0
            ada5p3  ONLINE       0     0     0

errors: No known data errors</code></pre></div></div></div><div class=sect2><h3 id=zfs-zpool-clear>19.3.4. Fehler beseitigen<a class=anchor href=#zfs-zpool-clear></a></h3><div class=paragraph><p>Wenn ein Fehler erkannt wurde, werden die Lese-, Schreib- oder Prüfsummenzähler erhöht. Die Fehlermeldung kann beseitigt und der Zähler mit <code>zpool clear <em>mypool</em></code> zurückgesetzt werden. Den Fehlerzustand zurückzusetzen kann wichtig sein, wenn automatisierte Skripte ablaufen, die den Administrator informieren, sobald der Pool Fehler anzeigt. Weitere Fehler werden nicht gemeldet, wenn der alte Fehlerbericht nicht entfernt wurde.</p></div></div><div class=sect2><h3 id=zfs-zpool-replace>19.3.5. Ein funktionierendes Gerät ersetzen<a class=anchor href=#zfs-zpool-replace></a></h3><div class=paragraph><p>Es gibt eine Reihe von Situationen, in denen es nötig ist, eine Platte mit einer anderen auszutauschen. Wenn eine funktionierende Platte ersetzt wird, hält der Prozess die alte Platte während des Ersetzungsvorganges noch aktiv. Der Pool wird nie den Zustand <a href=#zfs-term-degraded>degraded</a> erhalten, was das Risiko eines Datenverlustes minimiert. Alle Daten der alten Platte werden durch das Kommando <code>zpool replace</code> auf die Neue übertragen. Nachdem die Operation abgeschlossen ist, wird die alte Platte vom vdev getrennt. Falls die neue Platte grösser ist als die alte Platte , ist es möglich den Pool zu vergrößern, um den neuen Platz zu nutzen. Lesen Sie dazu <a href=#zfs-zpool-online>Einen Pool vergrößern</a>.</p></div><div class=paragraph><p>Ersetzen eines funktionierenden Geräts in einem Pool:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0

errors: No known data errors
<span class=c># zpool replace mypool ada1p3 ada2p3</span>
Make sure to <span class=nb>wait </span><span class=k>until </span>resilver is <span class=k>done </span>before rebooting.

If you boot from pool <span class=s1>&#39;zroot&#39;</span>, you may need to update
boot code on newly attached disk <span class=s1>&#39;ada2p3&#39;</span><span class=nb>.</span>

Assuming you use GPT partitioning und <span class=s1>&#39;da0&#39;</span> is your new boot disk
you may use the following <span class=nb>command</span>:

        gpart bootcode <span class=nt>-b</span> /boot/pmbr <span class=nt>-p</span> /boot/gptzfsboot <span class=nt>-i</span> 1 da0
<span class=c># gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 ada2</span>
<span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
status: One or more devices is currently being resilvered.  The pool will
        <span class=k>continue </span>to <span class=k>function</span>, possibly <span class=k>in </span>a degraded state.
action: Wait <span class=k>for </span>the resilver to complete.
  scan: resilver <span class=k>in </span>progress since Mon Jun  2 14:21:35 2014
        604M scanned out of 781M at 46.5M/s, 0h0m to go
        604M resilvered, 77.39% <span class=k>done
</span>config:

        NAME             STATE     READ WRITE CKSUM
        mypool           ONLINE       0     0     0
          mirror-0       ONLINE       0     0     0
            ada0p3       ONLINE       0     0     0
            replacing-1  ONLINE       0     0     0
              ada1p3     ONLINE       0     0     0
              ada2p3     ONLINE       0     0     0  <span class=o>(</span>resilvering<span class=o>)</span>

errors: No known data errors
<span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: resilvered 781M <span class=k>in </span>0h0m with 0 errors on Mon Jun  2 14:21:52 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0

errors: No known data errors</code></pre></div></div></div><div class=sect2><h3 id=zfs-zpool-resilver>19.3.6. Behandlung von fehlerhaften Geräten<a class=anchor href=#zfs-zpool-resilver></a></h3><div class=paragraph><p>Wenn eine Platte in einem Pool ausfällt, wird das vdev zu dem diese Platte gehört, den Zustand <a href=#zfs-term-degraded>degraded</a> erhalten. Alle Daten sind immer noch verfügbar, jedoch wird die Geschwindigkeit möglicherweise reduziert, weil die fehlenden Daten aus der verfügbaren Redundanz heraus berechnet werden müssen. Um das vdev in einen funktionierenden Zustand zurück zu versetzen, muss das physikalische Gerät ersetzt werden. ZFS wird dann angewiesen, den <a href=#zfs-term-resilver>resilver</a>-Vorgang zu beginnen. Daten, die sich auf dem defekten Gerät befanden, werden neu aus der vorhandenen Prüfsumme berechnet und auf das Ersatzgerät geschrieben. Nach Beendigung dieses Prozesses kehrt das vdev zum Status <a href=#zfs-term-online>online</a> zurück.</p></div><div class=paragraph><p>Falls das vdev keine Redundanz besitzt oder wenn mehrere Geräte ausgefallen sind und es nicht genug Redundanz gibt, um dies zu kompensieren, geht der Pool in den Zustand <a href=#zfs-term-faulted>faulted</a> über. Wenn keine ausreichende Anzahl von Geräten wieder an den Pool angeschlossen wird, fällt der Pool aus und die Daten müssen von Sicherungen wieder eingespielt werden.</p></div><div class=paragraph><p>Wenn eine defekte Platte ausgewechselt wird, wird der Name dieser defekten Platte mit der GUID des Geräts ersetzt. Ein neuer Gerätename als Parameter für <code>zpool replace</code> wird nicht benötigt, falls das Ersatzgerät den gleichen Gerätenamen besitzt.</p></div><div class=paragraph><p>Ersetzen einer defekten Platte durch <code>zpool replace</code>:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool status</span>
  pool: mypool
 state: DEGRADED
status: One or more devices could not be opened.  Sufficient replicas exist <span class=k>for
        </span>the pool to <span class=k>continue </span>functioning <span class=k>in </span>a degraded state.
action: Attach the missing device und online it using <span class=s1>&#39;zpool online&#39;</span><span class=nb>.</span>
   see: http://illumos.org/msg/ZFS-8000-2Q
  scan: none requested
config:

        NAME                    STATE     READ WRITE CKSUM
        mypool                  DEGRADED     0     0     0
          mirror-0              DEGRADED     0     0     0
            ada0p3              ONLINE       0     0     0
            316502962686821739  UNAVAIL      0     0     0  was /dev/ada1p3

errors: No known data errors
<span class=c># zpool replace mypool 316502962686821739 ada2p3</span>
<span class=c># zpool status</span>
  pool: mypool
 state: DEGRADED
status: One or more devices is currently being resilvered.  The pool will
        <span class=k>continue </span>to <span class=k>function</span>, possibly <span class=k>in </span>a degraded state.
action: Wait <span class=k>for </span>the resilver to complete.
  scan: resilver <span class=k>in </span>progress since Mon Jun  2 14:52:21 2014
        641M scanned out of 781M at 49.3M/s, 0h0m to go
        640M resilvered, 82.04% <span class=k>done
</span>config:

        NAME                        STATE     READ WRITE CKSUM
        mypool                      DEGRADED     0     0     0
          mirror-0                  DEGRADED     0     0     0
            ada0p3                  ONLINE       0     0     0
            replacing-1             UNAVAIL      0     0     0
              15732067398082357289  UNAVAIL      0     0     0  was /dev/ada1p3/old
              ada2p3                ONLINE       0     0     0  <span class=o>(</span>resilvering<span class=o>)</span>

errors: No known data errors
<span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: resilvered 781M <span class=k>in </span>0h0m with 0 errors on Mon Jun  2 14:52:38 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0

errors: No known data errors</code></pre></div></div></div><div class=sect2><h3 id=zfs-zpool-scrub>19.3.7. Einen Pool überprüfen<a class=anchor href=#zfs-zpool-scrub></a></h3><div class=paragraph><p>Es wird empfohlen, dass Pools regelmäßig geprüft (<a href=#zfs-term-scrub>scrubbed</a>) werden, idealerweise mindestens einmal pro Monat. Der <code>scrub</code>-Vorgang ist beansprucht die Platte sehr und reduziert die Geschwindigkeit während er läuft. Vermeiden Sie Zeiten, in denen großer Bedarf besteht, wenn Sie <code>scrub</code> starten oder benutzen Sie <a href=#zfs-advanced-tuning-scrub_delay><code>vfs.zfs.scrub_delay</code></a>, um die relative Priorität vom <code>scrub</code> einzustellen, um zu verhindern, dass es mit anderen Aufgaben kollidiert.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool scrub mypool</span>
<span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: scrub <span class=k>in </span>progress since Wed Feb 19 20:52:54 2014
        116G scanned out of 8.60T at 649M/s, 3h48m to go
        0 repaired, 1.32% <span class=k>done
</span>config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          raidz2-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0
            ada3p3  ONLINE       0     0     0
            ada4p3  ONLINE       0     0     0
            ada5p3  ONLINE       0     0     0

errors: No known data errors</code></pre></div></div><div class=paragraph><p>Falls eine Überrpüfaktion abgebrochen werden muss, geben Sie <code>zpool scrub -s <em>mypool</em></code> ein.</p></div></div><div class=sect2><h3 id=zfs-zpool-selfheal>19.3.8. Selbstheilung<a class=anchor href=#zfs-zpool-selfheal></a></h3><div class=paragraph><p>Die Prüfsummen, welche zusammen mit den Datenblöcken gespeichert werden, ermöglichen dem Dateisystem, sich <em>selbst zu heilen</em>. Diese Eigenschaft wird automatisch Daten korrigieren, deren Prüfsumme nicht mit der Gespeicherten übereinstimmt, die auf einem anderen Gerät, das Teil des Pools ist, vorhanden ist. Beispielsweise bei einem Spiegel aus zwei Platten, von denen eine anfängt, Fehler zu produzieren und nicht mehr länger Daten speichern kann. Dieser Fall ist sogar noch schlimmer, wenn auf die Daten seit einiger Zeit nicht mehr zugegriffen wurde, zum Beispiel bei einem Langzeit-Archivspeicher. Traditionelle Dateisysteme müssen dann Algorithmen wie <a href="https://man.freebsd.org/cgi/man.cgi?query=fsck&amp;sektion=8&amp;format=html">fsck(8)</a> ablaufen lassen, welche die Daten überprüfen und reparieren. Diese Kommandos benötigen einige Zeit und in gravierenden Fällen muss ein Administrator manuelle Entscheidungen treffen, welche Reparaturoperation vorgenommen werden soll. Wenn ZFS einen defekten Datenblock mit einer Prüfsumme erkennt, die nicht übereinstimmt, versucht es die Daten von der gespiegelten Platte zu lesen. Wenn diese Platte die korrekten Daten liefern kann, wird nicht nur dieser Datenblock an die anfordernde Applikation geschickt, sondern auch die falschen Daten auf der Disk reparieren, welche die falsche Prüfsumme erzeugt hat. Dies passiert während des normalen Betriebs des Pools, ohne dass eine Interaktion vom Systemadministrator notwendig wäre.</p></div><div class=paragraph><p>Das nächste Beispiel demonstriert dieses Verhalten zur Selbstheilung. Ein gespiegelter Pool mit den beiden Platten <span class=filename>/dev/ada0</span> und <span class=filename>/dev/ada1</span> wird angelegt.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool create healer mirror /dev/ada0 /dev/ada1</span>
<span class=c># zpool status healer</span>
  pool: healer
 state: ONLINE
  scan: none requested
config:

    NAME        STATE     READ WRITE CKSUM
    healer      ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
       ada0     ONLINE       0     0     0
       ada1     ONLINE       0     0     0

errors: No known data errors
<span class=c># zpool list</span>
NAME     SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG   CAP  DEDUP  HEALTH  ALTROOT
healer   960M  92.5K   960M         -         -     0%    0%  1.00x  ONLINE  -</code></pre></div></div><div class=paragraph><p>Ein paar wichtige Daten, die es vor Datenfehlern mittels der Selbstheilungsfunktion zu schützen gilt, werden auf den Pool kopiert. Eine Prüfsumme wird zum späteren Vergleich berechnet.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># cp /some/important/data /healer</span>
<span class=c># zfs list</span>
NAME     SIZE  ALLOC   FREE    CAP  DEDUP  HEALTH  ALTROOT
healer   960M  67.7M   892M     7%  1.00x  ONLINE  -
<span class=c># sha1 /healer &gt; checksum.txt</span>
<span class=c># cat checksum.txt</span>
SHA1 <span class=o>(</span>/healer<span class=o>)</span> <span class=o>=</span> 2753eff56d77d9a536ece6694bf0a82740344d1f</code></pre></div></div><div class=paragraph><p>Datenfehler werden durch das Schreiben von zufälligen Daten an den Anfang einer Platte des Spiegels simuliert. Um ZFS daran zu hindern, die Daten so schnell zu reparieren, wie es diese entdeckt, wird der Pool vor der Veränderung exportiert und anschließend wieder importiert.</p></div><div class="admonitionblock warning"><table><tbody><tr><td class=icon><i class="fa icon-warning" title=Warning></i></td><td class=content><div class=paragraph><p>Dies ist eine gefährliche Operation, die wichtige Daten zerstören kann. Es wird hier nur zu Demonstrationszwecken gezeigt und sollte nicht während des normalen Betriebs des Pools versucht werden. Dieses vorsätzliche Korrumpierungsbeispiel sollte auf gar keinen Fall auf einer Platte mit einem anderen Dateisystem durchgeführt werden. Verwenden Sie keine anderen Gerätenamen als diejenigen, die hier gezeigt werden, die Teil des Pools sind. Stellen Sie sicher, dass die passende Sicherungen angefertigt haben, bevor Sie dieses Kommando ausführen!</p></div></td></tr></tbody></table></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool export healer</span>
<span class=c># dd if=/dev/random of=/dev/ada1 bs=1m count=200</span>
200+0 records <span class=k>in
</span>200+0 records out
209715200 bytes transferred <span class=k>in </span>62.992162 secs <span class=o>(</span>3329227 bytes/sec<span class=o>)</span>
<span class=c># zpool import healer</span></code></pre></div></div><div class=paragraph><p>Der Status des Pools zeigt an, dass bei einem Gerät ein Fehler aufgetreten ist. Wichtig zu wissen ist, dass Anwendungen, die Daten vom Pool lesen keine ungültigen Daten erhalten haben. ZFS lieferte Daten vom <span class=filename>ada0</span>-Gerät mit der korrekten Prüfsumme aus. Das Gerät mit der fehlerhaften Prüfsumme kann sehr einfach gefunden werden, da die Spalte <code>CKSUM</code> einen Wert ungleich Null enthält.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool status healer</span>
    pool: healer
   state: ONLINE
  status: One or more devices has experienced an unrecoverable error.  An
          attempt was made to correct the error.  Applications are unaffected.
  action: Determine <span class=k>if </span>the device needs to be replaced, und clear the errors
          using <span class=s1>&#39;zpool clear&#39;</span> or replace the device with <span class=s1>&#39;zpool replace&#39;</span><span class=nb>.</span>
     see: http://illumos.org/msg/ZFS-8000-4J
    scan: none requested
  config:

      NAME        STATE     READ WRITE CKSUM
      healer      ONLINE       0     0     0
        mirror-0  ONLINE       0     0     0
         ada0     ONLINE       0     0     0
         ada1     ONLINE       0     0     1

errors: No known data errors</code></pre></div></div><div class=paragraph><p>Der Fehler wurde erkannt und korrigiert durch die vorhandene Redundanz, welche aus der nicht betroffenen Platte <span class=filename>ada0</span> des Spiegels gewonnen wurde. Ein Vergleich der Prüfsumme mit dem Original wird zeigen, ob sich der Pool wieder in einem konsistenten Zustand befindet.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># sha1 /healer &gt;&gt; checksum.txt</span>
<span class=c># cat checksum.txt</span>
SHA1 <span class=o>(</span>/healer<span class=o>)</span> <span class=o>=</span> 2753eff56d77d9a536ece6694bf0a82740344d1f
SHA1 <span class=o>(</span>/healer<span class=o>)</span> <span class=o>=</span> 2753eff56d77d9a536ece6694bf0a82740344d1f</code></pre></div></div><div class=paragraph><p>Die beiden Prüfsummen, die vor und nach der vorsätzlichen Korrumpierung der Daten des Pools angelegt wurden, stimmen immer noch überein. Dies zeigt wie ZFS in der Lage ist, Fehler automatisch zu erkennen und zu korrigieren, wenn die Prüfsummen nicht übereinstimmen. Beachten Sie, dass dies nur möglich ist, wenn genug Redundanz im Pool vorhanden ist. Ein Pool, der nur aus einer einzigen Platte besteht besitzt keine Selbstheilungsfunktion. Dies ist auch der Grund warum Prüfsummen bei ZFS so wichtig sind und deshalb aus keinem Grund deaktiviert werden sollten. Kein <a href="https://man.freebsd.org/cgi/man.cgi?query=fsck&amp;sektion=8&amp;format=html">fsck(8)</a> ist nötig, um diese Fehler zu erkennen und zu korrigieren und der Pool war während der gesamten Zeit, in der das Problem bestand, verfügbar. Eine scrub-Aktion ist nun nötig, um die fehlerhaften Daten auf <span class=filename>ada1</span> zu beheben.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool scrub healer</span>
<span class=c># zpool status healer</span>
  pool: healer
 state: ONLINE
status: One or more devices has experienced an unrecoverable error.  An
            attempt was made to correct the error.  Applications are unaffected.
action: Determine <span class=k>if </span>the device needs to be replaced, und clear the errors
            using <span class=s1>&#39;zpool clear&#39;</span> or replace the device with <span class=s1>&#39;zpool replace&#39;</span><span class=nb>.</span>
   see: http://illumos.org/msg/ZFS-8000-4J
  scan: scrub <span class=k>in </span>progress since Mon Dec 10 12:23:30 2012
        10.4M scanned out of 67.0M at 267K/s, 0h3m to go
        9.63M repaired, 15.56% <span class=k>done
</span>config:

    NAME        STATE     READ WRITE CKSUM
    healer      ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
       ada0     ONLINE       0     0     0
       ada1     ONLINE       0     0   627  <span class=o>(</span>repairing<span class=o>)</span>

errors: No known data errors</code></pre></div></div><div class=paragraph><p>Durch das scrub werden die Daten von <span class=filename>ada0</span> gelesen und alle Daten mit einer falschen durch diejenigen mit der richtigen Prüfsumme auf <span class=filename>ada1</span> ersetzt. Dies wird durch die Ausgabe <code>(repairing)</code> des Kommandos <code>zpool status</code> angezeigt. Nachdem die Operation abgeschlossen ist, ändert sich der Poolstatus zu:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool status healer</span>
  pool: healer
 state: ONLINE
status: One or more devices has experienced an unrecoverable error.  An
        attempt was made to correct the error.  Applications are unaffected.
action: Determine <span class=k>if </span>the device needs to be replaced, und clear the errors
             using <span class=s1>&#39;zpool clear&#39;</span> or replace the device with <span class=s1>&#39;zpool replace&#39;</span><span class=nb>.</span>
   see: http://illumos.org/msg/ZFS-8000-4J
  scan: scrub repaired 66.5M <span class=k>in </span>0h2m with 0 errors on Mon Dec 10 12:26:25 2012
config:

    NAME        STATE     READ WRITE CKSUM
    healer      ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
       ada0     ONLINE       0     0     0
       ada1     ONLINE       0     0 2.72K

errors: No known data errors</code></pre></div></div><div class=paragraph><p>Nach der scrub-Operation und der anschliessenden Synchronisation der Daten von <span class=filename>ada0</span> nach <span class=filename>ada1</span>, kann die Fehlermeldung vom Poolstatus durch die Eingabe von <code>zpool clear</code><a href=#zfs-zpool-clear>bereinigt</a> werden.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool clear healer</span>
<span class=c># zpool status healer</span>
  pool: healer
 state: ONLINE
  scan: scrub repaired 66.5M <span class=k>in </span>0h2m with 0 errors on Mon Dec 10 12:26:25 2012
config:

    NAME        STATE     READ WRITE CKSUM
    healer      ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
       ada0     ONLINE       0     0     0
       ada1     ONLINE       0     0     0

errors: No known data errors</code></pre></div></div><div class=paragraph><p>Der Pool ist jetzt wieder in einem voll funktionsfähigen Zustand versetzt worden und alle Fehler wurden beseitigt.</p></div></div><div class=sect2><h3 id=zfs-zpool-online>19.3.9. Einen Pool vergrössern<a class=anchor href=#zfs-zpool-online></a></h3><div class=paragraph><p>Die verwendbare Größe eines redundant ausgelegten Pools ist durch die Kapazität des kleinsten Geräts in jedem vdev begrenzt. Das kleinste Gerät kann durch ein größeres Gerät ersetzt werden. Nachdem eine <a href=#zfs-zpool-replace>replace</a> oder <a href=#zfs-term-resilver>resilver</a>-Operation abgeschlossen wurde, kann der Pool anwachsen, um die Kapazität des neuen Geräts zu nutzen. Nehmen wir als Beispiel einen Spiegel mit einer 1 TB und einer 2 TB Platte. Der verwendbare Plattenplatz beträgt 1 TB. Wenn die 1 TB Platte mit einer anderen 2 TB Platte ersetzt wird, kopiert der resilver-Prozess die existierenden Daten auf die neue Platte. Da beide Geräte nun 2 TB Kapazität besitzen, kann auch der verfügbare Plattenplatz auf die Größe von 2 TB anwachsen.</p></div><div class=paragraph><p>Die Erweiterung wird durch das Kommando <code>zpool online -e</code> auf jedem Gerät ausgelöst. Nachdem alle Geräte expandiert wurden, wird der Speicher im Pool zur Verfügung gestellt.</p></div></div><div class=sect2><h3 id=zfs-zpool-import>19.3.10. Importieren und Exportieren von Pools<a class=anchor href=#zfs-zpool-import></a></h3><div class=paragraph><p>Pools werden <em>exportiert</em> bevor diese an ein anderes System angeschlossen werden. Alle Datasets werden abgehängt und jedes Gerät wird als exportiert markiert, ist jedoch immer noch gesperrt, so dass es nicht von anderen Festplattensubsystemen verwendet werden kann. Dadurch können Pools auf anderen Maschinen <em>importiert</em> werden, die ZFS und sogar andere Hardwarearchitekturen (bis auf ein paar Ausnahmen, siehe <a href="https://man.freebsd.org/cgi/man.cgi?query=zpool&amp;sektion=8&amp;format=html">zpool(8)</a>) unterstützen. Besitzt ein Dataset offene Dateien, kann <code>zpool export -f</code> den Export des Pools erzwingen. Verwenden Sie dies mit Vorsicht. Die Datasets werden dadurch gewaltsam abgehängt, was bei Anwendungen, die noch offene Dateien auf diesem Dataset hatten, möglicherweise zu unerwartetem Verhalten führen kann.</p></div><div class=paragraph><p>Einen nichtverwendeten Pool exportieren:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool export mypool</span></code></pre></div></div><div class=paragraph><p>Beim Importieren eines Pool werden auch automatisch alle Datasets eingehängt. Dies ist möglicherweise nicht das bevorzugte Verhalten und wird durch <code>zpool import -N</code> verhindert. Durch <code>zpool import -o</code> temporäre Eigenschaften nur für diesen Import gesetzt. Mit dem Befehl <code>zpool import altroot=</code> ist es möglich, einen Pool mit einem anderen Basiseinhängepunkt anstatt der Wurzel des Dateisystems einzubinden. Wenn der Pool zuletzt auf einem anderen System verwendet und nicht korrekt exportiert wurde, muss unter Umständen ein Import erzwungen werden durch <code>zpool import -f</code>. Alle Pools, die momentan nicht durch ein anderes System verwendet werden, lassen sich mit <code>zpool import -a</code> importieren.</p></div><div class=paragraph><p>Alle zum Import verfügbaren Pools auflisten:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool import</span>
   pool: mypool
     <span class=nb>id</span>: 9930174748043525076
  state: ONLINE
 action: The pool can be imported using its name or numeric identifier.
 config:

        mypool      ONLINE
          ada2p3    ONLINE</code></pre></div></div><div class=paragraph><p>Den Pool mit einem anderen Wurzelverzeichnis importieren:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool import -o altroot=/mnt mypool</span>
<span class=c># zfs list</span>
zfs list
NAME                 USED  AVAIL  REFER  MOUNTPOINT
mypool               110K  47.0G    31K  /mnt/mypool</code></pre></div></div></div><div class=sect2><h3 id=zfs-zpool-upgrade>19.3.11. Einen Pool aktualisieren<a class=anchor href=#zfs-zpool-upgrade></a></h3><div class=paragraph><p>Nachdem FreeBSD aktualisiert wurde oder wenn der Pool von einem anderen System, das eine ältere Version von ZFS einsetzt, lässt sich der Pool manuell auf den aktuellen Stand von ZFS bringen, um die neuesten Eigenschaften zu unterstützen. Bedenken Sie, ob der Pool jemals wieder von einem älteren System eingebunden werden muss, bevor Sie die Aktualisierung durchführen. Das aktualisieren eines Pools ist ein nicht umkehrbarer Prozess. ältere Pools lassen sich aktualisieren, jedoch lassen sich Pools mit neueren Eigenschaften nicht wieder auf eine ältere Version bringen.</p></div><div class=paragraph><p>Aktualisierung eines v28-Pools, um <code>Feature Flags</code> zu unterstützen:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
status: The pool is formatted using a legacy on-disk format.  The pool can
        still be used, but some features are unavailable.
action: Upgrade the pool using <span class=s1>&#39;zpool upgrade&#39;</span><span class=nb>.</span>  Once this is <span class=k>done</span>, the
        pool will no longer be accessible on software that does not support feat
        flags.
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
	    ada0    ONLINE       0     0     0
	    ada1    ONLINE       0     0     0

errors: No known data errors
<span class=c># zpool upgrade</span>
This system supports ZFS pool feature flags.

The following pools are formatted with legacy version numbers und can
be upgraded to use feature flags.  After being upgraded, these pools
will no longer be accessible by software that does not support feature
flags.

VER  POOL
<span class=nt>---</span>  <span class=nt>------------</span>
28   mypool

Use <span class=s1>&#39;zpool upgrade -v&#39;</span> <span class=k>for </span>a list of available legacy versions.
Every feature flags pool has all supported features enabled.
<span class=c># zpool upgrade mypool</span>
This system supports ZFS pool feature flags.

Successfully upgraded <span class=s1>&#39;mypool&#39;</span> from version 28 to feature flags.
Enabled the following features on <span class=s1>&#39;mypool&#39;</span>:
  async_destroy
  empty_bpobj
  lz4_compress
  multi_vdev_crash_dump</code></pre></div></div><div class=paragraph><p>Die neueren Eigenschaften von ZFS werden nicht verfügbar sein, bis <code>zpool upgrade</code> abgeschlossen ist. <code>zpool upgrade -v</code> kann verwendet werden, um zu sehen, welche neuen Eigenschaften durch die Aktualisierung bereitgestellt werden, genauso wie diejenigen, die momentan schon verfügbar sind.</p></div><div class=paragraph><p>Einen Pool um zusätzliche Feature Flags erweitern:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool status</span>
  pool: mypool
 state: ONLINE
status: Some supported features are not enabled on the pool. The pool can
        still be used, but some features are unavailable.
action: Enable all features using <span class=s1>&#39;zpool upgrade&#39;</span><span class=nb>.</span> Once this is <span class=k>done</span>,
        the pool may no longer be accessible by software that does not support
        the features. See zpool-features<span class=o>(</span>7<span class=o>)</span> <span class=k>for </span>details.
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
	    ada0    ONLINE       0     0     0
	    ada1    ONLINE       0     0     0

errors: No known data errors
<span class=c># zpool upgrade</span>
This system supports ZFS pool feature flags.

All pools are formatted using feature flags.

Some supported features are not enabled on the following pools. Once a
feature is enabled the pool may become incompatible with software
that does not support the feature. See zpool-features<span class=o>(</span>7<span class=o>)</span> <span class=k>for </span>details.

POOL  FEATURE
<span class=nt>---------------</span>
zstore
      multi_vdev_crash_dump
      spacemap_histogram
      enabled_txg
      hole_birth
      extensible_dataset
      bookmarks
      filesystem_limits
<span class=c># zpool upgrade mypool</span>
This system supports ZFS pool feature flags.

Enabled the following features on <span class=s1>&#39;mypool&#39;</span>:
  spacemap_histogram
  enabled_txg
  hole_birth
  extensible_dataset
  bookmarks
  filesystem_limits</code></pre></div></div><div class="admonitionblock warning"><table><tbody><tr><td class=icon><i class="fa icon-warning" title=Warning></i></td><td class=content><div class=paragraph><p>Der Bootcode muss auf Systemen, die von dem Pool starten, aktualisiert werden, um diese neue Version zu unterstützen. Verwenden Sie <code>gpart bootcode</code> auf der Partition, die den Bootcode enthält. Es gibt zwei Arten von Bootcode, je nachdem, wie das System bootet: GPT (die häufigste Option) und EFI (für moderne Systeme).</p></div><div class=paragraph><p>Benutzen Sie für GPT den folgenden Befehl:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 ada1</span></code></pre></div></div><div class=paragraph><p>Für Systeme, die EFI zum Booten benutzen, führen Sie folgenden Befehl aus:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># gpart bootcode -p /boot/boot1.efifat -i 1 ada1</span></code></pre></div></div><div class=paragraph><p>Installieren Sie den Bootcode auf allen bootfähigen Platten im Pool. Lesen Sie <a href="https://man.freebsd.org/cgi/man.cgi?query=gpart&amp;sektion=8&amp;format=html">gpart(8)</a> für weitere Informationen.</p></div></td></tr></tbody></table></div></div><div class=sect2><h3 id=zfs-zpool-history>19.3.12. Aufgezeichnete Historie des Pools anzeigen<a class=anchor href=#zfs-zpool-history></a></h3><div class=paragraph><p>Befehle, die den Pool in irgendeiner Form verändern, werden aufgezeichnet. Diese Befehle beinhalten das Erstellen von Datasets, verändern von Eigenschaften oder das Ersetzen einer Platte. Diese Historie ist nützlich um nachzuvollziehen, wie ein Pool aufgebaut ist und welcher Benutzer eine bestimmte Aktion wann und wie getätigt hat. Die aufgezeichnete Historie wird nicht in einer Logdatei festgehalten, sondern ist Teil des Pools selbst. Das Kommando zum darstellen dieser Historie lautet passenderweise <code>zpool history</code>:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool history</span>
History <span class=k>for</span> <span class=s1>&#39;tank&#39;</span>:
2013-02-26.23:02:35 zpool create tank mirror /dev/ada0 /dev/ada1
2013-02-27.18:50:58 zfs <span class=nb>set </span><span class=nv>atime</span><span class=o>=</span>off tank
2013-02-27.18:51:09 zfs <span class=nb>set </span><span class=nv>checksum</span><span class=o>=</span>fletcher4 tank
2013-02-27.18:51:18 zfs create tank/backup</code></pre></div></div><div class=paragraph><p>Die Ausgabe zeigt <code>zpool</code> und <code>zfs</code>-Befehle, die ausgeführt wurden zusammen mit einem Zeitstempel. Nur Befehle, die den Pool verändern werden aufgezeichnet. Befehle wie <code>zfs list</code> sind dabei nicht enthalten. Wenn kein Name angegeben wird, erscheint die gesamte Historie aller Pools.</p></div><div class=paragraph><p>Der Befehl <code>zpool history</code> kann sogar noch mehr Informationen ausgeben, wenn die Optionen <code>-i</code> oder <code>-l</code> angegeben werden. Durch <code>-i</code> zeigt ZFS vom Benutzer eingegebene, als auch interne Ereignisse an.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool history -i</span>
History <span class=k>for</span> <span class=s1>&#39;tank&#39;</span>:
2013-02-26.23:02:35 <span class=o>[</span>internal pool create txg:5] pool spa 28<span class=p>;</span> zfs spa 28<span class=p>;</span> zpl 5<span class=p>;</span>uts  9.1-RELEASE 901000 amd64
2013-02-27.18:50:53 <span class=o>[</span>internal property <span class=nb>set </span>txg:50] <span class=nv>atime</span><span class=o>=</span>0 dataset <span class=o>=</span> 21
2013-02-27.18:50:58 zfs <span class=nb>set </span><span class=nv>atime</span><span class=o>=</span>off tank
2013-02-27.18:51:04 <span class=o>[</span>internal property <span class=nb>set </span>txg:53] <span class=nv>checksum</span><span class=o>=</span>7 dataset <span class=o>=</span> 21
2013-02-27.18:51:09 zfs <span class=nb>set </span><span class=nv>checksum</span><span class=o>=</span>fletcher4 tank
2013-02-27.18:51:13 <span class=o>[</span>internal create txg:55] dataset <span class=o>=</span> 39
2013-02-27.18:51:18 zfs create tank/backup</code></pre></div></div><div class=paragraph><p>Weitere Details lassen sich durch die Angabe von <code>-l</code> entlocken. Historische Einträge werden in einem langen Format ausgegeben, einschließlich Informationen wie der Name des Benutzers, welcher das Kommando eingegeben hat und der Hostname, auf dem die Änderung erfolgte.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool history -l</span>
History <span class=k>for</span> <span class=s1>&#39;tank&#39;</span>:
2013-02-26.23:02:35 zpool create tank mirror /dev/ada0 /dev/ada1 <span class=o>[</span>user 0 <span class=o>(</span>root<span class=o>)</span> on :global]
2013-02-27.18:50:58 zfs <span class=nb>set </span><span class=nv>atime</span><span class=o>=</span>off tank <span class=o>[</span>user 0 <span class=o>(</span>root<span class=o>)</span> on myzfsbox:global]
2013-02-27.18:51:09 zfs <span class=nb>set </span><span class=nv>checksum</span><span class=o>=</span>fletcher4 tank <span class=o>[</span>user 0 <span class=o>(</span>root<span class=o>)</span> on myzfsbox:global]
2013-02-27.18:51:18 zfs create tank/backup <span class=o>[</span>user 0 <span class=o>(</span>root<span class=o>)</span> on myzfsbox:global]</code></pre></div></div><div class=paragraph><p>Die Ausgabe zeigt, dass der Benutzer <code>root</code> den gespiegelten Pool mit den beiden Platten <span class=filename>/dev/ada0</span> und <span class=filename>/dev/ada1</span> angelegt hat. Der Hostname <code>myzfsbox</code> wird ebenfalls in den Kommandos angezeigt, nachdem der Pool erzeugt wurde. Die Anzeige des Hostnamens wird wichtig, sobald der Pool von einem System exportiert und auf einem anderen importiert wird. Die Befehle, welche auf dem anderen System verwendet werden, können klar durch den Hostnamen, der bei jedem Kommando mit verzeichnet wird, unterschieden werden.</p></div><div class=paragraph><p>Beide Optionen für <code>zpool history</code> lassen sich auch kombinieren, um die meisten Details zur Historie eines Pools auszugeben. Die Pool Historie liefert wertvolle Informationen, wenn Aktionen nachverfolgt werden müssen oder zur Fehlerbeseitigung mehr Informationen gebraucht werden.</p></div></div><div class=sect2><h3 id=zfs-zpool-iostat>19.3.13. Geschwindigkeitsüberwachung<a class=anchor href=#zfs-zpool-iostat></a></h3><div class=paragraph><p>Ein eingebautes Überwachungssystem kann I/O-Statistiken in Echtzeit liefern. Es zeigt die Menge von freiem und belegtem Speicherplatz auf dem Pool an, wieviele Lese- und Schreiboperationen pro Sekunde durchgeführt werden und die aktuell verwendete I/O-Bandbreite. Standardmäßig werden alle Pools in einem System überwacht und angezeigt. Ein Poolname kann angegeben werden, um die Anzeige auf diesen Pool zu beschränken. Ein einfaches Beispiel:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool iostat</span>
               capacity     operations    bundwidth
pool        alloc   free   <span class=nb>read  </span>write   <span class=nb>read  </span>write
<span class=nt>----------</span>  <span class=nt>-----</span>  <span class=nt>-----</span>  <span class=nt>-----</span>  <span class=nt>-----</span>  <span class=nt>-----</span>  <span class=nt>-----</span>
data         288G  1.53T      2     11  11.3K  57.1K</code></pre></div></div><div class=paragraph><p>Um kontinuierlich die I/O-Aktivität zu überprüfen, kann eine Zahl als letzter Parameter angegeben werden, die ein Intervall in Sekunden angibt, die zwischen den Aktualisierungen vergehen soll. Die nächste Zeile mit Statistikinformationen wird dann nach jedem Intervall ausgegeben. Drücken Sie <span class=keyseq><kbd>Ctrl</kbd>+<kbd>C</kbd></span>, um diese kontinuierliche Überwachung zu stoppen. Alternativ lässt sich auch eine zweite Zahl nach dem Intervall auf der Kommandozeile angeben, welche die Obergrenze von Statistikausgaben darstellt, die angezeigt werden sollen.</p></div><div class=paragraph><p>Noch mehr Informationen zu I/O-Statistiken können durch Angabe der Option <code>-v</code> angezeigt werden. Jedes Gerät im Pool wird dann mit einer eigenen Statistikzeile aufgeführt. Dies ist hilfreich um zu sehen, wieviele Lese- und Schreiboperationen von jedem Gerät durchgeführt werden und kann bei der Diagnose eines langsamen Geräts, das den Pool ausbremst, hilfreich sein. Dieses Beispiel zeigt einen gespiegelten Pool mit zwei Geräten:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool iostat -v</span>
                            capacity     operations    bundwidth
pool                     alloc   free   <span class=nb>read  </span>write   <span class=nb>read  </span>write
<span class=nt>-----------------------</span>  <span class=nt>-----</span>  <span class=nt>-----</span>  <span class=nt>-----</span>  <span class=nt>-----</span>  <span class=nt>-----</span>  <span class=nt>-----</span>
data                      288G  1.53T      2     12  9.23K  61.5K
  mirror                  288G  1.53T      2     12  9.23K  61.5K
    ada1                     -      -      0      4  5.61K  61.7K
    ada2                     -      -      1      4  5.04K  61.7K
<span class=nt>-----------------------</span>  <span class=nt>-----</span>  <span class=nt>-----</span>  <span class=nt>-----</span>  <span class=nt>-----</span>  <span class=nt>-----</span>  <span class=nt>-----</span></code></pre></div></div></div><div class=sect2><h3 id=zfs-zpool-split>19.3.14. Einen Pool aufteilen<a class=anchor href=#zfs-zpool-split></a></h3><div class=paragraph><p>Ein Pool, der aus einem oder mehreren gespiegelten vdevs besteht, kann in zwei Pools aufgespalten werden. Falls nicht anders angegeben, wird das letzte Mitglied eines Spiegels abgehängt und dazu verwendet, einen neuen Pool mit den gleichen Daten zu erstellen. Die Operation sollte zuerst mit der Option <code>-n</code> versucht werden. Die Details der vorgeschlagenen Option werden dargestellt, ohne die Aktion in Wirklichkeit durchzuführen. Das hilft dabei zu bestätigen, ob die Aktion das tut, was der Benutzer damit vor hatte.</p></div></div></div></div><div class=sect1><h2 id=zfs-zfs>19.4. <code>zfs</code> Administration<a class=anchor href=#zfs-zfs></a></h2><div class=sectionbody><div class=paragraph><p>Das <code>zfs</code>-Werkzeug ist dafür verantwortlich, alle ZFS Datasets innerhalb eines Pools zu erstellen, zerstören und zu verwalten. Der Pool selbst wird durch <a href=#zfs-zpool><code>zpool</code></a> verwaltet.</p></div><div class=sect2><h3 id=zfs-zfs-create>19.4.1. Datasets erstellen und zerstören<a class=anchor href=#zfs-zfs-create></a></h3><div class=paragraph><p>Anders als in traditionellen Festplatten- und Volumenmanagern wird der Plattenplatz in ZFS <em>nicht</em> vorher allokiert. Bei traditionellen Dateisystemen gibt es, nachdem der Plattenplatz partitioniert und zugeteilt wurde, keine Möglichkeit, ein zusätzliches Dateisystem hinzuzufügen, ohne eine neue Platte anzuschließen. Mit ZFS lassen sich neue Dateisysteme zu jeder Zeit anlegen. Jedes <a href=#zfs-term-dataset><em>Dataset</em></a> besitzt Eigenschaften wie Komprimierung, Deduplizierung, Zwischenspeicher (caching), Quotas, genauso wie andere nützliche Einstellungen wie Schreibschutz, Unterscheidung zwischen Groß- und Kleinschreibung, Netzwerkfreigaben und einen Einhängepunkt. Datasets können ineinander verschachtelt werden und Kind-Datasets erben die Eigenschaften ihrer Eltern. Jedes Dataset kann als eine Einheit verwaltet, <a href=#zfs-zfs-allow>delegiert</a>, <a href=#zfs-zfs-send>repliziert</a>, <a href=#zfs-zfs-snapshot>mit Schnappschüssen versehen</a>, <a href=#zfs-zfs-jail>in Jails gesteckt</a> und zerstört werden. Es gibt viele Vorteile, ein separates Dataset für jede Art von Dateien anzulegen. Der einzige Nachteil einer großen Menge an Datasets ist, dass manche Befehle wie <code>zfs list</code> langsamer sind und dass das Einhängen von hunderten oder hunderttausenden von Datasets den FreeBSD-Bootvorgang verzögert.</p></div><div class=paragraph><p>Erstellen eines neuen Datasets und aktivieren von <a href=#zfs-term-compression-lz4>LZ4 Komprimierung</a>:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs list</span>
NAME                  USED  AVAIL  REFER  MOUNTPOINT
mypool                781M  93.2G   144K  none
mypool/ROOT           777M  93.2G   144K  none
mypool/ROOT/default   777M  93.2G   777M  /
mypool/tmp            176K  93.2G   176K  /tmp
mypool/usr            616K  93.2G   144K  /usr
mypool/usr/home       184K  93.2G   184K  /usr/home
mypool/usr/ports      144K  93.2G   144K  /usr/ports
mypool/usr/src        144K  93.2G   144K  /usr/src
mypool/var           1.20M  93.2G   608K  /var
mypool/var/crash      148K  93.2G   148K  /var/crash
mypool/var/log        178K  93.2G   178K  /var/log
mypool/var/mail       144K  93.2G   144K  /var/mail
mypool/var/tmp        152K  93.2G   152K  /var/tmp
<span class=c># zfs create -o compress=lz4 mypool/usr/mydataset</span>
<span class=c># zfs list</span>
NAME                   USED  AVAIL  REFER  MOUNTPOINT
mypool                 781M  93.2G   144K  none
mypool/ROOT            777M  93.2G   144K  none
mypool/ROOT/default    777M  93.2G   777M  /
mypool/tmp             176K  93.2G   176K  /tmp
mypool/usr             704K  93.2G   144K  /usr
mypool/usr/home        184K  93.2G   184K  /usr/home
mypool/usr/mydataset  87.5K  93.2G  87.5K  /usr/mydataset
mypool/usr/ports       144K  93.2G   144K  /usr/ports
mypool/usr/src         144K  93.2G   144K  /usr/src
mypool/var            1.20M  93.2G   610K  /var
mypool/var/crash       148K  93.2G   148K  /var/crash
mypool/var/log         178K  93.2G   178K  /var/log
mypool/var/mail        144K  93.2G   144K  /var/mail
mypool/var/tmp         152K  93.2G   152K  /var/tmp</code></pre></div></div><div class=paragraph><p>Ein Dataset zu zerstören ist viel schneller, als alle Dateien zu löschen, die sich in dem Dataset befindet, da es keinen Scan aller Dateien und aktualisieren der dazugehörigen Metadaten erfordert.</p></div><div class=paragraph><p>Zerstören des zuvor angelegten Datasets:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs list</span>
NAME                   USED  AVAIL  REFER  MOUNTPOINT
mypool                 880M  93.1G   144K  none
mypool/ROOT            777M  93.1G   144K  none
mypool/ROOT/default    777M  93.1G   777M  /
mypool/tmp             176K  93.1G   176K  /tmp
mypool/usr             101M  93.1G   144K  /usr
mypool/usr/home        184K  93.1G   184K  /usr/home
mypool/usr/mydataset   100M  93.1G   100M  /usr/mydataset
mypool/usr/ports       144K  93.1G   144K  /usr/ports
mypool/usr/src         144K  93.1G   144K  /usr/src
mypool/var            1.20M  93.1G   610K  /var
mypool/var/crash       148K  93.1G   148K  /var/crash
mypool/var/log         178K  93.1G   178K  /var/log
mypool/var/mail        144K  93.1G   144K  /var/mail
mypool/var/tmp         152K  93.1G   152K  /var/tmp
<span class=c># zfs destroy mypool/usr/mydataset</span>
<span class=c># zfs list</span>
NAME                  USED  AVAIL  REFER  MOUNTPOINT
mypool                781M  93.2G   144K  none
mypool/ROOT           777M  93.2G   144K  none
mypool/ROOT/default   777M  93.2G   777M  /
mypool/tmp            176K  93.2G   176K  /tmp
mypool/usr            616K  93.2G   144K  /usr
mypool/usr/home       184K  93.2G   184K  /usr/home
mypool/usr/ports      144K  93.2G   144K  /usr/ports
mypool/usr/src        144K  93.2G   144K  /usr/src
mypool/var           1.21M  93.2G   612K  /var
mypool/var/crash      148K  93.2G   148K  /var/crash
mypool/var/log        178K  93.2G   178K  /var/log
mypool/var/mail       144K  93.2G   144K  /var/mail
mypool/var/tmp        152K  93.2G   152K  /var/tmp</code></pre></div></div><div class=paragraph><p>In modernen Versionen von ZFS ist <code>zfs destroy</code> asynchron und der freie Speicherplatz kann erst nach ein paar Minuten im Pool auftauchen. Verwenden Sie <code>zpool get freeing <em>poolname</em></code>, um die Eigenschaft <code>freeing</code> aufzulisten, die angibt, bei wievielen Datasets die Blöcke im Hintergrund freigegeben werden. Sollte es Kind-Datasets geben, <a href=#zfs-term-snapshot>Schnappschüsse</a> oder andere Datasets, dann lässt sich der Elternknoten nicht zerstören. Um ein Dataset und all seine Kinder zu zerstören, verwenden Sie die Option <code>-r</code>, um das Dataset und all seine Kinder rekursiv zu entfernen. Benutzen Sie die Option <code>-n</code> und <code>-v</code>, um Datasets und Snapshots anzuzeigen, die durch diese Aktion zerstört werden würden, dies jedoch nur zu simulieren und nicht wirklich durchzuführen. Speicherplatz, der dadurch freigegeben würde, wird ebenfalls angezeigt.</p></div></div><div class=sect2><h3 id=zfs-zfs-volume>19.4.2. Volumes erstellen und zerstören<a class=anchor href=#zfs-zfs-volume></a></h3><div class=paragraph><p>Ein Volume ist ein spezieller Typ von Dataset. Anstatt dass es als Dateisystem eingehängt wird, stellt es ein Block-Gerät unter <span class=filename>/dev/zvol/poolname/dataset</span> dar. Dies erlaubt es, das Volume für andere Dateisysteme zu verwenden, die Festplatten einer virtuellen Maschine bereitzustellen oder über Protokolle wie iSCSI oder HAST exportiert zu werden.</p></div><div class=paragraph><p>Ein Volume kann mit einem beliebigen Dateisystem formatiert werden oder auch ohne ein Dateisystem als reiner Datenspeicher fungieren. Für den Benutzer erscheint ein Volume als eine gewöhnliche Platte. Indem gewöhnliche Dateisysteme auf diesen <em>zvols</em> angelegt werden, ist es möglich, diese mit Eigenschaften auszustatten, welche diese normalerweise nicht besitzen. Beispielsweise wird durch Verwendung der Komprimierungseigenschaft auf einem 250 MB Volume das Erstellen eines komprimierten FAT Dateisystems möglich.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs create -V 250m -o compression=on tank/fat32</span>
<span class=c># zfs list tank</span>
NAME USED AVAIL REFER MOUNTPOINT
tank 258M  670M   31K /tank
<span class=c># newfs_msdos -F32 /dev/zvol/tank/fat32</span>
<span class=c># mount -t msdosfs /dev/zvol/tank/fat32 /mnt</span>
<span class=c># df -h /mnt | grep fat32</span>
Filesystem           Size Used Avail Capacity Mounted on
/dev/zvol/tank/fat32 249M  24k  249M     0%   /mnt
<span class=c># mount | grep fat32</span>
/dev/zvol/tank/fat32 on /mnt <span class=o>(</span>msdosfs, <span class=nb>local</span><span class=o>)</span></code></pre></div></div><div class=paragraph><p>Ein Volume zu zerstören ist sehr ähnlich wie ein herkömmliches Dataset zu entfernen. Die Operation wird beinahe sofort durchgeführt, jedoch kann es mehrere Minuten dauern, bis der freie Speicherplatz im Hintergrund wieder freigegeben ist.</p></div></div><div class=sect2><h3 id=zfs-zfs-rename>19.4.3. Umbenennen eines Datasets<a class=anchor href=#zfs-zfs-rename></a></h3><div class=paragraph><p>Der Name eines Datasets lässt sich durch <code>zfs rename</code> ändern. Das Eltern-Dataset kann ebenfalls mit diesem Kommando umbenannt werden. Ein Dataset unter einem anderen Elternteil umzubenennen wird den Wert dieser Eigenschaft verändern, die vom Elternteil vererbt wurden. Wird ein Dataset umbenannt, wird es abgehängt und dann erneut unter der neuen Stelle eingehängt (welche vom neuen Elternteil geerbt wird). Dieses Verhalten kann durch die Option <code>-u</code> verhindert werden.</p></div><div class=paragraph><p>Ein Dataset umbenennen und unter einem anderen Elterndataset verschieben:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs list</span>
NAME                   USED  AVAIL  REFER  MOUNTPOINT
mypool                 780M  93.2G   144K  none
mypool/ROOT            777M  93.2G   144K  none
mypool/ROOT/default    777M  93.2G   777M  /
mypool/tmp             176K  93.2G   176K  /tmp
mypool/usr             704K  93.2G   144K  /usr
mypool/usr/home        184K  93.2G   184K  /usr/home
mypool/usr/mydataset  87.5K  93.2G  87.5K  /usr/mydataset
mypool/usr/ports       144K  93.2G   144K  /usr/ports
mypool/usr/src         144K  93.2G   144K  /usr/src
mypool/var            1.21M  93.2G   614K  /var
mypool/var/crash       148K  93.2G   148K  /var/crash
mypool/var/log         178K  93.2G   178K  /var/log
mypool/var/mail        144K  93.2G   144K  /var/mail
mypool/var/tmp         152K  93.2G   152K  /var/tmp
<span class=c># zfs rename mypool/usr/mydataset mypool/var/newname</span>
<span class=c># zfs list</span>
NAME                  USED  AVAIL  REFER  MOUNTPOINT
mypool                780M  93.2G   144K  none
mypool/ROOT           777M  93.2G   144K  none
mypool/ROOT/default   777M  93.2G   777M  /
mypool/tmp            176K  93.2G   176K  /tmp
mypool/usr            616K  93.2G   144K  /usr
mypool/usr/home       184K  93.2G   184K  /usr/home
mypool/usr/ports      144K  93.2G   144K  /usr/ports
mypool/usr/src        144K  93.2G   144K  /usr/src
mypool/var           1.29M  93.2G   614K  /var
mypool/var/crash      148K  93.2G   148K  /var/crash
mypool/var/log        178K  93.2G   178K  /var/log
mypool/var/mail       144K  93.2G   144K  /var/mail
mypool/var/newname   87.5K  93.2G  87.5K  /var/newname
mypool/var/tmp        152K  93.2G   152K  /var/tmp</code></pre></div></div><div class=paragraph><p>Schnappschüsse können auf diese Weise ebenfalls umbenannt werden. Aufgrund der Art von Schnappschüssen können diese nicht unter einem anderen Elterndataset eingehängt werden. Um einen rekursiven Schnappschuss umzubenennen, geben Sie die Option <code>-r</code> an, um alle Schnappschüsse mit dem gleichen Namen im Kind-Dataset ebenfalls umzubenennen.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs list -t snapshot</span>
NAME                                USED  AVAIL  REFER  MOUNTPOINT
mypool/var/newname@first_snapshot      0      -  87.5K  -
<span class=c># zfs rename mypool/var/newname@first_snapshot new_snapshot_name</span>
<span class=c># zfs list -t snapshot</span>
NAME                                   USED  AVAIL  REFER  MOUNTPOINT
mypool/var/newname@new_snapshot_name      0      -  87.5K  -</code></pre></div></div></div><div class=sect2><h3 id=zfs-zfs-set>19.4.4. Festlegen von Dataset-Eigenschaften<a class=anchor href=#zfs-zfs-set></a></h3><div class=paragraph><p>Jedes ZFS-Dataset besitzt eine Menge von Eigenschaften, die sein Verhalten beeinflussen. Die meisten Eigenschaften werden automatisch vom Eltern-Dataset vererbt, können jedoch lokal überschrieben werden. Sie legen eine Eigenschaft durch <code>zfs set <em>property=value dataset</em></code> fest. Die meisten Eigenschaften haben eine begrenzte Menge von gültigen Werten. <code>zfs get</code> stellt diese dar und zeigt jede mögliche Eigenschaft und gültige Werte an. Die meisten Eigenschaften können über <code>zfs inherit</code> wieder auf ihren Ausgangswert zurückgesetzt werden.</p></div><div class=paragraph><p>Benutzerdefinierte Eigenschaften lassen sich ebenfalls setzen. Diese werden Teil der Konfiguration des Datasets und können dazu verwendet werden, zusätzliche Informationen über das Dataset oder seine Bestandteile zu speichern. Um diese benutzerdefinierten Eigenschaften von den ZFS-eigenen zu unterscheiden, wird ein Doppelpunkt (<code>:</code>) verwendet, um einen eigenen Namensraum für diese Eigenschaft zu erstellen.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set custom:costcenter=1234 tank</span>
<span class=c># zfs get custom:costcenter tank</span>
NAME PROPERTY           VALUE SOURCE
tank custom:costcenter  1234  <span class=nb>local</span></code></pre></div></div><div class=paragraph><p>Um eine selbstdefinierte Eigenschaft umzubenennen, verwenden Sie <code>zfs inherit</code> mit der Option <code>-r</code>. Wenn die benutzerdefinierte Eigenschaft nicht in einem der Eltern-Datasets definiert ist, wird diese komplett entfernt (obwohl diese Änderungen natürlich in der Historie des Pools noch aufgezeichnet sind).</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs inherit -r custom:costcenter tank</span>
<span class=c># zfs get custom:costcenter tank</span>
NAME    PROPERTY           VALUE              SOURCE
tank    custom:costcenter  -                  -
<span class=c># zfs get all tank | grep custom:costcenter</span>
<span class=c>#</span></code></pre></div></div><div class=sect3><h4 id=zfs-zfs-set-share>19.4.4.1. Festlegen und Abfragen von Eigenschaften für Freigaben<a class=anchor href=#zfs-zfs-set-share></a></h4><div class=paragraph><p>Zwei häufig verwendete und nützliche Dataset-Eigenschaften sind die Freigabeoptionen von NFS und SMB. Diese Optionen legen fest, ob und wie ZFS-Datasets im Netzwerk freigegeben werden. Derzeit unterstützt FreeBSD nur Freigaben von Datasets über NFS. Um den Status einer Freigabe zu erhalten, geben Sie folgendes ein:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs get sharenfs mypool/usr/home</span>
NAME              PROPERTY   VALUE   SOURCE
mypool/usr/home   sharenfs   on      <span class=nb>local</span>
<span class=c># zfs get sharesmb mypool/usr/home</span>
NAME              PROPERTY   VALUE   SOURCE
mypool/usr/home   sharesmb   off     <span class=nb>local</span></code></pre></div></div><div class=paragraph><p>Um ein Dataset freizugeben, geben Sie ein:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set sharenfs=on mypool/usr/home</span></code></pre></div></div><div class=paragraph><p>Es ist auch möglich, weitere Optionen für die Verwendung von Datasets über NFS zu definieren, wie etwa <code>-alldirs</code>, <code>-maproot</code> und <code>-network</code>. Um zusätzliche Optionen auf ein durch NFS freigegebenes Dataset festzulegen, geben Sie ein:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set sharenfs=&#34;-alldirs,maproot=root,-network=192.168.1.0/24&#34; mypool/usr/home</span></code></pre></div></div></div></div><div class=sect2><h3 id=zfs-zfs-snapshot>19.4.5. Verwalten von Schnappschüssen<a class=anchor href=#zfs-zfs-snapshot></a></h3><div class=paragraph><p><a href=#zfs-term-snapshot>Schnappschüsse</a> sind eine der mächtigen Eigenschaften von ZFS. Ein Schnappschuss bietet einen nur-Lese Zustand eines Datasets zu einem bestimmten Zeitpunkt. Mit Kopieren-beim-Schreiben (Copy-On-Write COW), können Schnappschüsse schnell erstellt werden durch das Aufheben der älteren Version der Daten auf der Platte. Falls kein Snapshot existiert, wird der Speicherplatz wieder für zukünftige Verwendung freigegeben wenn Daten geschrieben oder gelöscht werden. Schnappschüsse sparen Speicherplatz, indem diese nur die Unterschiede zwischen dem momentanen Dataset und der vorherigen Version aufzeichnen. Schnappschüsse sind nur auf ganzen Datasets erlaubt, nicht auf individuellen Dateien oder Verzeichnissen. Wenn ein Schnappschuss eines Datasets erstellt wird, wird alles was darin enthalten ist, dupliziert. Das beinhaltet Dateisystemeigenschaften, Dateien, Verzeichnisse, Rechte und so weiter. Schnappschüsse benötigen keinen zusätzlichen Speicherplatz wenn diese erstmals angelegt werden, nur wenn Blöcke, die diese referenzieren, geändert werden. Rekursive Schnappschüsse, die mit der Option <code>-r</code> erstellt, erzeugen einen mit dem gleichen Namen des Datasets und all seinen Kindern, was eine konsistente Momentaufnahme aller Dateisysteme darstellt. Dies kann wichtig sein, wenn eine Anwendung Dateien auf mehreren Datasets ablegt, die miteinander in Verbindung stehen oder voneinander abhängig sind. Ohne Schnappschüsse würde ein Backup Kopien dieser Dateien zu unterschiedlichen Zeitpunkten enthalten.</p></div><div class=paragraph><p>Schnappschüsse in ZFS bieten eine Vielzahl von Eigenschaften, die selbst in anderen Dateisystemen mit Schnappschussfunktion nicht vorhanden sind. Ein typisches Beispiel zur Verwendung von Schnappschüssen ist, den momentanen Stand des Dateisystems zu sichern, wenn eine riskante Aktion wie das Installieren von Software oder eine Systemaktualisierung durchgeführt wird. Wenn diese Aktion fehlschlägt, so kann der Schnappschuss zurückgerollt werden und das System befindet sich wieder in dem gleichen Zustand, wie zu dem, als der Schnappschuss erstellt wurde. Wenn die Aktualisierung jedoch erfolgreich war, kann der Schnappschuss gelöscht werden, um Speicherplatz frei zu geben. Ohne Schnappschüsse, wird durch ein fehlgeschlagenes Update eine Wiederherstellung der Sicherung fällig, was oft mühsam und zeitaufwändig ist, außerdem ist währenddessen das System nicht verwendbar. Schnappschüsse lassen sich schnell und mit wenig bis gar keiner Ausfallzeit zurückrollen, selbst wenn das System im normalen Betrieb läuft. Die Zeitersparnis ist enorm, wenn mehrere Terabyte große Speichersysteme eingesetzt werden und viel Zeit für das Kopieren der Daten vom Sicherungssystem benötigt wird. Schnappschüsse sind jedoch keine Ersatz für eine Vollsicherung des Pools, können jedoch als eine schnelle und einfache Sicherungsmethode verwendet werden, um eine Kopie eines Datasets zu einem bestimmten Zeitpunkt zu sichern.</p></div><div class=sect3><h4 id=zfs-zfs-snapshot-creation>19.4.5.1. Schnappschüsse erstellen<a class=anchor href=#zfs-zfs-snapshot-creation></a></h4><div class=paragraph><p>Schnappschüsse werden durch das Kommando <code>zfs snapshot <em>dataset</em>@<em>snapshotname</em></code> angelegt. Durch Angabe der Option <code>-r</code> werden Schnappschüsse rekursive angelegt, mit dem gleichen Namen auf allen Datasets.</p></div><div class=paragraph><p>Einen rekursiven Schnappschuss des gesamten Pools erzeugen:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs list -t all</span>
NAME                                   USED  AVAIL  REFER  MOUNTPOINT
mypool                                 780M  93.2G   144K  none
mypool/ROOT                            777M  93.2G   144K  none
mypool/ROOT/default                    777M  93.2G   777M  /
mypool/tmp                             176K  93.2G   176K  /tmp
mypool/usr                             616K  93.2G   144K  /usr
mypool/usr/home                        184K  93.2G   184K  /usr/home
mypool/usr/ports                       144K  93.2G   144K  /usr/ports
mypool/usr/src                         144K  93.2G   144K  /usr/src
mypool/var                            1.29M  93.2G   616K  /var
mypool/var/crash                       148K  93.2G   148K  /var/crash
mypool/var/log                         178K  93.2G   178K  /var/log
mypool/var/mail                        144K  93.2G   144K  /var/mail
mypool/var/newname                    87.5K  93.2G  87.5K  /var/newname
mypool/var/newname@new_snapshot_name      0      -  87.5K  -
mypool/var/tmp                         152K  93.2G   152K  /var/tmp
<span class=c># zfs snapshot -r mypool@my_recursive_snapshot</span>
<span class=c># zfs list -t snapshot</span>
NAME                                        USED  AVAIL  REFER  MOUNTPOINT
mypool@my_recursive_snapshot                   0      -   144K  -
mypool/ROOT@my_recursive_snapshot              0      -   144K  -
mypool/ROOT/default@my_recursive_snapshot      0      -   777M  -
mypool/tmp@my_recursive_snapshot               0      -   176K  -
mypool/usr@my_recursive_snapshot               0      -   144K  -
mypool/usr/home@my_recursive_snapshot          0      -   184K  -
mypool/usr/ports@my_recursive_snapshot         0      -   144K  -
mypool/usr/src@my_recursive_snapshot           0      -   144K  -
mypool/var@my_recursive_snapshot               0      -   616K  -
mypool/var/crash@my_recursive_snapshot         0      -   148K  -
mypool/var/log@my_recursive_snapshot           0      -   178K  -
mypool/var/mail@my_recursive_snapshot          0      -   144K  -
mypool/var/newname@new_snapshot_name           0      -  87.5K  -
mypool/var/newname@my_recursive_snapshot       0      -  87.5K  -
mypool/var/tmp@my_recursive_snapshot           0      -   152K  -</code></pre></div></div><div class=paragraph><p>Schnappschüsse werden nicht durch einen <code>zfs list</code>-Befehl angezeigt. Um Schnappschüsse mit aufzulisten, muss <code>-t snapshot</code> an das Kommando <code>zfs list</code> angehängt werden. Durch <code>-t all</code> werden sowohl Dateisysteme als auch Schnappschüsse nebeneinander angezeigt.</p></div><div class=paragraph><p>Schnappschüsse werden nicht direkt eingehängt, deshalb wird auch kein Pfad in der Spalte <code>MOUNTPOINT</code> angezeigt. Ebenso wird kein freier Speicherplatz in der Spalte <code>AVAIL</code> aufgelistet, da Schnappschüsse nicht mehr geschrieben werden können, nachdem diese angelegt wurden. Vergleichen Sie den Schnappschuss mit dem ursprünglichen Dataset von dem es abstammt:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs list -rt all mypool/usr/home</span>
NAME                                    USED  AVAIL  REFER  MOUNTPOINT
mypool/usr/home                         184K  93.2G   184K  /usr/home
mypool/usr/home@my_recursive_snapshot      0      -   184K  -</code></pre></div></div><div class=paragraph><p>Durch das Darstellen des Datasets und des Schnappschusses nebeneinander zeigt deutlich, wie Schnappschüsse in <a href=#zfs-term-cow>COW</a> Manier funktionieren. Sie zeichnen nur die Änderungen (<em>delta</em>) auf, die währenddessen entstanden sind und nicht noch einmal den gesamten Inhalt des Dateisystems. Das bedeutet, dass Schnappschüsse nur wenig Speicherplatz benötigen, wenn nur kleine Änderungen vorgenommen werden. Der Speicherverbrauch kann sogar noch deutlicher gemacht werden, wenn eine Datei auf das Dataset kopiert wird und anschließend ein zweiter Schnappschuss angelegt wird:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># cp /etc/passwd /var/tmp</span>
<span class=c># zfs snapshot mypool/var/tmp@after_cp</span>
<span class=c># zfs list -rt all mypool/var/tmp</span>
NAME                                   USED  AVAIL  REFER  MOUNTPOINT
mypool/var/tmp                         206K  93.2G   118K  /var/tmp
mypool/var/tmp@my_recursive_snapshot    88K      -   152K  -
mypool/var/tmp@after_cp                   0      -   118K  -</code></pre></div></div><div class=paragraph><p>Der zweite Schnappschuss enthält nur die Änderungen am Dataset, die nach der Kopieraktion gemacht wurden. Dies bedeutet enorme Einsparungen von Speicherplatz. Beachten Sie, dass sich die Größe des Schnappschusses <code><em>mypool/var/tmp@my_recursive_snapshot</em></code> in der Spalte <code>USED</code> ebenfalls geändert hat, um die Änderungen von sich selbst und dem Schnappschuss, der im Anschluss angelegt wurde, anzuzeigen.</p></div></div><div class=sect3><h4 id=zfs-zfs-snapshot-diff>19.4.5.2. Schnappschüsse vergleichen<a class=anchor href=#zfs-zfs-snapshot-diff></a></h4><div class=paragraph><p>ZFS enthält ein eingebautes Kommando, um die Unterschiede zwischen zwei Schnappschüssen miteinander zu vergleichen. Das ist hilfreich, wenn viele Schnappschüsse über längere Zeit angelegt wurden und der Benutzer sehen will, wie sich das Dateisystem über diesen Zeitraum verändert hat. Beispielsweise kann <code>zfs diff</code> den letzten Schnappschuss finden, der noch eine Datei enthält, die aus Versehen gelöscht wurde. Wenn dies für die letzten beiden Schnappschüsse aus dem vorherigen Abschnitt durchgeführt wird, ergibt sich folgende Ausgabe:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs list -rt all mypool/var/tmp</span>
NAME                                   USED  AVAIL  REFER  MOUNTPOINT
mypool/var/tmp                         206K  93.2G   118K  /var/tmp
mypool/var/tmp@my_recursive_snapshot    88K      -   152K  -
mypool/var/tmp@after_cp                   0      -   118K  -
<span class=c># zfs diff mypool/var/tmp@my_recursive_snapshot</span>
M       /var/tmp/
+       /var/tmp/passwd</code></pre></div></div><div class=paragraph><p>Das Kommando zeigt alle Änderungen zwischen dem angegebenen Schnappschuss (in diesem Fall <code><em>mypool/var/tmp@my_recursive_snapshot</em></code>) und dem momentan aktuellen Dateisystem. Die erste Spalte zeigt die Art der Änderung an:</p></div><table class="tableblock frame-all grid-all stretch informaltable"><col style=width:20%><col style=width:80%><tbody><tr><td class="tableblock halign-left valign-top"><p class=tableblock>+</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Das Verzeichnis oder die Datei wurde hinzugefügt.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock>-</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Das Verzeichnis oder die Datei wurde gelöscht.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock>M</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Das Verzeichnis oder die Datei wurde geändert.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock>R</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Das Verzeichnis oder die Datei wurde umbenannt.</p></td></tr></tbody></table><div class=paragraph><p>Vergleicht man die Ausgabe mit der Tabelle, wird klar, dass <span class=filename>passwd</span> hinzugefügt wurde, nachdem der Schnappschuss <code><em>mypool/var/tmp@my_recursive_snapshot</em></code> erstellt wurde. Das resultierte ebenfalls in einer Änderung am darüberliegenden Verzeichnis, das unter <code><em>/var/tmp</em></code> eingehängt ist.</p></div><div class=paragraph><p>Zwei Schnappschüsse zu vergleichen ist hilfreich, wenn die Replikationseigenschaft von ZFS verwendet wird, um ein Dataset auf einen anderen Host zu Sicherungszwecken übertragen.</p></div><div class=paragraph><p>Zwei Schnappschüsse durch die Angabe des kompletten Namens des Datasets und dem Namen des Schnappschusses beider Datasets vergleichen:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># cp /var/tmp/passwd /var/tmp/passwd.copy</span>
<span class=c># zfs snapshot mypool/var/tmp@diff_snapshot</span>
<span class=c># zfs diff mypool/var/tmp@my_recursive_snapshot mypool/var/tmp@diff_snapshot</span>
M       /var/tmp/
+       /var/tmp/passwd
+       /var/tmp/passwd.copy
<span class=c># zfs diff mypool/var/tmp@my_recursive_snapshot mypool/var/tmp@after_cp</span>
M       /var/tmp/
+       /var/tmp/passwd</code></pre></div></div><div class=paragraph><p>Ein Administrator, der für die Sicherung zuständig ist, kann zwei Schnappschüsse miteinander vergleichen, die vom sendenden Host empfangen wurden, um festzustellen, welche Änderungen am Dataset vorgenommen wurden. Lesen Sie dazu den Abschnitt <a href=#zfs-zfs-send>Replication</a> um weitere Informationen zu erhalten.</p></div></div><div class=sect3><h4 id=zfs-zfs-snapshot-rollback>19.4.5.3. Schnappschüsse zurückrollen<a class=anchor href=#zfs-zfs-snapshot-rollback></a></h4><div class=paragraph><p>Wenn zumindest ein Schnappschuss vorhanden ist, kann dieser zu einem beliebigen Zeitpunkt zurückgerollt werden. In den meisten Fällen passiert dies, wenn der aktuelle Zustand des Datasets nicht mehr benötigt wird und eine ältere Version bevorzugt wird. Szenarien wie lokale Entwicklungstests, die fehlgeschlagen sind, defekte Systemaktualisierungen, welche die Funktionalität des Gesamtsystems einschränken oder die Anforderung, versehentlich gelöschte Dateien oder Verzeichnisse wiederherzustellen, sind allgegenwärtig. Glücklicherweise ist das zurückrollen eines Schnappschusses so leicht wie die Eingabe von <code>zfs rollback <em>snapshotname</em></code>. Abhängig davon, wie viele Änderungen betroffen sind, wird diese Operation innerhalb einer gewissen Zeit abgeschlossen sein. Während dieser Zeit bleibt das Dataset in einem konsistenten Zustand, sehr ähnlich den ACID-Prinzipien, die eine Datenbank beim Zurückrollen entspricht. Während all dies passiert, ist das Dataset immer noch aktiv und erreichbar ohne dass eine Ausfallzeit nötig wäre. Sobald der Schnappschuss zurückgerollt wurde, besitzt das Dataset den gleichen Zustand, den es besaß, als der Schnappschuss angelegt wurde. Alle anderen Daten in diesem Dataset, die nicht Teil des Schnappschusses sind, werden verworfen. Einen Schnappschuss des aktuellen Zustandes des Datasets vor dem Zurückrollen anzulegen ist eine gute Idee, wenn hinterher noch Daten benötigt werden. Auf diese Weise kann der Benutzer vor und zurück zwischen den Schnappschüssen springen, ohne wertvolle Daten zu verlieren.</p></div><div class=paragraph><p>Im ersten Beispiel wird ein Schnappschuss aufgrund eines unvorsichtigen <code>rm</code>-Befehls zurückgerollt, der mehr Daten gelöscht hat, als vorgesehen.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs list -rt all mypool/var/tmp</span>
NAME                                   USED  AVAIL  REFER  MOUNTPOINT
mypool/var/tmp                         262K  93.2G   120K  /var/tmp
mypool/var/tmp@my_recursive_snapshot    88K      -   152K  -
mypool/var/tmp@after_cp               53.5K      -   118K  -
mypool/var/tmp@diff_snapshot              0      -   120K  -
<span class=c># ls /var/tmp</span>
passwd          passwd.copy     vi.recover
<span class=c># rm /var/tmp/passwd*</span>
<span class=c># ls /var/tmp</span>
vi.recover
<span class=c>#</span></code></pre></div></div><div class=paragraph><p>Zu diesem Zeitpunkt bemerkt der Benutzer, dass zuviele Dateien gelöscht wurden und möchte diese zurück haben. ZFS bietet eine einfache Möglichkeit, diese durch zurückrollen zurück zu bekommen, allerdings nur, wenn Schnappschüsse von wichtigen Daten regelmäßig angelegt werden. Um die Dateien zurückzuerhalten und vom letzten Schnappschuss wieder zu beginnen, geben Sie ein:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs rollback mypool/var/tmp@diff_snapshot</span>
<span class=c># ls /var/tmp</span>
passwd          passwd.copy     vi.recover</code></pre></div></div><div class=paragraph><p>Die Operation zum Zurückrollen versetzt das Dataset in den Zustand des letzten Schnappschusses zurück. Es ist ebenfalls möglich, zu einem Schnappschuss zurückzurollen, der viel früher angelegt wurde und es noch Schnappschüsse nach diesem gibt. Wenn Sie dies versuchen, gibt ZFS die folgende Warnung aus:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs list -rt snapshot mypool/var/tmp</span>
AME                                   USED  AVAIL  REFER  MOUNTPOINT
mypool/var/tmp@my_recursive_snapshot    88K      -   152K  -
mypool/var/tmp@after_cp               53.5K      -   118K  -
mypool/var/tmp@diff_snapshot              0      -   120K  -
<span class=c># zfs rollback mypool/var/tmp@my_recursive_snapshot</span>
cannot rollback to <span class=s1>&#39;mypool/var/tmp@my_recursive_snapshot&#39;</span>: more recent snapshots exist
use <span class=s1>&#39;-r&#39;</span> to force deletion of the following snapshots:
mypool/var/tmp@after_cp
mypool/var/tmp@diff_snapshot</code></pre></div></div><div class=paragraph><p>Diese Warnung bedeutet, dass noch Schnappschüsse zwischen dem momentanen Stand des Datasets und dem Schnappschuss, zu dem der Benutzer zurückrollen möchte, existieren. Um das Zurückrollen durchzuführen, müssen die Schnappschüsse gelöscht werden. ZFS kann nicht alle Änderungen zwischen verschiedenen Zuständen eines Datasets verfolgen, da Schnappschüsse nur gelesen werden können. ZFS wird nicht die betroffenen Schnappschüsse löschen, es sei denn, der Benutzer verwendet die Option <code>-r</code>, um anzugeben, dass dies die gewünschte Aktion ist. Falls dies der Fall ist und die Konsequenzen alle dazwischenliegenden Schnappschüsse zu verlieren verstanden wurden, kann der Befehl abgesetzt werden:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs rollback -r mypool/var/tmp@my_recursive_snapshot</span>
<span class=c># zfs list -rt snapshot mypool/var/tmp</span>
NAME                                   USED  AVAIL  REFER  MOUNTPOINT
mypool/var/tmp@my_recursive_snapshot     8K      -   152K  -
<span class=c># ls /var/tmp</span>
vi.recover</code></pre></div></div><div class=paragraph><p>Die Ausgabe von <code>zfs list -t snapshot</code> bestätigt, dass die dazwischenliegenden Schnappschüsse als Ergebnis von <code>zfs rollback -r</code> entfernt wurden.</p></div></div><div class=sect3><h4 id=zfs-zfs-snapshot-snapdir>19.4.5.4. Individuelle Dateien aus Schnappschüssen wiederherstellen<a class=anchor href=#zfs-zfs-snapshot-snapdir></a></h4><div class=paragraph><p>Schnappschüsse sind unter einem versteckten Verzeichnis unter dem Eltern-Dataset eingehängt: <span class=filename>.zfs/snapshots/snapshotname</span>. Standardmäßig werden diese Verzeichnisse nicht von einem gewöhnlichen <code>ls -a</code> angezeigt. Obwohl diese Verzeichnisse nicht angezeigt werden, sind diese trotzdem vorhanden und der Zugriff darauf erfolgt wie auf jedes andere Verzeichnis. Die Eigenschaft <code>snapdir</code> steuert, ob diese Verzeichnisse beim Auflisten eines Verzeichnisses angezeigt werden oder nicht. Das Einstellen der Eigenschaft auf den Wert <code>visible</code> erlaubt es, diese in der Ausgabe von <code>ls</code> und anderen Kommandos, die mit Verzeichnisinhalten umgehen können, anzuzeigen.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs get snapdir mypool/var/tmp</span>
NAME            PROPERTY  VALUE    SOURCE
mypool/var/tmp  snapdir   hidden   default
<span class=c># ls -a /var/tmp</span>
<span class=nb>.</span>               ..              passwd          vi.recover
<span class=c># zfs set snapdir=visible mypool/var/tmp</span>
<span class=c># ls -a /var/tmp</span>
<span class=nb>.</span>               ..              .zfs            passwd          vi.recover</code></pre></div></div><div class=paragraph><p>Einzelne Dateien lassen sich einfach auf einen vorherigen Stand wiederherstellen, indem diese aus dem Schnappschuss zurück in das Eltern-Dataset kopiert werden. Die Verzeichnisstruktur unterhalb von <span class=filename>.zfs/snapshot</span> enthält ein Verzeichnis, das exakt wie der Schnappschuss benannt ist, der zuvor angelegt wurde, um es einfacher zu machen, diese zu identifizieren. Im nächsten Beispiel wird angenommen, dass eine Datei aus dem versteckten <span class=filename>.zfs</span> Verzeichnis durch kopieren aus dem Schnappschuss, der die letzte Version dieser Datei enthielt, wiederhergestellt wird:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># rm /var/tmp/passwd</span>
<span class=c># ls -a /var/tmp</span>
<span class=nb>.</span>               ..              .zfs            vi.recover
<span class=c># ls /var/tmp/.zfs/snapshot</span>
after_cp                my_recursive_snapshot
<span class=c># ls /var/tmp/.zfs/snapshot/after_cp</span>
passwd          vi.recover
<span class=c># cp /var/tmp/.zfs/snapshot/after_cp/passwd /var/tmp</span></code></pre></div></div><div class=paragraph><p>Als <code>ls .zfs/snapshot</code> ausgeführt wurde, war die <code>snapdir</code>-Eigenschaft möglicherweise nicht auf hidden gesetzt, trotzdem ist es immer noch möglich, den Inhalt dieses Verzeichnisses aufzulisten. Es liegt am Administrator zu entscheiden, ob diese Verzeichnisse angezeigt werden soll. Es ist möglich, diese für bestimmte Datasets anzuzeigen und für andere zu verstecken. Das Kopieren von Dateien oder Verzeichnissen aus diesem versteckten <span class=filename>.zfs/snapshot</span> Verzeichnis ist einfach genug. Jedoch führt der umgekehrte Weg zu einem Fehler:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># cp /etc/rc.conf /var/tmp/.zfs/snapshot/after_cp/</span>
<span class=nb>cp</span>: /var/tmp/.zfs/snapshot/after_cp/rc.conf: Read-only file system</code></pre></div></div><div class=paragraph><p>Der Fehler erinnert den Benutzer daran, dass Schnappschüsse nur gelesen aber nicht mehr geändert werden können, nachdem diese angelegt wurden. Es können keine Dateien in diese Schnappschuss-Verzeichnisse kopiert oder daraus gelöscht werden, da dies sonst den Zustand des Datasets verändern würde, den sie repräsentieren.</p></div><div class=paragraph><p>Schnappschüsse verbrauchen Speicherplatz basierend auf der Menge an Änderungen, die am Eltern-Dataset durchgeführt wurden, seit der Zeit als der Schnappschuss erstellt wurde. Die Eigenschaft <code>written</code> eines Schnappschusses verfolgt, wieviel Speicherplatz vom Schnappschuss belegt wird.</p></div><div class=paragraph><p>Schnappschüsse werden zerstört und der belegte Platz wieder freigegeben durch den Befehl <code>zfs destroy <em>dataset</em>@<em>snapshot</em></code>. Durch hinzufügen von <code>-r</code> werden alle Schnappschüsse rekursiv gelöscht, die den gleichen Namen wie das Eltern-Dataset besitzen. Mit der Option <code>-n -v</code> wird eine Liste von Schnappschüssen, die gelöscht werden würden, zusammen mit einer geschätzten Menge an zurückgewonnenem Speicherplatz angezeigt, ohne die eigentliche Zerstöroperation wirklich durchzuführen.</p></div></div></div><div class=sect2><h3 id=zfs-zfs-clones>19.4.6. Klone verwalten<a class=anchor href=#zfs-zfs-clones></a></h3><div class=paragraph><p>Ein Klon ist eine Kopie eines Schnappschusses, der mehr wie ein reguläres Dataset behandelt wird. Im Gegensatz zu Schnappschüssen kann man von einem Klon nicht nur lesen, er ist eingehängt und kann seine eigenen Eigenschaften haben. Sobald ein Klon mittels <code>zfs clone</code> erstellt wurde, lässt sich der zugrundeliegende Schnappschuss nicht mehr zerstören. Die Eltern-/Kindbeziehung zwischen dem Klon und dem Schnappschuss kann über <code>zfs promote</code> aufgelöst werden. Nachdem ein Klon auf diese Weise befördert wurde, wird der Schnappschuss zum Kind des Klons, anstatt des ursprünglichen Datasets. Dies wird die Art und Weise, wie der Speicherplatz berechnet wird, verändern, jedoch nicht den bereits belegten Speicher anpassen. Der Klon kann an einem beliebigen Punkt innerhalb der ZFS-Dateisystemhierarchie eingehängt werden, nur nicht unterhalb der ursprünglichen Stelle des Schnappschusses.</p></div><div class=paragraph><p>Um diese Klon-Funktionalität zu demonstrieren, wird dieses Beispiel-Dataset verwendet:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs list -rt all camino/home/joe</span>
NAME                    USED  AVAIL  REFER  MOUNTPOINT
camino/home/joe         108K   1.3G    87K  /usr/home/joe
camino/home/joe@plans    21K      -  85.5K  -
camino/home/joe@backup    0K      -    87K  -</code></pre></div></div><div class=paragraph><p>Ein typischer Einsatzzweck für Klone ist das experimentieren mit einem bestimmten Dataset, während der Schnappschuss beibehalten wird für den Fall, dass etwas schiefgeht. Da Schnappschüsse nicht verändert werden können, wird ein Lese-/Schreibklon des Schnappschusses angelegt. Nachdem das gewünschte Ergebnis im Klon erreicht wurde, kann der Klon zu einem Dataset ernannt und das alte Dateisystem entfernt werden. Streng genommen ist das nicht nötig, da der Klon und das Dataset ohne Probleme miteinander koexistieren können.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs clone camino/home/joe@backup camino/home/joenew</span>
<span class=c># ls /usr/home/joe*</span>
/usr/home/joe:
backup.txz     plans.txt

/usr/home/joenew:
backup.txz     plans.txt
<span class=c># df -h /usr/home</span>
Filesystem          Size    Used   Avail Capacity  Mounted on
usr/home/joe        1.3G     31k    1.3G     0%    /usr/home/joe
usr/home/joenew     1.3G     31k    1.3G     0%    /usr/home/joenew</code></pre></div></div><div class=paragraph><p>Nachdem ein Klon erstellt wurde, stellt er eine exakte Kopie des Datasets zu dem Zeitpunkt dar, als der Schnappschuss angelegt wurde. Der Klon kann nun unabhängig vom ursprünglichen Dataset geändert werden. Die einzige Verbindung zwischen den beiden ist der Schnappschuss. ZFS zeichnet diese Verbindung in der Eigenschaft namens <code>origin</code> auf. Sobald die Abhängigkeit zwischen dem Schnappschuss und dem Klon durch das Befördern des Klons mittels <code>zfs promote</code> entfernt wurde, wird auch die <code>origin</code>-Eigenschaft des Klons entfernt, da es sich nun um ein eigenständiges Dataset handelt. Dieses Beispiel demonstriert dies:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs get origin camino/home/joenew</span>
NAME                  PROPERTY  VALUE                     SOURCE
camino/home/joenew    origin    camino/home/joe@backup    -
<span class=c># zfs promote camino/home/joenew</span>
<span class=c># zfs get origin camino/home/joenew</span>
NAME                  PROPERTY  VALUE   SOURCE
camino/home/joenew    origin    -       -</code></pre></div></div><div class=paragraph><p>Nachdem ein paar Änderungen, wie beispielsweise das Kopieren von <span class=filename>loader.conf</span> auf den beförderten Klon vorgenommen wurden, wird das alte Verzeichnis in diesem Fall überflüssig. Stattdessen kann der beförderte Klon diesen ersetzen. Dies kann durch zwei aufeinanderfolgende Befehl geschehen: <code>zfs destroy</code> auf dem alten Dataset und <code>zfs rename</code> auf dem Klon, um diesen genauso wie das alte Dataset zu benennen (es kann auch einen ganz anderen Namen erhalten).</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># cp /boot/defaults/loader.conf /usr/home/joenew</span>
<span class=c># zfs destroy -f camino/home/joe</span>
<span class=c># zfs rename camino/home/joenew camino/home/joe</span>
<span class=c># ls /usr/home/joe</span>
backup.txz     loader.conf     plans.txt
<span class=c># df -h /usr/home</span>
Filesystem          Size    Used   Avail Capacity  Mounted on
usr/home/joe        1.3G    128k    1.3G     0%    /usr/home/joe</code></pre></div></div><div class=paragraph><p>Der geklonte Schnappschuss wird jetzt wie ein gewöhnliches Dataset behandelt. Es enthält alle Daten aus dem ursprünglichen Schnappschuss inklusive der Dateien, die anschließend hinzugefügt wurden, wie <span class=filename>loader.conf</span>. Klone können in unterschiedlichen Szenarien eingesetzt werden, um nützliche Eigenschaften für ZFS-Anwender zur Verfügung zu stellen. Zum Beispiel können Jails als Schnappschüsse bereitgestellt werden, die verschiedene Arten von installierten Anwendungen anbieten. Anwender können diese Schnappschüsse klonen und ihre eigenen Anwendungen nach Belieben hinzufügen. Sobald sie mit den Änderungen zufrieden sind, können die Klone zu vollständigen Datasets ernannt werden und dem Anwender zur Verfügung gestellt werden, als würde es sich um echte Datasets handeln. Das spart Zeit und Administrationsaufwand, wenn diese Jails auf diese Weise zur Verfügung gestellt werden.</p></div></div><div class=sect2><h3 id=zfs-zfs-send>19.4.7. Replikation<a class=anchor href=#zfs-zfs-send></a></h3><div class=paragraph><p>Daten auf einem einzigen Pool an einem Platz aufzubewahren, setzt diese dem Risiko aus, gestohlen oder Opfer von Naturgewalten zu werden, sowie menschlichem Versagen auszusetzen. Regelmäßige Sicherungen des gesamten Pools ist daher unerlässlich. ZFS bietet eine Reihe von eingebauten Serialisierungsfunktionen an, die in der Lage ist, eine Repräsentation der Daten als Datenstrom auf die Standardausgabe zu schreiben. Mit dieser Methode ist es nicht nur möglich, die Daten auf einen anderen Pool zu schicken, der an das lokale System angeschlossen ist, sondern ihn auch über ein Netzwerk an ein anderes System zu senden. Schnappschüsse stellen dafür die Replikationsbasis bereit (lesen Sie dazu den Abschnitt zu <a href=#zfs-zfs-snapshot>ZFS snapshots</a>). Die Befehle, die für die Replikation verwendet werden, sind <code>zfs send</code> und <code>zfs receive</code>.</p></div><div class=paragraph><p>Diese Beispiele demonstrieren die Replikation von ZFS anhand dieser beiden Pools:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool list</span>
NAME    SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG   CAP  DEDUP  HEALTH  ALTROOT
backup  960M    77K   896M         -         -     0%    0%  1.00x  ONLINE  -
mypool  984M  43.7M   940M         -         -     0%    4%  1.00x  ONLINE  -</code></pre></div></div><div class=paragraph><p>Der Pool namens <em>mypool</em> ist der primäre Pool, auf den regelmäßig Daten geschrieben und auch wieder gelesen werden. Ein zweiter Pool, genannt <em>backup</em> wird verwendet, um als Reserve zu dienen im Falle, dass der primäre Pool nicht zur Verfügung steht. Beachten Sie, dass diese Ausfallsicherung nicht automatisch von ZFS durchgeführt wird, sondern manuell von einem Systemadministrator bei Bedarf eingerichtet werden muss. Ein Schnappschuss wird verwendet, um einen konsistenten Zustand des Dateisystems, das repliziert werden soll, zu erzeugen. Sobald ein Schnappschuss von <em>mypool</em> angelegt wurde, kann er auf den <em>backup</em>-Pool abgelegt werden. Nur Schnappschüsse lassen sich auf diese Weise replizieren. Änderungen, die seit dem letzten Schnappschuss entstanden sind, werden nicht mit repliziert.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs snapshot mypool@backup1</span>
<span class=c># zfs list -t snapshot</span>
NAME                    USED  AVAIL  REFER  MOUNTPOINT
mypool@backup1             0      -  43.6M  -</code></pre></div></div><div class=paragraph><p>Da nun ein Schnappschuss existiert, kann mit <code>zfs send</code> ein Datenstrom, der den Inhalt des Schnappschusses repräsentiert, erstellt werden. Dieser Datenstrom kann als Datei gespeichert oder von einem anderen Pool empfangen werden. Der Datenstrom wird auf die Standardausgabe geschrieben, muss jedoch in eine Datei oder in eine Pipe umgeleitet werden, sonst wird ein Fehler produziert:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs send mypool@backup1</span>
Error: Stream can not be written to a terminal.
You must redirect standard output.</code></pre></div></div><div class=paragraph><p>Um ein Dataset mit <code>zfs send</code> zu replizieren, leiten Sie dieses in eine Datei auf dem eingehängten Backup-Pool um. Stellen Sie sicher, dass der Pool genug freien Speicherplatz besitzt, um die Größe des gesendeten Schnappschusses aufzunehmen. Das beinhaltet alle Daten im Schnappschuss, nicht nur die Änderungen zum vorherigen Schnappschuss.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs send mypool@backup1 &gt; /backup/backup1</span>
<span class=c># zpool list</span>
NAME    SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
backup  960M  63.7M   896M         -         -     0%     6%  1.00x  ONLINE  -
mypool  984M  43.7M   940M         -         -     0%     4%  1.00x  ONLINE  -</code></pre></div></div><div class=paragraph><p>Das Kommando <code>zfs send</code> transferierte alle Daten im <em>backup1</em>-Schnappschuss auf den Pool namens <em>backup</em>. Erstellen und senden eines Schnappschusses kann automatisch von <a href="https://man.freebsd.org/cgi/man.cgi?query=cron&amp;sektion=8&amp;format=html">cron(8)</a> durchgeführt werden.</p></div><div class=paragraph><p>Anstatt die Sicherungen als Archivdateien zu speichern, kann ZFS diese auch als aktives Dateisystem empfangen, was es erlaubt, direkt auf die gesicherten Daten zuzugreifen. Um an die eigentlichen Daten in diesem Strom zu gelangen, wird <code>zfs receive</code> benutzt, um den Strom wieder in Dateien und Verzeichnisse umzuwandeln. Das Beispiel unten kombiniert <code>zfs send</code> und <code>zfs receive</code> durch eine Pipe, um die Daten von einem Pool auf den anderen zu kopieren. Die Daten können direkt auf dem empfangenden Pool verwendet werden, nachdem der Transfer abgeschlossen ist. Ein Dataset kann nur auf ein leeres Dataset repliziert werden.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs snapshot mypool@replica1</span>
<span class=c># zfs send -v mypool@replica1 | zfs receive backup/mypool</span>
send from @ to mypool@replica1 estimated size is 50.1M
total estimated size is 50.1M
TIME        SENT   SNAPSHOT

<span class=c># zpool list</span>
NAME    SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
backup  960M  63.7M   896M         -         -     0%     6%  1.00x  ONLINE  -
mypool  984M  43.7M   940M         -         -     0%     4%  1.00x  ONLINE  -</code></pre></div></div><div class=sect3><h4 id=zfs-send-incremental>19.4.7.1. Inkrementelle Sicherungen<a class=anchor href=#zfs-send-incremental></a></h4><div class=paragraph><p>Die Unterschiede zwischen zwei Schnappschüssen kann <code>zfs send</code> ebenfalls erkennen und nur diese übertragen. Dies spart Speicherplatz und Übertragungszeit. Beispielsweise:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs snapshot mypool@replica2</span>
<span class=c># zfs list -t snapshot</span>
NAME                    USED  AVAIL  REFER  MOUNTPOINT
mypool@replica1         5.72M      -  43.6M  -
mypool@replica2             0      -  44.1M  -
<span class=c># zpool list</span>
NAME    SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG   CAP  DEDUP  HEALTH  ALTROOT
backup  960M  61.7M   898M         -         -     0%    6%  1.00x  ONLINE  -
mypool  960M  50.2M   910M         -         -     0%    5%  1.00x  ONLINE  -</code></pre></div></div><div class=paragraph><p>Ein zweiter Schnappschuss genannt <em>replica2</em> wurde angelegt. Dieser zweite Schnappschuss enthält nur die Änderungen, die zwischen dem jetzigen Stand des Dateisystems und dem vorherigen Schnappschuss, <em>replica1</em>, vorgenommen wurden. Durch <code>zfs send -i</code> und die Angabe des Schnappschusspaares wird ein inkrementeller Replikationsstrom erzeugt, welcher nur die Daten enthält, die sich geändert haben. Das kann nur erfolgreich sein, wenn der initiale Schnappschuss bereits auf der Empfängerseite vorhanden ist.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs send -v -i mypool@replica1 mypool@replica2 | zfs receive /backup/mypool</span>
send from @replica1 to mypool@replica2 estimated size is 5.02M
total estimated size is 5.02M
TIME        SENT   SNAPSHOT

<span class=c># zpool list</span>
NAME    SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
backup  960M  80.8M   879M         -         -     0%     8%  1.00x  ONLINE  -
mypool  960M  50.2M   910M         -         -     0%     5%  1.00x  ONLINE  -

<span class=c># zfs list</span>
NAME                         USED  AVAIL  REFER  MOUNTPOINT
backup                      55.4M   240G   152K  /backup
backup/mypool               55.3M   240G  55.2M  /backup/mypool
mypool                      55.6M  11.6G  55.0M  /mypool

<span class=c># zfs list -t snapshot</span>
NAME                                         USED  AVAIL  REFER  MOUNTPOINT
backup/mypool@replica1                       104K      -  50.2M  -
backup/mypool@replica2                          0      -  55.2M  -
mypool@replica1                             29.9K      -  50.0M  -
mypool@replica2                                 0      -  55.0M  -</code></pre></div></div><div class=paragraph><p>Der inkrementelle Datenstrom wurde erfolgreich übertragen. Nur die Daten, die verändert wurden, sind übertragen worden, anstatt das komplette <em>replica1</em>. Nur die Unterschiede wurden gesendet, was weniger Zeit und Speicherplatz in Anspruch genommen hat, statt jedesmal den gesamten Pool zu kopieren. Das ist hilfreich wenn langsame Netzwerke oder Kosten für die übertragene Menge Bytes in Erwägung gezogen werden müssen.</p></div><div class=paragraph><p>Ein neues Dateisystem, <em>backup/mypool</em>, ist mit allen Dateien und Daten vom Pool <em>mypool</em> verfügbar. Wenn die Option <code>-P</code> angegeben wird, werden die Eigenschaften des Datasets kopiert, einschließlich der Komprimierungseinstellungen, Quotas und Einhängepunkte. Wird die Option <code>-R</code> verwendet, so werden alle Kind-Datasets des angegebenen Datasets kopiert, zusammen mit ihren Eigenschaften. Senden und Empfangen kann automatisiert werden, so dass regelmäßig Sicherungen auf dem zweiten Pool angelegt werden.</p></div></div><div class=sect3><h4 id=zfs-send-ssh>19.4.7.2. Sicherungen verschlüsselt über SSH senden<a class=anchor href=#zfs-send-ssh></a></h4><div class=paragraph><p>Datenströme über das Netzwerk zu schicken ist eine gute Methode, um Sicherungen außerhalb des Systems anzulegen. Jedoch ist dies auch mit einem Nachteil verbunden. Daten, die über die Leitung verschickt werden, sind nicht verschlüsselt, was es jedem erlaubt, die Daten abzufangen und die Ströme wieder zurück in Daten umzuwandeln, ohne dass der sendende Benutzer davon etwas merkt. Dies ist eine unerwünschte Situation, besonders wenn die Datenströme über das Internet auf ein entferntes System gesendet werden. SSH kann benutzt werden, um durch Verschlüsselung geschützte Daten über eine Netzwerkverbindung zu übertragen. Da ZFS nur die Anforderung hat, dass der Strom von der Standardausgabe umgeleitet wird, ist es relativ einfach, diesen durch SSH zu leiten. Um den Inhalt des Dateisystems während der Übertragung und auf dem entfernten System weiterhin verschlüsselt zu lassen, denken Sie über den Einsatz von <a href=https://wiki.freebsd.org/PEFS>PEFS</a> nach.</p></div><div class=paragraph><p>Ein paar Einstellungen und Sicherheitsvorkehrungen müssen zuvor abgeschlossen sein. Es werden hier nur die nötigen Schritte für die <code>zfs send</code>-Aktion gezeigt. Weiterführende Informationen zu SSH, gibt es im Kapitel <a href=../security/#openssh>OpenSSH</a>.</p></div><div class=paragraph><p>Die folgende Konfiguration wird benötigt:</p></div><div class=ulist><ul><li><p>Passwortloser SSH-Zugang zwischen dem sendenden und dem empfangenden Host durch den Einsatz von SSH-Schlüsseln.</p></li><li><p>Normalerweise werden die Privilegien des <code>root</code>-Benutzers gebraucht, um Strom zu senden und zu empfangen. Das beinhaltet das Anmelden auf dem empfangenden System als <code>root</code>. Allerdings ist das Anmelden als <code>root</code> aus Sicherheitsgründen standardmäßig deaktiviert. Mit <a href=#zfs-zfs-allow>ZFS Delegation</a> lassen sich nicht-<code>root</code>-Benutzer auf jedem System einrichten, welche die nötigen Rechte besitzen, um die Sende- und Empfangsoperation durchzuführen.</p></li><li><p>Auf dem sendenden System:</p><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs allow -u someuser send,snapshot mypool</span></code></pre></div></div></li><li><p>Um den Pool einzuhängen, muss der unprivilegierte Benutzer das Verzeichnis besitzen und gewöhnliche Benutzern muss die Erlaubnis gegeben werden, das Dateisystem einzuhängen. Auf dem empfangenden System nehmen Sie dazu die folgenden Einstellungen vor:</p><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># sysctl vfs.usermount=1</span>
vfs.usermount: 0 -&gt; 1
<span class=c># echo vfs.usermount=1 &gt;&gt; /etc/sysctl.conf</span>
<span class=c># zfs create recvpool/backup</span>
<span class=c># zfs allow -u someuser create,mount,receive recvpool/backup</span>
<span class=c># chown someuser /recvpool/backup</span></code></pre></div></div></li></ul></div><div class=paragraph><p>Der unprivilegierte Benutzer hat jetzt die Fähigkeit, Datasets zu empfangen und einzuhängen und das <em>home</em>-Dataset auf das entfernte System zu replizieren:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell>% zfs snapshot <span class=nt>-r</span> mypool/home@monday
% zfs send <span class=nt>-R</span> mypool/home@monday | ssh someuser@backuphost zfs recv <span class=nt>-dvu</span> recvpool/backup</code></pre></div></div><div class=paragraph><p>Ein rekursiver Schnappschuss namens <em>monday</em> wird aus dem Dataset <em>home</em> erstellt, dass auf dem Pool <em>mypool</em> liegt. Es wird dann mit <code>zfs send -R</code> gesendet, um das Dataset, alle seine Kinder, Schnappschüsse, Klone und Einstellungen in den Strom mit aufzunehmen. Die Ausgabe wird an das wartende System <em>backuphost</em> mittels <code>zfs receive</code> durch SSH umgeleitet. Die Verwendung des Fully Qulified Domänennamens oder der IP-Adresse wird empfohlen. Die empfangende Maschine schreibt die Daten auf das <em>backup</em>-Dataset im <em>recvpool</em>-Pool. Hinzufügen der Option <code>-d</code> zu <code>zfs recv</code> überschreibt den Namen des Pools auf der empfangenden Seite mit dem Namen des Schnappschusses. Durch Angabe von <code>-u</code> wird das Dateisystem nicht auf der Empfängerseite eingehängt. Wenn <code>-v</code> enthalten ist, werden mehr Details zum Transfer angezeigt werden, einschließlich der vergangenen Zeit und der Menge an übertragenen Daten.</p></div></div></div><div class=sect2><h3 id=zfs-zfs-quota>19.4.8. Dataset-, Benutzer- und Gruppenquotas<a class=anchor href=#zfs-zfs-quota></a></h3><div class=paragraph><p><a href=#zfs-term-quota>Dataset-Quotas</a> werden eingesetzt, um den Speicherplatz einzuschränken, den ein bestimmtes Dataset verbrauchen kann. <a href=#zfs-term-refquota>Referenz-Quotas</a> funktionieren auf eine ähnliche Weise, jedoch wird dabei der Speicherplatz des Datasets selbst gezählt, wobei Schnappschüsse und Kind-Datasets dabei ausgenommen sind. Ähnlich dazu werden <a href=#zfs-term-userquota>Benutzer</a>- und <a href=#zfs-term-groupquota>Gruppen</a>-Quotas dazu verwendet, um Benutzer oder Gruppen daran zu hindern, den gesamten Speicherplatz im Pool oder auf dem Dataset zu verbrauchen.</p></div><div class=paragraph><p>Die folgenden Beispiele gehen davon aus, dass die Benutzer bereits im System vorhanden sind. Bevor Sie einen Benutzer hinzufügen, stellen Sie sicher, dass Sie zuerst ein Dataset für das Heimatverzeichnis anlegen und den <code>mountpoint</code> auf <code>/home/<em>bob</em></code> festlegen. Legen Sie dann den Benutzer an und stellen Sie sicher, dass das Heimatverzeichnis auf den auf den <code>mountpoint</code> des Datasets verweist. Auf diese Weise werden die Eigentümer- und Gruppenberechtigungen richtig gesetzt, ohne dass bereits vorhandene Heimatverzeichnisse verschleiert werden.</p></div><div class=paragraph><p>Um ein 10 GB großes Quota auf dem Dataset <span class=filename>storage/home/bob</span> zu erzwingen, verwenden Sie folgenden Befehl:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set quota=10G storage/home/bob</span></code></pre></div></div><div class=paragraph><p>Um ein Referenzquota von 10 GB für <span class=filename>storage/home/bob</span> festzulegen, geben Sie ein:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set refquota=10G storage/home/bob</span></code></pre></div></div><div class=paragraph><p>Um das Quota für <span class=filename>storage/home/bob</span> wieder zu entfernen:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set quota=none storage/home/bob</span></code></pre></div></div><div class=paragraph><p>Das generelle Format ist <code>userquota@<em>user</em>=<em>size</em></code> und der Name des Benutzers muss in einem der folgenden Formate vorliegen:</p></div><div class=ulist><ul><li><p>POSIX-kompatibler Name wie <em>joe</em>.</p></li><li><p>POSIX-numerische ID wie <em>789</em>.</p></li><li><p>SID-Name wie <em>joe.bloggs@example.com</em>.</p></li><li><p>SID-numerische ID wie <em>S-1-123-456-789</em>.</p></li></ul></div><div class=paragraph><p>Um beispielsweise ein Benutzerquota von 50 GB für den Benutzer names <em>joe</em> zu erzwingen:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set userquota@joe=50G</span></code></pre></div></div><div class=paragraph><p>Um jegliche Quotas zu entfernen:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set userquota@joe=none</span></code></pre></div></div><div class="admonitionblock note"><table><tbody><tr><td class=icon><i class="fa icon-note" title=Note></i></td><td class=content><div class=paragraph><p>Benutzerquota-Eigenschaften werden nicht von <code>zfs get all</code> dargestellt. Nicht-<code>root</code>-Benutzer können nur ihre eigenen Quotas sehen, ausser ihnen wurde das <code>userquota</code>-Privileg zugeteilt. Benutzer mit diesem Privileg sind in der Lage, jedermanns Quota zu sehen und zu verändern.</p></div></td></tr></tbody></table></div><div class=paragraph><p>Das generelle Format zum Festlegen einer Gruppenquota lautet: <code>groupquota@<em>group</em>=<em>size</em></code>.</p></div><div class=paragraph><p>Um ein Quota für die Gruppe <em>firstgroup</em> von 50 GB zu setzen, geben Sie ein:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set groupquota@firstgroup=50G</span></code></pre></div></div><div class=paragraph><p>Um eine Quota für die Gruppe <em>firstgroup</em> zu setzen oder sicherzustellen, dass diese nicht gesetzt ist, verwenden Sie stattdessen:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set groupquota@firstgroup=none</span></code></pre></div></div><div class=paragraph><p>Genau wie mit der Gruppenquota-Eigenschaft, werden nicht-<code>root</code>-Benutzer nur die Quotas sehen, die den Gruppen zugeordnet ist, in denen Sie Mitglied sind. Allerdings ist <code>root</code> oder ein Benutzer mit dem <code>groupquota</code>-Privileg in der Lage, die Quotas aller Gruppen zu sehen und festzusetzen.</p></div><div class=paragraph><p>Um die Menge an Speicherplatz zusammen mit der Quota anzuzeigen, die von jedem Benutzer auf dem Dateisystem oder Schnappschuss verbraucht wird, verwenden Sie <code>zfs userspace</code>. Für Gruppeninformationen, nutzen Sie <code>zfs groupspace</code>. Für weitere Informationen zu unterstützten Optionen oder wie sich nur bestimmte Optionen anzeigen lassen, lesen Sie <a href="https://man.freebsd.org/cgi/man.cgi?query=zfs&amp;sektion=1&amp;format=html">zfs(1)</a>.</p></div><div class=paragraph><p>Benutzer mit ausreichenden Rechten sowie <code>root</code> können das Quota für <span class=filename>storage/home/bob</span> anzeigen lassen:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs get quota storage/home/bob</span></code></pre></div></div></div><div class=sect2><h3 id=zfs-zfs-reservation>19.4.9. Reservierungen<a class=anchor href=#zfs-zfs-reservation></a></h3><div class=paragraph><p><a href=#zfs-term-reservation>Reservierungen</a> garantieren ein Minimum an Speicherplatz, der immer auf dem Dataset verfügbar sein wird. Der reservierte Platz wird nicht für andere Datasets zur Verfügung stehen. Diese Eigenschaft kann besonders nützlich sein, um zu gewährleisten, dass freier Speicherplatz für ein wichtiges Dataset oder für Logdateien bereit steht.</p></div><div class=paragraph><p>Das generelle Format der <code>reservation</code>-Eigenschaft ist <code>reservation=<em>size</em></code>. Um also eine Reservierung von 10 GB auf <span class=filename>storage/home/bob</span> festzulegen, geben Sie Folgendes ein:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set reservation=10G storage/home/bob</span></code></pre></div></div><div class=paragraph><p>Um die Reservierung zu beseitigen:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set reservation=none storage/home/bob</span></code></pre></div></div><div class=paragraph><p>Das gleiche Prinzip kann auf die <code>refreservation</code>-Eigenschaft angewendet werden, um eine <a href=#zfs-term-refreservation>Referenzreservierung</a> mit dem generellen Format <code>refreservation=<em>size</em></code> festzulegen.</p></div><div class=paragraph><p>Dieser Befehl zeigt die Reservierungen oder Referenzreservierungen an, die auf <span class=filename>storage/home/bob</span> existieren:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs get reservation storage/home/bob</span>
<span class=c># zfs get refreservation storage/home/bob</span></code></pre></div></div></div><div class=sect2><h3 id=zfs-zfs-compression>19.4.10. Komprimierung<a class=anchor href=#zfs-zfs-compression></a></h3><div class=paragraph><p>ZFS bietet transparente Komprimierung. Datenkomprimierung auf Blockebene während diese gerade geschrieben werden, spart nicht nur Plattenplatz ein, sondern kann auch den Durchsatz der Platte steigern. Falls Daten zu 25% komprimiert sind, jedoch die komprimierten Daten im gleichen Tempo wie ihre unkomprimierte Version, resultiert das in einer effektiven Schreibgeschwindigkeit von 125%. Komprimierung kann auch eine Alternative zu <a href=#zfs-zfs-deduplication>Deduplizierung</a> darstellen, da es viel weniger zusätzlichen Hauptspeicher benötigt.</p></div><div class=paragraph><p>ZFS bietet mehrere verschiedene Kompressionsalgorithmen an, jede mit unterschiedlichen Kompromissen. Mit der Einführung von LZ4-Komprimierung in ZFS v5000, ist es möglich, Komprimierung für den gesamten Pool zu aktivieren, ohne die großen Geschwindigkeitseinbußen der anderen Algorithmen. Der größte Vorteil von LZ4 ist die Eigenschaft <em>früher Abbruch</em>. Wenn LZ4 nicht mindestens 12,5% Komprimierung im ersten Teil der Daten erreicht, wird der Block unkomprimiert geschrieben, um die Verschwendung von CPU-Zyklen zu vermeiden, weil die Daten entweder bereits komprimiert sind oder sich nicht komprimieren lassen. Für Details zu den verschiedenen verfügbaren Komprimierungsalgorithmen in ZFS, lesen Sie den Eintrag <a href=#zfs-term-compression>Komprimierung</a> im Abschnitt Terminologie</p></div><div class=paragraph><p>Der Administrator kann die Effektivität der Komprimierung über eine Reihe von Dataset-Eigenschaften überwachen.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs get used,compressratio,compression,logicalused mypool/compressed_dataset</span>
NAME        PROPERTY          VALUE     SOURCE
mypool/compressed_dataset  used              449G      -
mypool/compressed_dataset  compressratio     1.11x     -
mypool/compressed_dataset  compression       lz4       <span class=nb>local
</span>mypool/compressed_dataset  logicalused       496G      -</code></pre></div></div><div class=paragraph><p>Dieses Dataset verwendet gerade 449 GB Plattenplatz (used-Eigenschaft. Ohne Komprimierung würde es stattdessen 496 GB Plattenplatz belegen (<code>logicalused</code>). Das ergibt eine Kompressionsrate von 1,11:1.</p></div><div class=paragraph><p>Komprimierung kann einen unerwarteten Nebeneffekt haben, wenn diese mit <a href=#zfs-term-userquota>Benutzerquotas</a> kombiniert wird. Benutzerquotas beschränken, wieviel Speicherplatz ein Benutzer auf einem Dataset verbrauchen kann. Jedoch basieren die Berechnungen darauf, wieviel Speicherplatz <em>nach der Komprimierung</em> belegt ist. Wenn also ein Benutzer eine Quota von10 GB besitzt und 10 GB von komprimierbaren Daten schreibt, wird dieser immer noch in der Lage sein, zusätzliche Daten zu speichern. Wenn später eine Datei aktualisiert wird, beispielsweise eine Datenbank, mit mehr oder weniger komprimierbaren Daten, wird sich die Menge an verfügbarem Speicherplatz ändern. Das kann in einer merkwürdigen Situation resultieren, in welcher der Benutzer nicht die eigentliche Menge an Daten (die Eigenschaft <code>logicalused</code>) überschreitet, jedoch die Änderung in der Komprimierung dazu führt, dass das Quota-Limit erreicht ist.</p></div><div class=paragraph><p>Kompression kann ebenso unerwartet mit Sicherungen interagieren. Quotas werden oft verwendet, um einzuschränken, wieviele Daten gespeichert werden können um sicherzustellen, dass ausreichend Speicherplatz für die Sicherung vorhanden ist. Wenn jedoch Quotas Komprimierung nicht berücksichtigen, werden womöglich mehr Daten geschrieben als in der unkomprimierten Sicherung Platz ist.</p></div></div><div class=sect2><h3 id=zfs-zfs-deduplication>19.4.11. Deduplizierung<a class=anchor href=#zfs-zfs-deduplication></a></h3><div class=paragraph><p>Wenn aktiviert, verwendet <a href=#zfs-term-deduplication>Deduplizierung</a> die Prüfsumme jedes Blocks, um Duplikate dieses Blocks zu ermitteln. Sollte ein neuer Block ein Duplikat eines existierenden Blocks sein, dann schreibt ZFS eine zusätzliche Referenz auf die existierenden Daten anstatt des kompletten duplizierten Blocks. Gewaltige Speicherplatzeinsparungen sind möglich wenn die Daten viele Duplikate von Dateien oder wiederholte Informationen enthalten. Seien Sie gewarnt: Deduplizierung benötigt eine extrem große Menge an Hauptspeicher und die meistens Einsparungen können stattdessen durch das Aktivieren von Komprimierung erreicht werden.</p></div><div class=paragraph><p>Um Deduplizierung zu aktivieren, setzen Sie die <code>dedup</code>-Eigenschaft auf dem Zielpool:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zfs set dedup=on pool</span></code></pre></div></div><div class=paragraph><p>Nur neu auf den Pool geschriebene Daten werden dedupliziert. Daten, die bereits auf den Pool geschrieben wurden, werden nicht durch das Aktivieren dieser Option dedupliziert. Ein Pool mit einer gerade aktivierten Deduplizierung wird wie in diesem Beispiel aussehen:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool list</span>
NAME  SIZE ALLOC  FREE   CKPOINT  EXPANDSZ   FRAG   CAP DEDUP HEALTH ALTROOT
pool 2.84G 2.19M 2.83G         -         -     0%    0% 1.00x ONLINE -</code></pre></div></div><div class=paragraph><p>Die Spalte <code>DEDUP</code> zeigt das aktuelle Verhältnis der Deduplizierung für diesen Pool an. Ein Wert von <code>1.00x</code> zeigt an, dass die Daten noch nicht dedupliziert wurden. Im nächsten Beispiel wird die Ports-Sammlung dreimal in verschiedene Verzeichnisse auf dem deduplizierten Pool kopiert.</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># for d in dir1 dir2 dir3; do</span>
<span class=o>&gt;</span> <span class=nb>mkdir</span> <span class=nv>$d</span> <span class=o>&amp;&amp;</span> <span class=nb>cp</span> <span class=nt>-R</span> /usr/ports <span class=nv>$d</span> &amp;
<span class=o>&gt;</span> <span class=k>done</span></code></pre></div></div><div class=paragraph><p>Redundante Daten werden erkannt und dedupliziert:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zpool list</span>
NAME SIZE  ALLOC FREE CAP DEDUP HEALTH ALTROOT
pool 2.84G 20.9M 2.82G 0% 3.00x ONLINE -</code></pre></div></div><div class=paragraph><p>Die <code>DEDUP</code>-Spalte zeigt einen Faktor von <code>3.00x</code>. Mehrere Kopien der Ports-Sammlung wurden erkannt und dedupliziert, was nur ein Drittel des Speicherplatzes benötigt. Das Potential für Einsparungen beim Speicherplatz ist enorm, wird jedoch damit erkauft, dass genügend Speicher zur Verfügung stehen muss, um die deduplizierten Blöcke zu verwalten.</p></div><div class=paragraph><p>Deduplizierung ist nicht immer gewinnbringend, besonders wenn die Daten auf dem Pool nicht redundant sind. ZFS kann potentielle Speicherplatzeinsparungen durch Deduplizierung auf einem Pool simulieren:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># zdb -S pool</span>
Simulated DDT histogram:

bucket              allocated                       referenced
______   ______________________________   ______________________________
refcnt   blocks   LSIZE   PSIZE   DSIZE   blocks   LSIZE   PSIZE   DSIZE
<span class=nt>------</span>   <span class=nt>------</span>   <span class=nt>-----</span>   <span class=nt>-----</span>   <span class=nt>-----</span>   <span class=nt>------</span>   <span class=nt>-----</span>   <span class=nt>-----</span>   <span class=nt>-----</span>
     1    2.58M    289G    264G    264G    2.58M    289G    264G    264G
     2     206K   12.6G   10.4G   10.4G     430K   26.4G   21.6G   21.6G
     4    37.6K    692M    276M    276M     170K   3.04G   1.26G   1.26G
     8    2.18K   45.2M   19.4M   19.4M    20.0K    425M    176M    176M
    16      174   2.83M   1.20M   1.20M    3.33K   48.4M   20.4M   20.4M
    32       40   2.17M    222K    222K    1.70K   97.2M   9.91M   9.91M
    64        9     56K   10.5K   10.5K      865   4.96M    948K    948K
   128        2   9.50K      2K      2K      419   2.11M    438K    438K
   256        5   61.5K     12K     12K    1.90K   23.0M   4.47M   4.47M
    1K        2      1K      1K      1K    2.98K   1.49M   1.49M   1.49M
 Total    2.82M    303G    275G    275G    3.20M    319G    287G    287G

dedup <span class=o>=</span> 1.05, compress <span class=o>=</span> 1.11, copies <span class=o>=</span> 1.00, dedup <span class=k>*</span> compress / copies <span class=o>=</span> 1.16</code></pre></div></div><div class=paragraph><p>Nachdem <code>zdb -S</code> die Analyse des Pool abgeschlossen hat, zeigt es die Speicherplatzeinsparungen, die durch aktivierte Deduplizierung erreichbar sind, an. In diesem Fall ist <code>1.16</code> ein sehr schlechter Faktor, der größtenteils von Einsparungen durch Komprimierung beeinflusst wird. Aktivierung von Deduplizierung auf diesem Pool würde also keine signifikante Menge an Speicherplatz einsparen und ist daher nicht die Menge an Speicher wert, die nötig sind, um zu deduplizieren. Über die Formel <em>ratio = dedup * compress / copies</em> kann ein Systemadministrator die Speicherplatzbelegung planen und entscheiden, ob es sich lohnt, den zusätzlichen Hauptspeicher für die Deduplizierung anhand des späteren Workloads aufzuwenden. Wenn sich die Daten verhältnismäßig gut komprimieren lassen, sind die Speicherplatzeinsparungen sehr gut. Es wird empfohlen, in dieser Situation zuerst die Komprimierung zu aktivieren, da diese auch erhöhte Geschwindigkeit mit sich bringt. Aktivieren Sie Deduplizierung nur in solchen Fällen, bei denen die Einsparungen beträchtlich sind und genug Hauptspeicher zur Verfügung steht, um die <a href=#zfs-term-deduplication>DDT</a> aufzunehmen.</p></div></div><div class=sect2><h3 id=zfs-zfs-jail>19.4.12. ZFS und Jails<a class=anchor href=#zfs-zfs-jail></a></h3><div class=paragraph><p>Um ein ZFS-Dataset einem <a href=../jails/#jails>Jail</a> zuzuweisen, wird der Befehl <code>zfs jail</code> und die dazugehörige Eigenschaft <code>jailed</code> verwendet. Durch Angabe von <code>zfs jail <em>jailid</em></code> wird ein Dataset dem spezifizierten Jail zugewiesen und kann mit <code>zfs unjail</code> wieder abgehängt werden. Damit das Dataset innerhalb der Jail kontrolliert werden kann, muss die Eigenschaft <code>jailed</code> gesetzt sein. Sobald ein Dataset sich im Jail befindet, kann es nicht mehr länger auf dem Hostsystem eingehängt werden, da es Einhängepunkte aufweisen könnte, welche die Sicherheit des Systems gefährden.</p></div></div></div></div><div class=sect1><h2 id=zfs-zfs-allow>19.5. Delegierbare Administration<a class=anchor href=#zfs-zfs-allow></a></h2><div class=sectionbody><div class=paragraph><p>Ein umfassendes System zur Berechtigungsübertragung erlaubt unprivilegierten Benutzern, ZFS-Administrationsaufgaben durchzuführen. Beispielsweise, wenn jedes Heimatverzeichnis eines Benutzers ein Dataset ist, können Benutzer das Recht darin erhalten, Schnappschüsse zu erstellen und zu zerstören. Einem Benutzer für die Sicherung kann die Erlaubnis eingeräumt werden, die Replikationseigenschaft zu verwenden. Einem Skript zum Sammeln von Speicherplatzverbrauch kann die Berechtigung gegeben werden, nur auf die Verbrauchsdaten aller Benutzer zuzugreifen. Es ist sogar möglich, die Möglichkeit zum Delegieren zu delegieren. Die Berechtigung zur Delegation ist für jedes Unterkommando und die meisten Eigenschaften möglich.</p></div><div class=sect2><h3 id=zfs-zfs-allow-create>19.5.1. Delegieren, ein Dataset zu erstellen<a class=anchor href=#zfs-zfs-allow-create></a></h3><div class=paragraph><p><code>zfs allow <em>someuser</em> create <em>mydataset</em></code> gibt dem angegebenen Benutzer die Berechtigung, Kind-Datasets unter dem ausgewählten Elterndataset anzulegen. Es gibt einen Haken: ein neues Dataset anzulegen beinhaltet, dass es eingehängt wird. Dies bedeutet, dass FreeBSDs <code>vfs.usermount</code> <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a> auf <code>1</code> gesetzt wird, um nicht-root Benutzern zu erlauben, Dateisysteme einzubinden. Es gibt eine weitere Einschränkung um Missbrauch zu verhindern: nicht-<code>root</code> Benutzer müssen Besitzer des Einhängepunktes sein, an dem das Dateisystem eingebunden werden soll.</p></div></div><div class=sect2><h3 id=zfs-zfs-allow-allow>19.5.2. Delegationsberechtigung delegieren<a class=anchor href=#zfs-zfs-allow-allow></a></h3><div class=paragraph><p><code>zfs allow <em>someuser</em> allow <em>mydataset</em></code> gibt dem angegebenen Benutzer die Fähigkeit, jede Berechtigung, die er selbst auf dem Dataset oder dessen Kindern besitzt, an andere Benutzer weiterzugeben. Wenn ein Benutzer die <code>snapshot</code>- und die <code>allow</code>-Berechtigung besitzt, kann dieser dann die <code>snapshot</code>-Berechtigung an andere Benutzer delegieren.</p></div></div></div></div><div class=sect1><h2 id=zfs-advanced>19.6. Themen für Fortgeschrittene<a class=anchor href=#zfs-advanced></a></h2><div class=sectionbody><div class=sect2><h3 id=zfs-advanced-tuning>19.6.1. Anpassungen<a class=anchor href=#zfs-advanced-tuning></a></h3><div class=paragraph><p>Eine Reihe von Anpassungen können vorgenommen werden, um ZFS unter verschiedenen Belastungen während des Betriebs bestmöglich einzustellen.</p></div><div class=ulist><ul><li><p><code><em>vfs.zfs.arc_max</em></code> - Maximale Größe des <a href=#zfs-term-arc>ARC</a>. Die Voreinstellung ist der gesamte RAM weniger 1 GB oder 5/8 vom RAM, je nachdem, was mehr ist. Allerdings sollte ein niedriger Wert verwendet werden, wenn das System weitere Dienste oder Prozesse laufen lässt, welche Hauptspeicher benötigen. Dieser Wert kann zur Laufzeit mit <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a> eingestellt und in <span class=filename>/boot/loader.conf</span> permanent gespeichert werden.</p></li><li><p><code><em>vfs.zfs.arc_meta_limit</em></code> - Schränkt die Menge des <a href=#zfs-term-arc>ARC</a> ein, welche für die Speicherung von Metadaten verwendet wird. Die Voreinstellung ist ein Viertel von <code>vfs.zfs.arc_max</code>. Diesen Wert zu erhöhen steigert die Geschwindigkeit, wenn die Arbeitslast Operationen auf einer großen Menge an Dateien und Verzeichnissen oder häufigen Metadatenoperationen beinhaltet. Jedoch bedeutet dies auch weniger Dateidaten, die in den <a href=#zfs-term-arc>ARC</a> passen. Dieser Wert kann zur Laufzeit mit <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a> eingestellt und in <span class=filename>/boot/loader.conf</span> oder <span class=filename>/etc/sysctl.conf</span> dauerhaft gespeichert werden.</p></li><li><p><code><em>vfs.zfs.arc_min</em></code> - Minimale Größe des <a href=#zfs-term-arc>ARC</a>. Der Standard beträgt die Hälfte von <code>vfs.zfs.arc_meta_limit</code>. Passen Sie diesen Wert an, um zu verhindern, dass andere Anwendungen den gesamten <a href=#zfs-term-arc>ARC</a> verdrängen. Dieser Wert kann zur Laufzeit mit <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a> geändert und in <span class=filename>/boot/loader.conf</span> oder <span class=filename>/etc/sysctl.conf</span> dauerhaft gespeichert werden.</p></li><li><p><code><em>vfs.zfs.vdev.cache.size</em></code> - Eine vorallokierte Menge von Speicher, die als Cache für jedes Gerät im Pool reserviert wird. Die Gesamtgröße von verwendetem Speicher ist dieser Wert multipliziert mit der Anzahl an Geräten. Nur zur Bootzeit kann dieser Wert angepasst werden und wird in <span class=filename>/boot/loader.conf</span> eingestellt.</p></li><li><p><code><em>vfs.zfs.min_auto_ashift</em></code> - Minimaler <code>ashift</code>-Wert (Sektorgröße), welche zur Erstellungszeit des Pools automatisch verwendet wird. Der Wert ist ein Vielfaches zur Basis Zwei. Der Standardwert von <code>9</code> repräsentiert <code>2^9 = 512</code>, eine Sektorgröße von 512 Bytes. Um <em>write amplification</em> zu vermeiden und die bestmögliche Geschwindigkeit zu erhalten, setzen Sie diesen Wert auf die größte Sektorgröße, die bei einem Gerät im Pool vorhanden ist.</p><div class=paragraph><p>Viele Geräte besitzen 4 KB große Sektoren. Die Verwendung der Voreinstellung <code>9</code> bei <code>ashift</code> mit diesen Geräten resultiert in einer write amplification auf diesen Geräten. Daten, welche in einem einzelnen 4 KB Schreibvorgang Platz finden würden, müssen stattdessen in acht 512-byte Schreibvorgänge aufgeteilt werden. ZFS versucht, die allen Geräten zugrundeliegende Sektorgröße während der Poolerstellung zu lesen, jedoch melden viele Geräte mit 4 KB Sektoren, dass ihre Sektoren aus Kompatibilitätsgründen 512 Bytes betragen. Durch das Setzen von <code>vfs.zfs.min_auto_ashift</code> auf <code>12</code> (<code>2^12 = 4096</code>) bevor der Pool erstellt wird, zwingt ZFS dazu, für diese Geräte 4 KB Blöcke für bessere Geschwindigkeit zu nutzen.</p></div><div class=paragraph><p>Erzwingen von 4 KB Blöcken ist ebenfalls hilfreich auf Pools bei denen Plattenaufrüstungen geplant sind. Zukünftige Platten werden wahrscheinlich 4 KB große Sektoren und der Wert von <code>ashift</code> lässt sich nach dem Erstellen des Pools nicht mehr ändern.</p></div><div class=paragraph><p>In besonderen Fällen ist die kleinere Blockgröße von 512-Byte vorzuziehen. Weniger Daten werden bei kleinen, zufälligen Leseoperationen übertragen, was besonders bei 512-Byte großen Platten für Datenbanken oder Plattenplatz für virtuelle Maschinen der Fall ist. Dies kann bessere Geschwindigkeit bringen, ganz besonders wenn eine kleinere ZFS record size verwendet wird.</p></div></li><li><p><code><em>vfs.zfs.prefetch_disable</em></code> - Prefetch deaktivieren. Ein Wert von <code>0</code> bedeutet aktiviert und <code>1</code> heißt deaktiviert. Die Voreinstellung ist <code>0</code>, außer, das System besitzt weniger als 4 GB RAM. Prefetch funktioniert durch das Lesen von grösseren Blöcken in den <a href=#zfs-term-arc>ARC</a> als angefordert wurden, in der Hoffnung, dass diese Daten ebenfalls bald benötigt werden. Wenn die I/O-Last viele große Mengen von zufälligen Leseoperationen beinhaltet, ist das Deaktivieren von prefetch eine Geschwindigkeitssteigerung durch die Reduzierung von unnötigen Leseoperationen. Dieser Wert kann zu jeder Zeit über <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a> angepasst werden.</p></li><li><p><code><em>vfs.zfs.vdev.trim_on_init</em></code> - Steuert, ob neue Geräte, die dem Pool hinzugefügt werden, das <code>TRIM</code>-Kommando ausführen sollen. Das beinhaltet die beste Geschwindigkeit und Langlebigkeit für SSDs, benötigt jedoch zusätzliche Zeit. Wenn das Gerät bereits sicher gelöscht wurde, kann durch deaktivieren dieser Option das Hinzufügen neuer Geräte schneller geschehen. Über <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a> lässt sich dieser Wert jederzeit einstellen.</p></li><li><p><code><em>vfs.zfs.vdev.max_pending</em></code> - Begrenzt die Menge von ausstehenden I/O-Anfragen pro Gerät. Ein größerer Wert wird die Gerätewarteschlange für Befehle gefüllt lassen und möglicherweise besseren Durchsatz erzeugen. Ein niedrigerer Wert reduziert die Latenz. Jederzeit kann dieser Wert über <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a> angepasst werden.</p></li><li><p><code><em>vfs.zfs.top_maxinflight</em></code> - Maximale Anzahl von ausstehenden I/Os pro darüberliegendem <a href=#zfs-term-vdev>vdev</a>. Begrenzt die Tiefe Kommandowarteschlange, um hohe Latenzen zu vermeiden. Das Limit ist pro darüberliegendem vdev, was bedeutet, dass das Limit für jeden <a href=#zfs-term-vdev-mirror>mirror</a>, <a href=#zfs-term-vdev-raidz>RAID-Z</a>, oder anderes vdev unabhängig gilt. Mit <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a> kann dieser Wert jederzeit angepasst werden.</p></li><li><p><code><em>vfs.zfs.l2arc_write_max</em></code> - Begrenzt die Menge an Daten, die pro Sekunde in den <a href=#zfs-term-l2arc>L2ARC</a> geschrieben wird. Durch diese Einstellung lässt sich die Lebensdauer von SSDs erhöhen, indem die Menge an Daten beschränkt wird, die auf das Gerät geschrieben wird. Dieser Wert ist über <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a> zu einem beliebigen Zeitpunkt änderbar.</p></li><li><p><code><em>vfs.zfs.l2arc_write_boost</em></code> - Der Wert dieser Einstellung wird zu <a href=#zfs-advanced-tuning-l2arc_write_max><code>vfs.zfs.l2arc_write_max</code></a> addiert und erhöht die Schreibgeschwindigkeit auf die SSD bis der erste Block aus dem <a href=#zfs-term-l2arc>L2ARC</a> verdrängt wurde. Diese "Turbo Warmup Phase" wurde entwickelt, um den Geschwindigkeitsverlust eines leeren <a href=#zfs-term-l2arc>L2ARC</a> nach einem Neustart zu reduzieren. Jederzeit kann dieser Wert mit <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a> geändert werden.</p></li><li><p><code><em>vfs.zfs.scrub_delay</em></code> - Anzahl von Ticks an Verzögerung zwischen jedem I/O während eines <a href=#zfs-term-scrub><code>scrub</code></a>. Um zu gewährleisten, dass ein <code>scrub</code> nicht mit die normalen Vorgänge eines Pools beeinträchtigt. Wenn währenddessen andere I/Os durchgeführt werden, wird der <code>scrub</code> zwischen jedem Befehl verzögert. Dieser Wert regelt die Gesamtmenge von IOPS (I/Os Per Second), die von <code>scrub</code> generiert werden. Die Granularität der Einstellung ist bestimmt durch den Wert von <code>kern.hz</code>, welcher standardmäßig auf auf 1000 Ticks pro Sekunde eingestellt ist. Diese Einstellung kann geändert werden, was in einer unterschiedlich effektiven Limitierung der IOPS resultiert. Der Standardwert ist <code>4</code>, was ein Limit von 1000 ticks/sec / 4 = 250 IOPS ergibt. Ein Wert von <em>20</em> würde ein Limit von 1000 ticks/sec / 20 = 50 IOPS ergeben. Die <code>scrub</code>-Geschwindigkeit ist nur begrenzt, wenn es kürzlich Aktivität auf dem Pool gab, wie der Wert von <a href=#zfs-advanced-tuning-scan_idle><code>vfs.zfs.scan_idle</code></a> verrät. Zu einem beliebigen Zeitpunkt kann über <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a> eine Änderung an diesem Wert erfolgen.</p></li><li><p><code><em>vfs.zfs.resilver_delay</em></code> - Anzahl an Millisekunden Verzögerung, die zwischen jedem I/O während eines <a href=#zfs-term-resilver>resilver</a> eingefügt wird. Um zu versichern, dass ein resilver nicht die normalen Vorgänge auf dem Pool stört, wird dieser zwischen jedem Kommando verzögert, wenn andere I/Os auf dem Pool passieren. Dieser Wert steuert das Limit der Gesamt-IOPS (I/Os Pro Sekunde), die vom resilver erzeugt werden. Die Granularität der Einstellung wird durch den Wert von <code>kern.hz</code> bestimmt, welcher standardmäßig 1000 Ticks pro Sekunde beträgt. Diese Einstellung lässt sich ändern, was in einem unterschiedlich effizienten IOPS-Limit resultiert. Die Voreinstellung ist 2, was ein Limit von 1000 ticks/sec / 2 = 500 IOPS beträgt. Einen Pool wieder in den Zustand <a href=#zfs-term-online>Online</a> zu versetzen ist möglicherweise wichtiger wenn eine andere Platte den Pool in den <a href=#zfs-term-faulted>Fault</a>-Zustand versetzt, was Datenverlust zur Folge hat. Ein Wert von 0 wird der resilver-Operation die gleiche Priorität wie anderen Operationen geben, was den Heilungsprozess beschleunigt. Die Geschwindigkeit des resilver wird nur begrenzt, wenn es kürzlich andere Aktivitäten auf dem Pool gab, wie von <a href=#zfs-advanced-tuning-scan_idle><code>vfs.zfs.scan_idle</code></a> festgestellt wird. Dieser Wert kann zu jeder Zeit über. <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a> eingestellt werden.</p></li><li><p><code><em>vfs.zfs.scan_idle</em></code> - Anzahl an Millisekunden seit der letzten Operation bevor der Pool als im Leerlauf befindlich deklariert wird. Wenn sich der Pool im Leerlauf befindet, wird die Begrenzung für <a href=#zfs-term-scrub><code>scrub</code></a> und <a href=#zfs-term-resilver>resilver</a> deaktiviert. Dieser Wert kann mittels <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a> jederzeit angepasst werden.</p></li><li><p><code><em>vfs.zfs.txg.timeout</em></code> - Maximale Anzahl von Sekunden zwischen <a href=#zfs-term-txg>Transaktionsgruppen</a> (transaction group). Die momentane Transaktionsgruppe wird auf den Pool geschrieben und eine frische Transaktionsgruppe begonnen, wenn diese Menge an Zeit seit der vorherigen Transaktionsgruppe abgelaufen ist. Eine Transaktionsgruppe kann verfrüht ausgelöst werden, wenn genug Daten geschrieben werden. Der Standardwert beträgt 5 Sekunden. Ein größerer Wert kann die Lesegeschwindigkeit durch verzögern von asynchronen Schreibvorgängen verbessern, allerdings kann dies ungleiche Geschwindigkeiten hervorrufen, wenn eine Transaktionsgruppe geschrieben wird. Dieser Wert kann zu einem beliebigen Zeitpunkt mit <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a> geändert werden.</p></li></ul></div></div><div class=sect2><h3 id=zfs-advanced-i386>19.6.2. ZFS auf i386<a class=anchor href=#zfs-advanced-i386></a></h3><div class=paragraph><p>Manche der Eigenschaften, die von ZFS bereitgestellt werden, sind speicherintensiv und benötigen Anpassungen für die maximale Effizienz auf Systemen mit begrenztem RAM.</p></div><div class=sect3><h4 id=_hauptspeicher>19.6.2.1. Hauptspeicher<a class=anchor href=#_hauptspeicher></a></h4><div class=paragraph><p>Als absolutes Minimum sollte der gesamte verfügbare Hauptspeicher mindestens ein Gigabyte betragen. Die vorgeschlagene Menge an RAM ist bedingt durch die Poolgröße und welche Eigenschaften von ZFS verwendet werden. Eine Faustregel besagt, dass 1 GB RAM für jedes 1 TB Storage vorgesehen werden sollte. Wenn Deduplizierung zum Einsatz kommt, besagt die Regel, dass 5 GB RAM pro TB an Speicher, der dedupliziert werden soll, bereitgestellt sein muss. Obwohl manche Anwender ZFS mit weniger RAM einsetzen, stürzen Systeme häufiger wegen unzureichendem Hauptspeicher ab. Weitere Anpassungen sind unter Umständen nötig für Systeme mit weniger als die vorgeschlagene Menge an RAM.</p></div></div><div class=sect3><h4 id=_kernel_konfiguration>19.6.2.2. Kernel-Konfiguration<a class=anchor href=#_kernel_konfiguration></a></h4><div class=paragraph><p>Wegen des begrenzten Addressraumes der i386™-Plattform müssen ZFS-Anwendern auf der i386™-Architektur diese Option der Kernelkonfigurationsdatei hinzufügen, den Kernel erneut bauen und das System neu starten:</p></div><div class="literalblock programlisting"><div class=content><pre>options        KVA_PAGES=512</pre></div></div><div class=paragraph><p>Dies erweitert den Addressraum des Kernels, was es erlaubt, die Einstellung <code>vm.kvm_size</code> hinter die momentan vorgegebene Grenze von 1 GB oder das Limit von 2 GB für PAE zu bringen. Um den passenden Wert für diese Option zu finden, teilen Sie den gewünschten Addressraum in Megabyte durch vier. In diesem Beispiel beträgt sie <code>512</code> für 2 GB.</p></div></div><div class=sect3><h4 id=_loader_anpassungen>19.6.2.3. Loader-Anpassungen<a class=anchor href=#_loader_anpassungen></a></h4><div class=paragraph><p>Der <span class=filename>kmem</span>-Addressraum kann auf allen FreeBSD-Architekturen erhöht werden. Auf einem Testsystem mit 1 GB physischen Speichers wurden mit diesen Optionen in <span class=filename>/boot/loader.conf</span> und einem anschließenden Systemneustart Erfolge erzielt:</p></div><div class="literalblock programlisting"><div class=content><pre>vm.kmem_size=&#34;330M&#34;
vm.kmem_size_max=&#34;330M&#34;
vfs.zfs.arc_max=&#34;40M&#34;
vfs.zfs.vdev.cache.size=&#34;5M&#34;</pre></div></div><div class=paragraph><p>Für eine detailliertere Liste an Empfehlungen für ZFS-bezogene Einstellungen, lesen Sie <a href=https://wiki.freebsd.org/ZFSTuningGuide class=bare>https://wiki.freebsd.org/ZFSTuningGuide</a>.</p></div></div></div></div></div><div class=sect1><h2 id=zfs-links>19.7. Zusätzliche Informationen<a class=anchor href=#zfs-links></a></h2><div class=sectionbody><div class=ulist><ul><li><p><a href=http://open-zfs.org>OpenZFS</a></p></li><li><p><a href=https://wiki.freebsd.org/ZFSTuningGuide>FreeBSD Wiki - ZFS Tuning</a></p></li><li><p><a href=http://docs.oracle.com/cd/E19253-01/819-5461/index.html>Oracle Solaris ZFS Administration Guide</a></p></li><li><p><a href=https://calomel.org/zfs_raid_speed_capacity.html>Calomel Blog - ZFS Raidz Performance, Capacity und Integrity</a></p></li></ul></div></div></div><div class=sect1><h2 id=zfs-term>19.8. ZFS-Eigenschaften und Terminologie<a class=anchor href=#zfs-term></a></h2><div class=sectionbody><div class=paragraph><p>ZFS ist ein fundamental anderes Dateisystem aufgrund der Tatsache, dass es mehr als ein Dateisystem ist. ZFS kombiniert die Rolle eines Dateisystems mit dem Volumemanager, was es ermöglicht, zusätzliche Speichermedien zu einem laufenden System hinzuzufügen und diesen neuen Speicher sofort auf allen auf dem Pool existierenden Dateisystemen zur Verfügung zu haben. Durch die Kombination von traditionell getrennten Rollen ist ZFS in der Lage, Einschränkungen, die zuvor RAID-Gruppen daran gehindert hatten, zu wachsen. Jedes Gerät auf höchster Ebene in einem Pool wird ein <em>vdev</em> genannt, was eine einfache Platte oder eine RAID-Transformation wie ein Spiegel oder RAID-Z-Verbund sein kann. ZFS-Dateisysteme (<em>datasets</em> genannt), haben jeweils Zugriff auf den gesamten freien Speicherplatz des gesamten Pools. Wenn Blöcke aus diesem Pool allokiert werden, verringert sich auch der freie Speicherplatz für jedes Dateisystem. Dieser Ansatz verhindert die allgegenwärtige Falle von umfangreichen Partitionen, bei denen freier Speicherplatz über alle Partitionen hinweg fragmentiert wird.</p></div><table class="tableblock frame-all grid-all stretch informaltable"><col style=width:10%><col style=width:90%><tbody><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-pool></a>zpool</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Ein Speicher-<em>Pool</em> ist der grundlegendste Baustein von ZFS. Ein Pool besteht aus einem oder mehreren vdevs, was die zugrundeliegenden Geräte repräsentiert, welche die Daten speichern. Ein Pool wird dann verwendet, um ein oder mehrere Dateisysteme (Datasets) oder Blockgeräte (Volumes) zu erstellen. Diese Datasets und Volumes teilen sich den im Pool verfügbaren Speicherplatz. Jeder Pool wird eindeutig durch einen Namen und eine GUID identifiziert. Die verfügbaren Eigenschaften werden durch die ZFS-Versionsnummer des Pools bestimmt.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-vdev></a>vdev Arten</p></td><td class="tableblock halign-left valign-top"><div class=content><div class=paragraph><p>Ein Pool besteht aus einem oder mehreren vdevs, die selbst eine einfache Platte oder im Fall von RAID eine Gruppe von Platten darstellt. Wenn mehrere vdevs eingesetzt werden, verteilt ZFS die Daten über die vdevs, um die Geschwindigkeit zu steigern und den verfügbaren Platz zu maximieren.</p></div><div class=ulist><ul><li><p><a id=zfs-term-vdev-disk></a><em>Festplatte</em> - Der einfachste Typ von vdev ist ein Standard-Blockgerät. Dies kann die komplette Platte (wie <span class=filename>/dev/ada0</span> oder <span class=filename>/dev/da0</span>) oder auch eine Partition (<span class=filename>/dev/ada0p3</span>) sein. Auf FreeBSD gibt es keine Geschwindigkeitseinbußen bei der Verwendung einer Partition anstatt einer kompletten Platte. Dies unterscheidet sich von den Empfehlungen, welche in der Solaris Dokumentation gegeben werden.</p><div class="admonitionblock caution"><table><tbody><tr><td class=icon><i class="fa icon-caution" title=Caution></i></td><td class=content><div class=paragraph><p>Es wird dringend davon abgeraten, eine ganze Platte für einen bootbaren Pool zu benutzen, da dies dazu führen kann, dass der Pool nicht mehr bootet. Ebenso sollten Sie nicht eine ganze Platte als Teil eines Spiegels oder RAID-Z vdev verwenden, weil es dann nicht mehr möglich ist, die Größe einer nicht partitionierten Platte beim Booten zuverlässig zu bestimmen. Zudem gibt es dann keinen Platz mehr, um Boot-Code einzufügen.</p></div></td></tr></tbody></table></div></li><li><p><a id=zfs-term-vdev-file></a><em>File</em> - Zusätzlich zu Festplatten können ZFS-Pools aus regulären Dateien aufgebaut sein, was besonders hilfreich ist, um zu testen und zu experimentieren. Verwenden Sie den kompletten Pfad zu der Datei als Gerätepfad im Befehl <code>zpool create</code>. Alle vdevs müssen mindestens 128 MB groß sein.</p></li><li><p><a id=zfs-term-vdev-mirror></a><em>Mirror</em> - Wenn ein Spiegel erstellt wird, verwenden Sie das Schlüsselwort <code>mirror</code>, gefolgt von der Liste an Mitgliedsgeräten für den Spiegel. Ein Spiegel besteht aus zwei oder mehr Geräten und sämtliche Daten werden auf alle Geräte, die Mitglied des Spiegels sind, geschrieben. Ein Spiegel-vdev wird nur soviele Daten speichern, wie das kleinste Gerät im Verbund aufnehmen kann. Ein Spiegel-vdev kann den Verlust von allen Mitgliedsgeräten bis auf eines verkraften, ohne irgendwelche Daten zu verlieren.</p><div class="admonitionblock note"><table><tbody><tr><td class=icon><i class="fa icon-note" title=Note></i></td><td class=content><div class=paragraph><p>Ein reguläre einzelne vdev-Platte kann jederzeit zu einem Spiegel-vdev über das Kommando <code>zpool <a href=#zfs-zpool-attach>attach</a></code> aktualisiert werden.</p></div></td></tr></tbody></table></div></li><li><p><a id=zfs-term-vdev-raidz></a><em>RAID-Z</em> - ZFS implementiert RAID-Z, eine Varianten des RAID-5-Standards, der bessere Verteilung der Parität bietet und das "RAID-5 write hole" eliminiert, bei dem die Daten und Parität nach einem unerwarteten Neustart inkonsistent werden können. ZFS unterstützt drei Stufen von RAID-Z, die unterschiedliche Arten von Redundanz im Austausch gegen niedrigere Stufen von verwendbarem Speicher. Diese Typen werden RAID-Z1 bis RAID-Z3 genannt, basierend auf der Anzahl der Paritätsgeräte im Verbund und der Anzahl an Platten, die ausfallen können, während der Pool immer noch normal funktioniert.</p><div class=paragraph><p>In einer RAID-Z1-Konfiguration mit vier Platten, bei der jede 1 TB besitzt, beträgt der verwendbare Plattenplatz 3 TB und der Pool wird immer noch im Modus degraded weiterlaufen, wenn eine Platte davon ausfällt. Wenn eine zusätzliche Platte ausfällt, bevor die defekte Platte ersetzt wird, können alle Daten im Pool verloren gehen.</p></div><div class=paragraph><p>Eine Konfiguration von acht Platten zu je 1 TB als RAID-Z3 wird 5 TB verwendbaren Speicher bieten und in der Lage sein, weiterhin zu funktionieren, wenn drei Platten ausgefallen sind. Sun™ empfiehlt nicht mehr als neun Platten in einem einzelnen vdev. Wenn die Konfiguration mehr Platten aufweist, wird empfohlen, diese in getrennten vdevs aufzuteilen, so dass die Daten des Pools zwischen diesen aufgeteilt werden.</p></div><div class=paragraph><p>Eine Konfiguration von zwei RAID-Z2-vdevs, bestehend aus jeweils 8 Platten würde etwa einem RAID-60-Verbund entsprechen. Der Speicherplatz einer RAID-Z-Gruppe ist ungefähr die Größe der kleinsten Platte multipliziert mit der Anzahl von nicht-Paritätsplatten. Vier 1 TB Platten in einem RAID-Z1 besitzt eine effektive Größe von ungefähr 3 TB und ein Verbund von acht 1 TB-Platten als RAID-Z3 enthält 5 TB verfügbarer Plattenplatz.</p></div></li><li><p><a id=zfs-term-vdev-spare></a><em>Spare</em> - ZFS besitzt einen speziellen Pseudo-vdev Typ, um einen Überblick über die verfügbaren hot spares zu behalten. Beachten Sie, dass hot spares nicht automatisch eingesetzt werden. Diese müssen manuell konfiguriert werden, um ein ausgefallenes Gerät über <code>zfs replace</code> zu ersetzen.</p></li><li><p><a id=zfs-term-vdev-log></a><em>Log</em> - ZFS Log-Geräte, auch bezeichnet als ein ZFS Intent Log (<a href=#zfs-term-zil>ZIL</a>) verschieben das Intent Log von den regulären Geräten im Pool auf ein dediziertes Gerät, typischerweise eine SSD. Ein dediziertes Log-Gerät zu besitzen kann die Geschwindigkeit von Anwendungen mit einer großen Anzahl von synchronen Schreibvorgängen, besonders Datenbanken, signifikant steigern. Log-Geräte können gespiegelt werden, jedoch wird RAID-Z nicht unterstützt. Werden mehrere Log-Geräte verwendet, so werden Schreibvorgänge gleichmäßig unter diesen aufgeteilt.</p></li><li><p><a id=zfs-term-vdev-cache></a><em>Cache</em> - Ein Cache-vdev einem Pool hinzuzufügen, erhöht den Speicher des <a href=#zfs-term-l2arc>L2ARC</a> Caches. Cache-Geräte lassen sich nicht spiegeln. Da ein Cache-Gerät nur zusätzliche Kopien von existierenden Daten speichert, gibt es kein Risiko, Daten zu verlieren.</p></li></ul></div></div></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-txg></a>Transaktionsgruppe (Transaction Group, TXG)</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Transaktionsgruppen sind die Art und Weise, wie geänderte Blöcke zusammen gruppiert und letztendlich auf den Pool geschrieben werden. Transaktionsgruppen sind die atomare Einheit, welche ZFS verwendet, um Konsistenz zu gewährleisten. Jeder Transaktionsgruppe wird eine einzigartige, fortlaufende 64-Bit Identifikationsnummer zugewiesen. Es kann bis zu drei aktive Transaktionsgruppen gleichzeitig geben, wobei sich jede davon in einem der folgenden drei Zustände befinden kann:</p><p class=tableblock>* <em>Open (Offen)</em> - Wenn eine neue Transaktionsgruppe erstellt wird, befindet diese sich im Zustand offen und akzeptiert neue Schreibvorgänge. Es ist immer eine Transaktionsgruppe in diesem Zustand, jedoch kann die Transaktionsgruppe neue Schreibvorgänge ablehnen, wenn diese ein Limit erreicht hat. Sobald eine offene Transaktionsgruppe an das Limit stößt oder das <a href=#zfs-advanced-tuning-txg-timeout><code>vfs.zfs.txg.timeout</code></a> wurde erreicht, geht die Transaktionsgruppe in den nächsten Zustand über.
* <em>Quiescing (Stilllegen)</em> - Ein kurzer Zustand, der es noch ausstehenden Operationen erlaubt, zum Abschluss zu kommen, währenddessen das Erstellen einer neuen Transaktionsgruppe jedoch nicht blockiert wird. Sobald alle Transaktionen in der Gruppe abgeschlossen sind, geht die Transaktionsgruppen in den letzten Zustand über.
* <em>Syncing (Sychronisieren)</em> - Alle Daten in der Transaktionsgruppe werden auf das Speichermedium geschrieben. Dieser Prozess wird wiederum andere Daten wie Metadaten und space maps verändern, die ebenfalls auf das Speichermedium geschrieben werden müssen. Der Prozess des Synchronisierens beinhaltet mehrere Durchläufe. Der erste Prozess, welches der größte, gefolgt von den Metadaten, ist, beinhaltet alle geänderten Datenblöcke und kann mehrere Durchläufe benötigen, um zum Ende zu gelangen. Da das Allokieren von Speicher für die Datenblöcke neue Metadaten generiert, kann der Synchronisationsprozess nicht beendet werden, bis ein Durchlauf fertig ist, der keinen zusätzlichen Speicher allokiert. Der Synchronisierungszustand ist der Zustand, in dem auch <em>synctasks</em> abgeschlossen werden. Synctasks sind administrative Operationen, wie das Erstellen oder zerstören von Schnappschüssen und Datasets, welche den Überblock verändern, wenn sie abgeschlossen sind. Sobald der Synchronisationszustand abgeschlossen ist, geht die Transaktionsgruppe aus dem Stilllegungszustand über in den Synchronisationszustand.
Alle administrativen Funktionen, wie <a href=#zfs-term-snapshot><code>Schnappschüsse</code></a> werden als Teil einer Transaktionsgruppe geschrieben. Wenn ein synctask erstellt ist, wird dieser der momentan geöffneten Transaktionsgruppe hinzugefügt und diese Gruppe wird so schnell wie möglich in den Synchronisationszustand versetzt, um die Latenz von administrativen Befehlen zu reduzieren.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-arc></a>Adaptive Replacement Cache (ARC)</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>ZFS verwendet einen Adaptive Replacement Cache (ARC), anstatt eines traditionellen Least Recently Used (LRU) Caches. Ein LRU-Cache ist eine einfache Liste von Elementen im Cache, sortiert nach der letzten Verwendung jedes Elements in der Liste. Neue Elemente werden an den Anfang der Liste eingefügt. Wenn der Cache voll ist, werden Elemente vom Ende der Liste verdrängt, um Platz für aktivere Objekte zu schaffen. Ein ARC besteht aus vier Listen: derjenigen der Most Recently Used (MRU) und Most Frequently Used (MFU) Objekte, plus einer sogenannten ghost list für jede von beiden. Diese Ghost Lists verfolgen die kürzlich verdrängten Objekte, um zu verhindern, dass diese erneut in den Cache aufgenommen werden. Dies erhöht die Trefferrate (hit ratio) des Caches, indem verhindert wird, dass Elemente, die in der Vergangenheit nur ab und zu benutzt wurden, wieder im Cache landen. Ein weiterer Vorteil der Verwendung sowohl einer MRU und einer MFU ist, dass das Scannen eines gesamten Dateisystems normalerweise alle Daten aus einem MRU- oder LRU-Cache verdrängt, um dem gerade frisch zugegriffenem Inhalt den Vorzug zu geben. Mit ZFS gibt es also eine MFU, die nur die am häufigsten verwendeten Elemente beinhaltet und der Cache von am meisten zugegriffenen Blöcken bleibt erhalten.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-l2arc></a>L2ARC</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>L2ARC ist die zweite Stufe des Caching-Systems von ZFS. Der Haupt-ARC wird im RAM abgelegt. Da die Menge an verfügbarem RAM meist begrenzt ist, kann ZFS auch <a href=#zfs-term-vdev-cache>cache vdevs</a> verwenden. Solid State Disks (SSDs) werden oft als diese Cache-Geräte eingesetzt, aufgrund ihrer höheren Geschwindigkeit und niedrigeren Latenz im Vergleich zu traditionellen drehenden Speichermedien wie Festplatten. Der Einsatz des L2ARC ist optional, jedoch wird durch die Verwendung eine signifikante Geschwindigkeitssteigerung bei Lesevorgängen bei Dateien erzielt, welche auf der SSD zwischengespeichert sind, anstatt von der regulären Platte gelesen werden zu müssen. L2ARC kann ebenfalls die <a href=#zfs-term-deduplication>Deduplizierung</a> beschleunigen, da eine DDT, welche nicht in den RAM passt, jedoch in den L2ARC wesentlich schneller sein wird als eine DDT, die von der Platte gelesen werden muss. Die Häufigkeit, in der Daten zum Cache-Gerät hinzugefügt werden, ist begrenzt, um zu verhindern, dass eine SSD frühzeitig durch zu viele Schreibvorgänge aufgebraucht ist. Bis der Cache voll ist (also der erste Block verdrängt wurde, um Platz zu schaffen), wird das Schreiben auf den L2ARC begrenzt auf die Summe der Schreibbegrenzung und das Bootlimit, sowie hinterher auf das Schreiblimit. Ein paar <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a>-Werte steuert diese Limits. <a href=#zfs-advanced-tuning-l2arc_write_max><code>vfs.zfs.l2arc_write_max</code></a> steuert, wie viele Bytes in den Cache pro Sekunde geschrieben werden, während <a href=#zfs-advanced-tuning-l2arc_write_boost><code>vfs.zfs.l2arc_write_boost</code></a> zu diesem Limit während der "Turbo Warmup Phase" hinzuaddiert wird (Write Boost).</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-zil></a>ZIL</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>ZIL beschleunigt synchrone Transaktionen durch die Verwendung von Speichermedien wie SSDs, welche schneller sind als diejenigen, welche Teil des Speicherpools sind. Wenn eine Anwendung einen synchronen Schreibvorgang anfordert (eine Garantie, dass die Daten sicher auf den Platten gespeichert wurden anstatt nur zwischengespeichert zu sein, um später geschrieben zu werden), werden die Daten auf den schnelleren ZIL-Speicher geschrieben und dann später auf die regulären Festplatten. Dies reduziert die Latenz sehr und verbessert die Geschwindigkeit. Nur synchrone Vorgänge wie die von Datenbanken werden durch den Einsatz eines ZIL profitieren. Reguläre, asynchrone Schreibvorgänge wie das Kopieren von Dateien wird den ZIL überhaupt nicht verwenden.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-cow></a>Copy-On-Write</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Im Gegensatz zu traditionellen Dateisystemen werden beim Überschreiben von Daten bei ZFS die neuen Daten an einen anderen Block geschrieben, anstatt die alten Daten an der gleichen Stelle zu überschreiben. Nur wenn dieser Schreibvorgang beendet wurde, werden die Metadaten aktualisiert, um auf die neue Position zu verweisen. Im Falle eines kurzen Schreibvorgangs (ein Systemabsturz oder Spannungsverlust während eine Datei geschrieben wird) sind die gesamten Inhalte der Originaldatei noch vorhanden und der unvollständige Schreibvorgang wird verworfen. Das bedeutet auch, dass ZFS nach einem unvorhergesehenen Ausfall keinen <a href="https://man.freebsd.org/cgi/man.cgi?query=fsck&amp;sektion=8&amp;format=html">fsck(8)</a> benötigt.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-dataset></a>Dataset</p></td><td class="tableblock halign-left valign-top"><p class=tableblock><em>Dataset</em> ist der generische Begriff für ein ZFS-Dateisystem, Volume, Schnappschüsse oder Klone. Jedes Dataset besitzt einen eindeutigen Namen in der Form <em>poolname/path@snapshot</em> Die Wurzel des Pools ist technisch gesehen auch ein Dataset. Kind-Datasets werden hierarchisch wie Verzeichnisse benannt. Beispielsweise ist <em>mypool/home</em> das Heimatdataset, ein Kind von <em>mypool</em> und erbt die Eigenschaften von diesem. Dies kann sogar noch erweitert werden durch das Erstellen von <em>mypool/home/user</em>. Dieses Enkelkind-Dataset wird alle Eigenschaften von den Eltern und Großeltern erben. Eigenschaften auf einem Kind können die geerbten Standardwerte der Eltern und Großeltern ändern und überschreiben. Die Verwaltung von Datasets und dessen Kindern lässt sich <a href=#zfs-zfs-allow>delegieren</a>.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-filesystem></a>Dateisystem</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Ein ZFS-Dataset wird meistens als ein Dateisystem verwendet. Wie jedes andere Dateisystem kann auch ein ZFS-Dateisystem irgendwo in der Verzeichnishierarchie eingehängt werden und enthält seine eigenen Dateien und Verzeichnisse mit Berechtigungen, Flags und anderen Metadaten.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-volume></a>Volume</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Zusätzlich zu regulären Dateisystem-Datasets, kann ZFS auch Volumes erstellen, die Blockgeräte sind. Volumes besitzen viele der gleichen Eigenschaften, inklusive copy-on-write, Schnappschüsse, Klone und Prüfsummen. Volumes sind nützlich, um andere Dateisystemformate auf ZFS aufzusetzen, so wie UFS Virtualisierung, oder das Exportieren von iSCSI-Abschnitten.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-snapshot></a>Snapshot (Schnappschuss)</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Das <a href=#zfs-term-cow>copy-on-write</a> (COW)-Entwicklung von ZFS erlaubt das Erstellen von beinahe sofortigen, konsistenten Schnappschüssen mit beliebigen Namen. Nachdem ein Schnappschuss von einem Dataset angelegt oder ein rekursiver Schnappschuss eines Elterndatasets, welcher alle Kinddatasets enthält, erstellt wurde, werden neue Daten auf neue Blöcke geschrieben, jedoch die alten Blöcke nicht wieder als freier Speicher zurückgewonnen. Der Schnappschuss enthält die Originalversion des Dateisystems und das aktive Dateisystem besitzt alle Änderungen, die seit dem Schnappschuss erstellt wurden. Kein zusätzlicher Platz wird benötigt. Werden neue Daten auf das aktive Dateisystem geschrieben, werden neue Blöcke allokiert, um diese Daten zu speichern. Die scheinbare Größe des Schnappschusses wird wachsen, da die Blöcke nicht mehr länger im aktiven Dateisystem, sondern nur noch im Schnappschuss Verwendung finden. Diese Schnappschüsse können nur lesend eingehängt werden, um vorherige Versionen von Dateien wiederherzustellen. Ein <a href=#zfs-zfs-snapshot>rollback</a> eines aktiven Dateisystems auf einen bestimmten Schnappschuss ist ebenfalls möglich, was alle Änderungen, die seit dem Anlegen des Schnappschusses vorgenommen wurden, wieder Rückgängig macht. Jeder Block im Pool besitzt einen Referenzzähler, der verfolgt, wieviele Schnappschüsse, Klone, Datasets oder Volumes diesen Block nutzen. Wenn Dateien und Schnappschüsse gelöscht werden, verringert dies auch den Referenzzähler. Wenn ein Block nicht mehr länger referenziert wird, kann er als freier Speicher wieder genutzt werden. Schnappschüsse können auch mit <a href=#zfs-zfs-snapshot>hold</a> markiert werden. Wenn versucht wird, einen solchen Schnappschuss zu zerstören, wird stattdessen ein <code>EBUSY</code>-Fehler ausgegeben. Jeder Schnappschuss kann mehrere holds besitzen, jeder mit einem eindeutigen Namen. Das Kommando <a href=#zfs-zfs-snapshot>release</a> entfernt diese, damit der Schnappschuss gelöscht werden kann. Schnappschüsse lassen sich auf Volumes ebenfalls anlegen, allerdings können diese nur geklont oder zurückgerollt werden, nicht jedoch unabhängig eingehängt.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-clone></a>Clone (Klone)</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Schnappschüsse können auch geklont werden. Ein Klon stellt eine veränderbare Version eines Schnappschusses dar, was es ermöglicht, das Dateisystem als neues Dataset aufzuspalten. Genau wie bei einem Schnappschuss verbraucht ein Klon keinen zusätzlichen Platz. Wenn neue Daten auf einen Klon geschrieben und neue Blöcke allokiert werden, wächst auch die Größe des Klons. Wenn Blöcke im geklonten Dateisystem oder Volume überschrieben werden, verringert sich auch der Referenzzähler im vorherigen Block. Der Schnappschuss, auf dem der Klon basiert kann nicht gelöscht werden, weil der Klon darauf eine Abhängigkeit besitzt. Der Schnappschuss stellt den Elternteil dar und der Klon das Kind. Klone lassen sich <em>promoted</em> (befördern), was die Abhängigkeit auflöst und den Klon zum Elternteil macht und den vorherigen Elternteil das Kind. Diese Operation benötigt keinen zusätzlichen Plattenplatz. Da die Menge an verwendetem Speicher vom Elternteil und dem Kind vertauscht wird, betrifft dies eventuell vorhandene Quotas und Reservierungen.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-checksum></a>Checksum (Prüfsumme)</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Jeder Block, der allokiert wird erhält auch eine Prüfsumme. Der verwendete Prüfsummenalgorithmus ist eine Eigenschaft jedes Datasets, siehe dazu <a href=#zfs-zfs-set><code>set</code></a>. Die Prüfsumme jedes Blocks wird transparent validiert wenn er gelesen wird, was es ZFS ermöglicht, stille Verfälschung zu entdecken. Wenn die gelesenen Daten nicht mit der erwarteten Prüfsumme übereinstimmen, wird ZFS versuchen, die Daten aus jeglicher verfügbarer Redundanz (wie Spiegel oder RAID-Z) zu rekonstruieren. Eine Überprüfung aller Prüfsummen kann durch das Kommando <a href=#zfs-term-scrub><code>scrub</code></a> ausgelöst werden. Prüfsummenalgorithmen sind:</p><p class=tableblock>* <code>fletcher2</code>
* <code>fletcher4</code>
* <code>sha256</code>
Die <code>fletcher</code>-Algorithmen sind schneller, aber dafür ist <code>sha256</code> ein starker kryptographischer Hash und besitzt eine viel niedrigere Chance auf Kollisionen zu stoßen mit dem Nachteil geringerer Geschwindigkeit. Prüfsummen können deaktiviert werden, dies wird aber nicht empfohlen.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-compression></a>Compression</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Jedes Dataset besitzt eine compression-Eigenschaft, die standardmäßig ausgeschaltet ist. Diese Eigenschaft kann auf eine Reihe von Kompressionsalgorithmen eingestellt werden. Dadurch werden alle neuen Daten, die auf das Dataset geschrieben werden, komprimiert. Neben einer Reduzierung von verbrauchtem Speicher wird oft der Lese- und Schreibdurchsatz erhöht, weil weniger Blöcke gelesen oder geschrieben werden müssen.</p><p class=tableblock><a id=zfs-term-compression-lz4></a>* <em>LZ4</em> - Wurde in der ZFS Poolversion 5000 (feature flags) hinzugefügt und LZ4 ist jetzt der empfohlene Kompressionsalgorithmus. LZ4 komprimiert ungefähr 50% schneller als LZJB, wenn er auf komprimierbaren Daten angewendet wird und ist über dreimal schneller, wenn unkomprimierbare Daten vorliegen. LZ4 entkomprimiert auch ungefähr 80% schneller als LZJB. Auf modernen CPUs, kann LZ4 oft über 500 MB/s komprimieren und entkomprimiert (pro einzelnem CPU-Kern) bei über 1.5 GB/s.
<a id=zfs-term-compression-lzjb></a>* <em>LZJB</em> - Der Standardkompressionsalgorithmus wurde von Jeff Bonwick, einem der ursprünglichen Entwickler von ZFS, entworfen. LZJB bietet gute Komprimierung mit weniger CPU-Überhang im Vergleich zu GZIP. In der Zukunft wird der Standardkompressionsalgorithmus wahrscheinlich auf LZ4 gewechselt.
<a id=zfs-term-compression-gzip></a>* <em>GZIP</em> - Ein populärer Stromkompressionsalgorithmus ist auch in ZFS verfügbar. Einer der Hauptvorteile von der Verwendung von GZIP ist seine konfigurierbare Komprimierungsstufe. Wenn die Eigenschaft <code>compress</code> gesetzt wird, kann der Administrator die Stufe der Komprimierung wählen, die von <code>gzip1</code>, der kleinsten Komprimierungsstufe, bis zu <code>gzip9</code>, der höchsten Komprimierungsstufe, reicht. Dies erlaubt es dem Administrator zu steuern, wieviel CPU-Zeit für eingesparten Plattenplatz eingetauscht werde soll.
<a id=zfs-term-compression-zle></a>* <em>ZLE</em> - Zero Length Encoding ist ein besonderer Kompressionsalgorithmus, welcher nur fortlaufende Aneinanderreihungen von Nullen komprimiert. Dieser Komprimierungsalgorithmus ist nur sinnvoll, wenn das Dataset viele große Blöcke von Nullen aufweist.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-copies></a>Copies</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Wenn die Eigenschaft <code>copies</code> auf einen Wert grösser als 1 gesetzt wird, weist das ZFS an, mehrere Kopien eines Blocks im <a href=#zfs-term-filesystem>Dateisystem</a> oder <a href=#zfs-term-volume>Volume</a> anzulegen. Diese Eigenschaft auf einem wichtigen Dataset einzustellen sorgt für zusätzliche Redundanz, aus der ein Block wiederhergestellt werden kann, der nicht mehr mit seiner Prüfsumme übereinstimmt. In Pools ohne Redundanz ist die copies-Eigenschaft die einzige Form von Redundanz. Die Eigenschaft kann einen einzelnen schlechten Sektor oder andere Formen von kleineren Verfälschungen wiederherstellen, schützt jedoch nicht den Pool vom Verlust einer gesamten Platte.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-deduplication></a>Deduplizierung</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Prüfsummen ermöglichen es, Duplikate von Blöcken zu erkennen, wenn diese geschrieben werden. Mit Deduplizierung erhöht sich der Referenzzähler eines existierenden, identischen Blocks, was Speicherplatz einspart. Um Blockduplikate zu erkennen, wird im Speicher eine Deduplizierungstabelle (DDT) geführt. Die Tabelle enthält eine Liste von eindeutigen Prüfsummen, die Position dieser Blöcke und einen Referenzzähler. Werden neue Daten geschrieben, wird die Prüfsumme berechnet und mit der Liste verglichen. Wird eine Übereinstimmung gefunden, wird der existierende Block verwendet. Der SHA256-Prüfsummenalgorithmus wird mit Deduplizierung benutzt, um einen sicheren kryptographischen Hash zu bieten. Deduplizierung lässt sich konfigurieren. Wenn <code>dedup</code> auf <code>on</code> steht, wird angenommen, dass eine übereinstimmende Prüfsumme bedeutet, dass die Daten identisch sind. Steht <code>dedup</code> auf <code>verify</code>, werden die Daten in den beiden Blöcken Byte für Byte geprüft, um sicherzustellen, dass diese wirklich identisch sind. Wenn die Daten nicht identisch sind, wird die Kollision im Hash vermerkt und die beiden Blöcke separat gespeichert. Da die DDT den Hash jedes einzigartigen Blocks speichern muss, benötigt sie eine große Menge an Speicher. Eine generelle Faustregel besagt, dass 5-6 GB RAM pro 1 TB deduplizierter Daten benötigt werden. In Situationen, in denen es nicht praktikabel ist, genug RAM vorzuhalten, um die gesamte DDT im Speicher zu belassen, wird die Geschwindigkeit stark darunter leiden, da die DDT von der Platte gelesen werden muss, bevor jeder neue Block geschrieben wird. Deduplizierung kann den L2ARC nutzen, um die DDT zu speichern, was einen guten Mittelweg zwischen schnellem Systemspeicher und langsameren Platten darstellt. Bedenken Sie, dass durch die Verwendung von Komprimierung meistens genauso große Platzersparnis möglich ist, ohne den zusätzlichen Hauptspeicherplatzbedarf.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-scrub></a>Scrub (Bereinigung)</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Anstatt einer Konsistenzprüfung wie <a href="https://man.freebsd.org/cgi/man.cgi?query=fsck&amp;sektion=8&amp;format=html">fsck(8)</a> verwendet ZFS <code>scrub</code>. <code>scrub</code> liest alle Datenblöcke, die auf dem Pool gespeichert sind und prüft deren Prüfsumme gegen die als richtig in den Metadaten gespeicherte Prüfsumme. Eine periodische Prüfung aller im Pool gespeicherten Daten versichert, dass verfälschte Blöcke rekonstruiert werden können, bevor dies nötig ist. Ein Scrub wird nicht nach einem unsauberen Herunterfahren benötigt, wird jedoch einmal alle drei Monate angeraten. Die Prüfsumme von jedem Block wird verifiziert, wenn Blöcke während des normalen Betriebs gelesen werden, jedoch stellt ein Scrub sicher, dass sogar weniger häufig verwendete Blöcke auf stille Verfälschungen hin untersucht werden. Datenintegrität wird dadurch erhöht, besonders wenn es sich um Archivspeichersituationen handelt. Die relative Priorität des <code>scrub</code> lässt sich mit <a href=#zfs-advanced-tuning-scrub_delay><code>vfs.zfs.scrub_delay</code></a> anpassen, um zu verhindern, dass der scrub die Geschwindigkeit von anderen Anfragen auf dem Pool beeinträchtigt.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-quota></a>Dataset Quotas</p></td><td class="tableblock halign-left valign-top"><div class=content><div class=paragraph><p>ZFS bietet sehr schnelle und akkurate Dataset-, Benutzer- und Gruppenspeicherplatzbuchhaltung, zusätzlich zu Quotas und Speicherplatzreservierungen. Dies gibt dem Administrator feingranulare Kontrolle darüber, wie Speicherplatz allokiert und die Reservierung für kritische Dateisysteme vorgenommen wird</p></div><div class=paragraph><p>ZFS unterstützt verschiedene Arten von Quotas: die Dataset-Quota, die <a href=#zfs-term-refquota>Referenzquota (refquota)</a>, die <a href=#zfs-term-userquota>Benutzerquota</a> und die <a href=#zfs-term-groupquota>Gruppenquota</a> sind verfügbar.</p></div><div class=paragraph><p>Quotas beschränken die Menge an Speicherplatz, welche ein Dataset, seine Kinder, einschließlich Schnappschüsse des Datasets, deren Kinder und die Schnappschüsse von diesen Datasets, verbrauchen können.</p></div><div class="admonitionblock note"><table><tbody><tr><td class=icon><i class="fa icon-note" title=Note></i></td><td class=content><div class=paragraph><p>Quotas können nicht auf Volumes gesetzt werden, da die Eigenschaft <code>volsize</code> als eine implizite Quota agiert.</p></div></td></tr></tbody></table></div></div></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-refquota></a>Referenzquota</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Ein Referenzquota beschränkt die Menge an Speicherplatz, die ein Dataset verbrauchen kann durch das Erzwingen einer harten Grenze. Jedoch beinhaltet diese harte Grenze nur Speicherplatz, die das Dataset referenziert und beinhaltet nicht den Speicher, der von Kindern, wie Dateisystemen oder Schnappschüssen, verbraucht wird.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-userquota></a>Benutzerquota</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Benutzerquotas sind hilfreich, um die Menge an Speicherplatz, die ein bestimmter Benutzer verbrauchen kann, einzuschränken.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-groupquota></a>Gruppenquota</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Die Gruppenquota beschränkt die Menge an Speicherplatz, die eine bestimmte Gruppe verbrauchen darf.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-reservation></a>Dataset-Reservierung</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Die Eigenschaft <code>reservation</code> ermöglicht es, ein Minimum an Speicherplatz für ein bestimmtes Dataset und dessen Kinder zu garantieren. Wenn eine Reservierung von 10 GB auf <span class=filename>storage/home/bob</span> gesetzt ist und ein anderes Dataset versucht, allen freien Speicherplatz zu verwenden, bleiben zumindest noch 10 GB an Speicher reserviert. Wenn von <span class=filename>storage/home/bob</span> ein Schnappschuss angelegt wird, wird dieser von der Reservierung abgezogen und zählt damit dagegen. Die Eigenschaft <a href=#zfs-term-refreservation><code>refreservation</code></a> funktioniert auf ähnliche Weise, jedoch <em>exkludiert</em> diese Kinder wie Schnappschüsse.</p><p class=tableblock>Reservierungen jeder Art sind in vielen Situationen nützlich, so wie bei der Planung und dem Testen der richtigen Speicherplatzallokation in einem neuen System oder durch die Zusicherung, dass genug Speicherplatz auf Dateisystemen für Audio-Logs oder Systemwiederherstellungsprozeduren und Dateien verfügbar ist.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-refreservation></a>Referenzreservierung</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Die Eigenschaft <code>refreservation</code> ermöglicht es, ein Minimum an Speicherplatz für die Verwendung eines bestimmten Datasets zu garantieren, <em>exklusiv</em> dessen Kinder. Das bedeutet, dass wenn eine 10 GB-Reservierung auf <span class=filename>storage/home/bob</span> vorhanden ist und ein anderes Dataset versucht, alle freien Speicherplatz aufzubrauchen, sind zumindest noch 10 GB Speicher reserviert. Im Gegensatz zu einer regulären <a href=#zfs-term-reservation>Reservierung</a> wird der Speicher von Schnappschüssen und Kinddataset nicht gegen die Reservierung gezählt. Beispielsweise, wenn ein Schnappschuss von <span class=filename>storage/home/bob</span> angelegt wird, muss genug Plattenplatz außerhalb der Menge an <code>refreservation</code> vorhanden sein, damit die Operation erfolgreich durchgeführt wird. Kinder des Hauptdatasets werden nicht in die Menge an <code>refreservation</code> gezählt und dringen auf diese Weise auch nicht in den gesetzten Speicher ein.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-resilver></a>Resilver</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Wenn eine Platte ausfällt und ersetzt wird, muss die neue Platte mit den Daten gefüllt werden, die verloren gegangen sind. Der Prozess der Verwendung der Paritätsinformationen, welche über die übrigen Platten verteilt sind, um die fehlenden Daten zu berechnen und auf die neue Platte zu übertragen, wird <em>resilvering</em> genannt.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-online></a>Online</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Ein Pool oder vdev im Zustand <code>Online</code> besitzt alle verbundenen Mitgliedsgeräte und ist voll funktionsfähig. Individuelle Geräte im Zustand <code>Online</code> funktionieren normal.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-offline></a>Offline</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Individuelle Geräte lassen sich vom Administrator in den Zustand <code>Offline</code> versetzen, wenn es ausreichend Redundanz gibt, um zu verhindern, dass der Pool oder das vdev in den Zustand <a href=#zfs-term-faulted>Faulted</a> versetzt wird. Ein Administrator kann eine Platte vor einem Austausch offline nehmen oder um es leichter zu machen, diese zu identifizieren.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-degraded></a>Degraded</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Ein Pool oder vdev im Zustand <code>Degraded</code> hat eine oder mehrere Platten, welche getrennt wurden oder ausgefallen sind. Der Pool kann immer noch verwendet werden, doch wenn noch weitere Geräte ausfallen, kann der Pool nicht wiederhergestellt werden. Die fehlenden Geräte anzuschließen oder die defekten Platten zu ersetzen wird den Pool wieder in den Zustand <a href=#zfs-term-online>Online</a> versetzen, nachdem die angeschlossenen oder neuen Geräte den <a href=#zfs-term-resilver>Resilver</a>-Prozess abgeschlossen haben.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock><a id=zfs-term-faulted></a>Faulted</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Ein Pool oder vdev im Zustand <code>Faulted</code> funktioniert nicht länger. Die Daten darauf sind nicht mehr länger verfügbar. Ein Pool oder vdev geht in den Zustand <code>Faulted</code> über, wenn die Anzahl der fehlenden oder defekten Geräte die Redundanzstufe im vdev überschreiten. Wenn fehlende Geräte angeschlossen werden, geht der Pool wieder in den Zustand <a href=#zfs-term-online>Online</a>. Wenn es nicht genügend Redundanz gibt, um die Anzahl an defekten Platten zu kompensieren, sind die Inhalte des Pools verloren und müssen von der Sicherung wiederhergestellt werden.</p></td></tr></tbody></table></div></div></div><hr><div class=last-modified><p><strong>Last modified on</strong>: 9. März 2024 by <a href="https://cgit.freebsd.org/doc/commit/?id=6199af92e7" target=_blank>Danilo G. Baio</a></p></div><div class=buttons><div class=prev><i class="fa fa-angle-left" aria-hidden=true title=Prev></i><div class=container><a href=http://172.16.201.134:1313/de/books/handbook/geom class=direction>Prev</a></div></div><div class=home><i class="fa fa-home" aria-hidden=true title=Home></i><div class=container><a href=../ class=direction>Home</a></div></div><div class=next><div class=container><a href=http://172.16.201.134:1313/de/books/handbook/filesystems class=direction>Next</a></div><i class="fa fa-angle-right" aria-hidden=true title=Next></i></div></div><label class="hidden book-menu-overlay" for=menu-control></label></div><aside class=toc><div class=toc-content><h3>Table of Contents</h3><nav id=TableOfContents><ul><li><a href=#zfs-differences>19.1. Was ZFS anders macht</a></li><li><a href=#zfs-quickstart>19.2. Schnellstartanleitung</a></li><li><a href=#zfs-zpool>19.3. <code>zpool</code> Administration</a></li><li><a href=#zfs-zfs>19.4. <code>zfs</code> Administration</a></li><li><a href=#zfs-zfs-allow>19.5. Delegierbare Administration</a></li><li><a href=#zfs-advanced>19.6. Themen für Fortgeschrittene</a></li><li><a href=#zfs-links>19.7. Zusätzliche Informationen</a></li><li><a href=#zfs-term>19.8. ZFS-Eigenschaften und Terminologie</a></li></ul></nav><hr><div class=resources><h3>Resources</h3><ul class=contents><li><i class="fa fa-file-pdf-o" aria-hidden=true title="Download PDF"></i><a href=https://download.freebsd.org/doc/de/books/handbook/handbook_de.pdf>Download PDF</a></li><li><i class="fa fa-pencil-square-o" aria-hidden=true title="Edit this page"></i><a href=https://github.com/freebsd/freebsd-doc/blob/main/documentation/content/de/_index target=_blank>Edit this page</a></li></ul></div></div></aside><a class=to-top href=#top><i class="fa fa-arrow-circle-up" aria-hidden=true></i></a></main><footer><div class=footer-container><section class=logo-column><img src=http://172.16.201.134:1313/images/FreeBSD-colors.svg width=160 height=50 alt="FreeBSD logo"><div class=options-container><div class=language-container><a id=languages href=http://172.16.201.134:1313/de/languages><img src=http://172.16.201.134:1313/images/language.png class=language-image alt="Choose language">
<span>German</span></a></div><div class=theme-container><select id=theme-chooser><option value=theme-system>System</option><option value=theme-light>Light</option><option value=theme-dark>Dark</option><option value=theme-high-contrast>High contrast</option></select></div></div></section><section class=about-column><h3 class=column-title>About</h3><ul class=column-elements-container><li><a href=https://www.freebsd.org/about/ target=_blank class=column-element>FreeBSD</a></li><li><a href=https://freebsdfoundation.org/ target=_blank class=column-element>FreeBSD Foundation</a></li><li><a href=https://www.freebsd.org/where/ target=_blank class=column-element>Get FreeBSD</a></li><li><a href=https://www.freebsd.org/internal/code-of-conduct target=_blank class=column-element>Code of Conduct</a></li><li><a href=https://www.freebsd.org/security/ target=_blank class=column-element>Security Advisories</a></li></ul></section><section class=documentation-column><h3 class=column-title>Documentation</h3><ul class=column-elements-container><li><a href=/de class=column-element>Documentation portal</a></li><li><a href=https://man.FreeBSD.org target=_blank class=column-element>Manual pages</a></li><li><a href=https://papers.FreeBSD.org target=_blank class=column-element>Presentations and papers</a></li><li><a href=https://docs-archive.freebsd.org/doc/ target=_blank class=column-element>Previous versions</a></li><li><a href=https://docs-archive.freebsd.org/44doc/ target=_blank class=column-element>4.4BSD Documents</a></li><li><a href=https://wiki.freebsd.org/ target=_blank class=column-element>Wiki</a></li></ul></section><section class=community-column><h3 class=column-title>Community</h3><ul class=column-elements-container><li><a href=http://172.16.201.134:1313/de/articles/contributing class=column-element>Get involved</a></li><li><a href=https://forums.freebsd.org/ target=_blank class=column-element>Community forum</a></li><li><a href=https://lists.freebsd.org/ target=_blank class=column-element>Mailing lists</a></li><li><a href=https://wiki.freebsd.org/IRC/Channels target=_blank class=column-element>IRC Channels</a></li><li><a href=https://bugs.freebsd.org/bugzilla/ target=_blank class=column-element>Bug Tracker</a></li></ul></section><section class=legal-column><h3 class=column-title>Legal</h3><ul class=column-elements-container><li><a href=https://freebsdfoundation.org/donate/ target=_blank class=column-element>Donations</a></li><li><a href=https://www.freebsd.org/copyright/freebsd-license/ target=_blank class=column-element>Licensing</a></li><li><a href=https://www.freebsd.org/privacy/ target=_blank class=column-element>Privacy Policy</a></li><li><a href=https://www.freebsd.org/copyright/ target=_blank class=column-element>Legal notices</a></li></ul></section><section class=copyright-column><p>&copy; 1994-2024 The FreeBSD Project. All rights reserved</p><span>Made with <span class=heart>♥</span> by the FreeBSD Community</span></section></div></footer></body></html>