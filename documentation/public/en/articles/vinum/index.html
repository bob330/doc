<!doctype html><html class=theme-light lang=en><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="The vinum Volume Manager in FreeBSD"><meta name=keywords content="vinum,Volume Manager,FreeBSD"><meta name=copyright content="1995-2024 The FreeBSD Foundation"><link rel=canonical href=http://172.16.201.134:1313/en/articles/vinum/><title>The vinum Volume Manager | FreeBSD Documentation Portal</title>
<meta name=theme-color content="#790000"><meta name=color-scheme content="system light dark high-contrast"><link rel="shortcut icon" href=http://172.16.201.134:1313/favicon.ico><link rel=stylesheet href=http://172.16.201.134:1313/styles/main.min.css><link rel=stylesheet href=http://172.16.201.134:1313/css/font-awesome-min.css><script defer src=/js/theme-chooser.min.js></script><script defer src=/js/copy-clipboard.min.js></script><script defer src=/js/search.min.js></script><meta name=twitter:card content="summary"><meta name=twitter:domain content="docs.FreeBSD.org"><meta name=twitter:site content="@freebsd"><meta name=twitter:url content="https://twitter.com/freebsd"><meta property="og:title" content="The vinum Volume Manager"><meta property="og:description" content="The vinum Volume Manager in FreeBSD"><meta property="og:type" content="website"><meta property="og:image" content="http://172.16.201.134:1313/favicon.ico"><meta property="og:image:alt" content="FreeBSD Logo"><meta property="og:locale" content="en"><meta property="og:url" content="http://172.16.201.134:1313/en/articles/vinum/"><meta property="og:site_name" content="FreeBSD Documentation Portal"><script type=application/ld+json>{"@context":"http://schema.org","@type":"Article","url":"http:\/\/172.16.201.134:1313\/en\/articles\/vinum\/","name":"FreeBSD Documentation Portal","headline":"FreeBSD Documentation Portal","description":"FreeBSD Documentation Portal"}</script></head><body><header><div class=header-container><div class=logo-menu-bars-container><a href=https://www.FreeBSD.org class=logo><img src=http://172.16.201.134:1313/images/FreeBSD-monochromatic.svg width=160 height=50 alt="FreeBSD logo">
</a><label class=menu-bars for=menu-bars><i class="fa fa-bars" aria-hidden=true></i></label></div><input id=menu-bars type=checkbox><nav><ul class=menu><li class=menu-item><input id=about type=checkbox>
<a href=# aria-label="Navigate to About section"><label class=menu-item-description for=about>About
<i class="fa fa-angle-down fa-lg" aria-hidden=true></i></label></a><ul class=sub-menu><li class=title><a href=https://www.freebsd.org/about/ target=_blank>About</a></li><li><a href=https://www.freebsd.org/about/ target=_blank>FreeBSD</a></li><li><a href=https://freebsdfoundation.org/about-us/about-the-foundation/ target=_blank>FreeBSD Foundation</a></li><li><a href=https://www.freebsd.org/internal/code-of-conduct/ target=_blank>Code of Conduct</a></li></ul></li><li class=menu-item><input id=download type=checkbox>
<a href=# aria-label="Navigate to get FreeBSD section"><label class=menu-item-description for=download>Get FreeBSD
<i class="fa fa-angle-down fa-lg" aria-hidden=true></i></label></a><ul class=sub-menu><li class=title><a href=https://www.freebsd.org/where/ target=_blank>Get FreeBSD</a></li><li><a href=https://www.freebsd.org/releases/ target=_blank>Release Information</a></li><li><a href=https://www.freebsd.org/releng/ target=_blank>Release Engineering</a></li><li><a href=https://www.freebsd.org/security/ target=_blank>Security Advisories</a></li></ul></li><li class=menu-item><input id=documentation type=checkbox>
<a href=# aria-label="Navigate to get Documentation section"><label class=menu-item-description for=documentation>Documentation
<i class="fa fa-angle-down fa-lg" aria-hidden=true></i></label></a><ul class=sub-menu><li class=title><a href=/en>Documentation portal</a></li><li><a href=http://172.16.201.134:1313/en/books/handbook>FreeBSD Handbook</a></li><li><a href=http://172.16.201.134:1313/en/books/porters-handbook>Porter's Handbook</a></li><li><a href=https://docs.FreeBSD.org/en/books/fdp-primer>Documentation Project Handbook</a></li><li><a href=https://man.FreeBSD.org target=_blank>Manual pages</a></li><li><a href=https://papers.FreeBSD.org target=_blank>Presentations and papers</a></li><li><a href=https://wiki.FreeBSD.org target=_blank>Wiki</a></li><li><a href=http://172.16.201.134:1313/en/books>Books</a></li><li><a href=http://172.16.201.134:1313/en/articles>Articles</a></li></ul></li><li class=menu-item><input id=community type=checkbox>
<a href=# aria-label="Navigate to get Community section"><label class=menu-item-description for=community>Community
<i class="fa fa-angle-down fa-lg" aria-hidden=true></i></label></a><ul class=sub-menu><li class=title><a href=https://www.freebsd.org/community/>Community</a></li><li><a href=http://172.16.201.134:1313/en/articles/contributing>Get involved</a></li><li><a href=https://forums.freebsd.org/ target=_blank>Forum</a></li><li><a href=https://lists.freebsd.org/ target=_blank>Mailing lists</a></li><li><a href=https://wiki.freebsd.org/IRC/Channels target=_blank>IRC Channels</a></li><li><a href=https://bugs.freebsd.org/bugzilla/ target=_blank>Bug Tracker</a></li><li><a href=https://www.freebsd.org/support/ target=_blank>Support</a></li></ul></li></ul></nav><div class=search-donate-container><form class=search method=get id=search-header-form action=https://docs.freebsd.org/search name=search-header-form><input type=hidden name=DB value=en>
<input id=words name=P type=text size=20 maxlength=255>
<button>
<i class="fa fa-search" aria-hidden=true></i></button></form><div class=donate><a href=https://freebsdfoundation.org/donate/ target=_blank><span class=heart>♥</span>
Donate</a></div></div></div></header><main class=main-wrapper-article><div class=article><h1 class=title>The vinum Volume Manager</h1><div class=toc-mobile><h3>Table of Contents</h3><nav id=TableOfContents><ul><li><a href=#vinum-synopsis>1. Synopsis</a></li><li><a href=#vinum-access-bottlenecks>2. Access Bottlenecks</a></li><li><a href=#vinum-data-integrity>3. Data Integrity</a></li><li><a href=#vinum-objects>4. <span class=filename>vinum</span> Objects</a></li><li><a href=#vinum-examples>5. Some Examples</a></li><li><a href=#vinum-object-naming>6. Object Naming</a></li><li><a href=#vinum-config>7. Configuring <span class=filename>vinum</span></a></li><li><a href=#vinum-root>8. Using <span class=filename>vinum</span> for the Root File System</a></li></ul></nav></div><div id=preamble><div class=sectionbody><hr></div></div><div class=sect1><h2 id=vinum-synopsis>1. Synopsis<a class=anchor href=#vinum-synopsis></a></h2><div class=sectionbody><div class=paragraph><p>No matter the type of disks, there are always potential problems.
The disks can be too small, too slow, or too unreliable to meet the system’s requirements.
While disks are getting bigger, so are data storage requirements.
Often a file system is needed that is bigger than a disk’s capacity.
Various solutions to these problems have been proposed and implemented.</p></div><div class=paragraph><p>One method is through the use of multiple, and sometimes redundant, disks.
In addition to supporting various cards and controllers for hardware Redundant Array of Independent Disks RAID systems, the base FreeBSD system includes the <span class=filename>vinum</span> volume manager, a block device driver that implements virtual disk drives and addresses these three problems.
<span class=filename>vinum</span> provides more flexibility, performance, and reliability than traditional disk storage and implements <code>RAID</code>-0, <code>RAID</code>-1, and <code>RAID</code>-5 models, both individually and in combination.</p></div><div class=paragraph><p>This chapter provides an overview of potential problems with traditional disk storage, and an introduction to the <span class=filename>vinum</span> volume manager.</p></div><div class="admonitionblock note"><table><tbody><tr><td class=icon><i class="fa icon-note" title=Note></i></td><td class=content><div class=paragraph><p>Starting with FreeBSD 5, <span class=filename>vinum</span> has been rewritten to fit into the <a href=https://docs.freebsd.org/en/books/handbook/#geom>GEOM architecture</a>, while retaining the original ideas, terminology, and on-disk metadata.
This rewrite is called <em>gvinum</em> (for <em>GEOM vinum</em>).
While this chapter uses the term <span class=filename>vinum</span>, any command invocations should be performed with <code>gvinum</code>.
The name of the kernel module has changed from the original <span class=filename>vinum.ko</span> to <span class=filename>geom_vinum.ko</span>, and all device nodes reside under <span class=filename>/dev/gvinum</span> instead of <span class=filename>/dev/vinum</span>.
As of FreeBSD 6, the original <span class=filename>vinum</span> implementation is no longer available in the code base.</p></div></td></tr></tbody></table></div></div></div><div class=sect1><h2 id=vinum-access-bottlenecks>2. Access Bottlenecks<a class=anchor href=#vinum-access-bottlenecks></a></h2><div class=sectionbody><div class=paragraph><p>Modern systems frequently need to access data in a highly concurrent manner.
For example, large FTP or HTTP servers can maintain thousands of concurrent sessions and have multiple 100 Mbit/s connections to the outside world, well beyond the sustained transfer rate of most disks.</p></div><div class=paragraph><p>Current disk drives can transfer data sequentially at up to 70 MB/s, but this value is of little importance in an environment where many independent processes access a drive, and where they may achieve only a fraction of these values.
In such cases, it is more interesting to view the problem from the viewpoint of the disk subsystem.
The important parameter is the load that a transfer places on the subsystem, or the time for which a transfer occupies the drives involved in the transfer.</p></div><div class=paragraph><p>In any disk transfer, the drive must first position the heads, wait for the first sector to pass under the read head, and then perform the transfer.
These actions can be considered to be atomic as it does not make any sense to interrupt them.</p></div><div class=paragraph><p><a id=vinum-latency></a>Consider a typical transfer of about 10 kB: the current generation of high-performance disks can position the heads in an average of 3.5 ms.
The fastest drives spin at 15,000 rpm, so the average rotational latency (half a revolution) is 2 ms.
At 70 MB/s, the transfer itself takes about 150 μs, almost nothing compared to the positioning time.
In such a case, the effective transfer rate drops to a little over 1 MB/s and is clearly highly dependent on the transfer size.</p></div><div class=paragraph><p>The traditional and obvious solution to this bottleneck is "more spindles": rather than using one large disk, use several smaller disks with the same aggregate storage space.
Each disk is capable of positioning and transferring independently, so the effective throughput increases by a factor close to the number of disks used.</p></div><div class=paragraph><p>The actual throughput improvement is smaller than the number of disks involved.
Although each drive is capable of transferring in parallel, there is no way to ensure that the requests are evenly distributed across the drives.
Inevitably the load on one drive will be higher than on another.</p></div><div class=paragraph><p>The evenness of the load on the disks is strongly dependent on the way the data is shared across the drives.
In the following discussion, it is convenient to think of the disk storage as a large number of data sectors which are addressable by number, rather like the pages in a book.
The most obvious method is to divide the virtual disk into groups of consecutive sectors the size of the individual physical disks and store them in this manner, rather like taking a large book and tearing it into smaller sections.
This method is called <em>concatenation</em> and has the advantage that the disks are not required to have any specific size relationships.
It works well when the access to the virtual disk is spread evenly about its address space.
When access is concentrated on a smaller area, the improvement is less marked.
<a href=../vinum/#vinum-concat>Concatenated Organization</a> illustrates the sequence in which storage units are allocated in a concatenated organization.</p></div><div id=vinum-concat class=imageblock><div class=content><img src=../../../images/articles/vinum/vinum-concat.png alt="vinum concat"></div><div class=title>Figure 1. Concatenated Organization</div></div><div class=paragraph><p>An alternative mapping is to divide the address space into smaller, equal-sized components and store them sequentially on different devices.
For example, the first 256 sectors may be stored on the first disk, the next 256 sectors on the next disk and so on.
After filling the last disk, the process repeats until the disks are full.
This mapping is called <em>striping</em> or RAID-0.</p></div><div class=paragraph><p><code>RAID</code> offers various forms of fault tolerance, though RAID-0 is somewhat misleading as it provides no redundancy.
Striping requires somewhat more effort to locate the data, and it can cause additional I/O load where a transfer is spread over multiple disks, but it can also provide a more constant load across the disks.
<a href=../vinum/#vinum-striped>Striped Organization</a> illustrates the sequence in which storage units are allocated in a striped organization.</p></div><div id=vinum-striped class=imageblock><div class=content><img src=../../../images/articles/vinum/vinum-striped.png alt="vinum striped"></div><div class=title>Figure 2. Striped Organization</div></div></div></div><div class=sect1><h2 id=vinum-data-integrity>3. Data Integrity<a class=anchor href=#vinum-data-integrity></a></h2><div class=sectionbody><div class=paragraph><p>The final problem with disks is that they are unreliable.
Although reliability has increased tremendously over the last few years, disk drives are still the most likely core component of a server to fail.
When they do, the results can be catastrophic and replacing a failed disk drive and restoring data can result in server downtime.</p></div><div class=paragraph><p>One approach to this problem is <em>mirroring</em>, or <code>RAID-1</code>, which keeps two copies of the data on different physical hardware.
Any write to the volume writes to both disks; a read can be satisfied from either, so if one drive fails, the data is still available on the other drive.</p></div><div class=paragraph><p>Mirroring has two problems:</p></div><div class=ulist><ul><li><p>It requires twice as much disk storage as a non-redundant solution.</p></li><li><p>Writes must be performed to both drives, so they take up twice the bandwidth of a non-mirrored volume. Reads do not suffer from a performance penalty and can even be faster.</p></li></ul></div><div class=paragraph><p>An alternative solution is <em>parity</em>, implemented in <code>RAID</code> levels 2, 3, 4 and 5.
Of these, <code>RAID-5</code> is the most interesting.
As implemented in <span class=filename>vinum</span>, it is a variant on a striped organization which dedicates one block of each stripe to parity one of the other blocks.
As implemented by <span class=filename>vinum</span>, a <code>RAID-5</code> plex is similar to a striped plex, except that it implements <code>RAID-5</code> by including a parity block in each stripe.
As required by <code>RAID-5</code>, the location of this parity block changes from one stripe to the next.
The numbers in the data blocks indicate the relative block numbers.</p></div><div id=vinum-raid5-org class=imageblock><div class=content><img src=../../../images/articles/vinum/vinum-raid5-org.png alt="vinum raid5 org"></div><div class=title>Figure 3. <code>RAID</code>-5 Organization</div></div><div class=paragraph><p>Compared to mirroring, <code>RAID-5</code> has the advantage of requiring significantly less storage space.
Read access is similar to that of striped organizations, but write access is significantly slower, approximately 25% of the read performance.
If one drive fails, the array can continue to operate in degraded mode where a read from one of the remaining accessible drives continues normally, but a read from the failed drive is recalculated from the corresponding block from all the remaining drives.</p></div></div></div><div class=sect1><h2 id=vinum-objects>4. <span class=filename>vinum</span> Objects<a class=anchor href=#vinum-objects></a></h2><div class=sectionbody><div class=paragraph><p>To address these problems, <span class=filename>vinum</span> implements a four-level hierarchy of objects:</p></div><div class=ulist><ul><li><p>The most visible object is the virtual disk, called a <em>volume</em>. Volumes have essentially the same properties as a UNIX® disk drive, though there are some minor differences. For one, they have no size limitations.</p></li><li><p>Volumes are composed of <em>plexes</em>, each of which represent the total address space of a volume. This level in the hierarchy provides redundancy. Think of plexes as individual disks in a mirrored array, each containing the same data.</p></li><li><p>Since <span class=filename>vinum</span> exists within the UNIX® disk storage framework, it would be possible to use UNIX® partitions as the building block for multi-disk plexes. In fact, this turns out to be too inflexible as UNIX® disks can have only a limited number of partitions. Instead, <span class=filename>vinum</span> subdivides a single UNIX® partition, the <em>drive</em>, into contiguous areas called <em>subdisks</em>, which are used as building blocks for plexes.</p></li><li><p>Subdisks reside on <span class=filename>vinum</span><em>drives</em>, currently UNIX® partitions. <span class=filename>vinum</span> drives can contain any number of subdisks. With the exception of a small area at the beginning of the drive, which is used for storing configuration and state information, the entire drive is available for data storage.</p></li></ul></div><div class=paragraph><p>The following sections describe the way these objects provide the functionality required of <span class=filename>vinum</span>.</p></div><div class=sect2><h3 id=_volume_size_considerations>4.1. Volume Size Considerations<a class=anchor href=#_volume_size_considerations></a></h3><div class=paragraph><p>Plexes can include multiple subdisks spread over all drives in the <span class=filename>vinum</span> configuration.
As a result, the size of an individual drive does not limit the size of a plex or a volume.</p></div></div><div class=sect2><h3 id=_redundant_data_storage>4.2. Redundant Data Storage<a class=anchor href=#_redundant_data_storage></a></h3><div class=paragraph><p><span class=filename>vinum</span> implements mirroring by attaching multiple plexes to a volume.
Each plex is a representation of the data in a volume.
A volume may contain between one and eight plexes.</p></div><div class=paragraph><p>Although a plex represents the complete data of a volume, it is possible for parts of the representation to be physically missing, either by design (by not defining a subdisk for parts of the plex) or by accident (as a result of the failure of a drive).
As long as at least one plex can provide the data for the complete address range of the volume, the volume is fully functional.</p></div></div><div class=sect2><h3 id=_which_plex_organization>4.3. Which Plex Organization?<a class=anchor href=#_which_plex_organization></a></h3><div class=paragraph><p><span class=filename>vinum</span> implements both concatenation and striping at the plex level:</p></div><div class=ulist><ul><li><p>A <em>concatenated plex</em> uses the address space of each subdisk in turn. Concatenated plexes are the most flexible as they can contain any number of subdisks, and the subdisks may be of different length. The plex may be extended by adding additional subdisks. They require less CPU time than striped plexes, though the difference in CPU overhead is not measurable. On the other hand, they are most susceptible to hot spots, where one disk is very active and others are idle.</p></li><li><p>A <em>striped plex</em> stripes the data across each subdisk. The subdisks must all be the same size and there must be at least two subdisks to distinguish it from a concatenated plex. The greatest advantage of striped plexes is that they reduce hot spots. By choosing an optimum sized stripe, about 256 kB, the load can be evened out on the component drives. Extending a plex by adding new subdisks is so complicated that <span class=filename>vinum</span> does not implement it.</p></li></ul></div><div class=paragraph><p><a href=../vinum/#vinum-comparison><span class=filename>vinum</span> Plex Organizations</a> summarizes the advantages and disadvantages of each plex organization.</p></div><table id=vinum-comparison class="tableblock frame-none grid-all stretch"><caption class=title>Table 1. <span class=filename>vinum</span> Plex Organizations</caption><col style=width:20%><col style=width:20%><col style=width:20%><col style=width:20%><col style=width:20%><thead><tr><th class="tableblock halign-left valign-top">Plex type</th><th class="tableblock halign-left valign-top">Minimum subdisks</th><th class="tableblock halign-left valign-top">Can add subdisks</th><th class="tableblock halign-left valign-top">Must be equal size</th><th class="tableblock halign-left valign-top">Application</th></tr></thead><tbody><tr><td class="tableblock halign-left valign-top"><p class=tableblock>concatenated</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>1</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>yes</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>no</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>Large data storage with maximum placement flexibility and moderate performance</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock>striped</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>2</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>no</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>yes</p></td><td class="tableblock halign-left valign-top"><p class=tableblock>High performance in combination with highly concurrent access</p></td></tr></tbody></table></div></div></div><div class=sect1><h2 id=vinum-examples>5. Some Examples<a class=anchor href=#vinum-examples></a></h2><div class=sectionbody><div class=paragraph><p><span class=filename>vinum</span> maintains a <em>configuration database</em> which describes the objects known to an individual system.
Initially, the user creates the configuration database from one or more configuration files using <a href="https://man.freebsd.org/cgi/man.cgi?query=gvinum&amp;sektion=8&amp;format=html">gvinum(8)</a>.
<span class=filename>vinum</span> stores a copy of its configuration database on each disk <em>device</em> under its control.
This database is updated on each state change, so that a restart accurately restores the state of each <span class=filename>vinum</span> object.</p></div><div class=sect2><h3 id=_the_configuration_file>5.1. The Configuration File<a class=anchor href=#_the_configuration_file></a></h3><div class=paragraph><p>The configuration file describes individual <span class=filename>vinum</span> objects.
The definition of a simple volume might be:</p></div><div class="literalblock programlisting"><div class=content><pre>    drive a device /dev/da3h
    volume myvol
      plex org concat
        sd length 512m drive a</pre></div></div><div class=paragraph><p>This file describes four <span class=filename>vinum</span> objects:</p></div><div class=ulist><ul><li><p>The <em>drive</em> line describes a disk partition (<em>drive</em>) and its location relative to the underlying hardware. It is given the symbolic name <em>a</em>. This separation of symbolic names from device names allows disks to be moved from one location to another without confusion.</p></li><li><p>The <em>volume</em> line describes a volume. The only required attribute is the name, in this case <em>myvol</em>.</p></li><li><p>The <em>plex</em> line defines a plex. The only required parameter is the organization, in this case <em>concat</em>. No name is necessary as the system automatically generates a name from the volume name by adding the suffix <em>.px</em>, where <em>x</em> is the number of the plex in the volume. Thus this plex will be called <em>myvol.p0</em>.</p></li><li><p>The <em>sd</em> line describes a subdisk. The minimum specifications are the name of a drive on which to store it, and the length of the subdisk. No name is necessary as the system automatically assigns names derived from the plex name by adding the suffix <em>.sx</em>, where <em>x</em> is the number of the subdisk in the plex. Thus <span class=filename>vinum</span> gives this subdisk the name <em>myvol.p0.s0</em>.</p></li></ul></div><div class=paragraph><p>After processing this file, <a href="https://man.freebsd.org/cgi/man.cgi?query=gvinum&amp;sektion=8&amp;format=html">gvinum(8)</a> produces the following output:</p></div><div class="literalblock programlisting"><div class=content><pre>#  gvinum -&gt; create config1
Configuration summary
Drives:         1 (4 configured)
Volumes:        1 (4 configured)
Plexes:         1 (8 configured)
Subdisks:       1 (16 configured)

  D a                     State: up       Device /dev/da3h      Avail: 2061/2573 MB (80%)

  V myvol                 State: up       Plexes:       1 Size:      512 MB

  P myvol.p0            C State: up       Subdisks:     1 Size:      512 MB

  S myvol.p0.s0           State: up       PO:        0  B Size:      512 MB</pre></div></div><div class=paragraph><p>This output shows the brief listing format of <a href="https://man.freebsd.org/cgi/man.cgi?query=gvinum&amp;sektion=8&amp;format=html">gvinum(8)</a>.
It is represented graphically in <a href=../vinum/#vinum-simple-vol>A Simple <span class=filename>vinum</span> Volume</a>.</p></div><div id=vinum-simple-vol class=imageblock><div class=content><img src=../../../images/articles/vinum/vinum-simple-vol.png alt="vinum simple vol"></div><div class=title>Figure 4. A Simple <span class=filename>vinum</span> Volume</div></div><div class=paragraph><p>This figure, and the ones which follow, represent a volume, which contains the plexes, which in turn contains the subdisks.
In this example, the volume contains one plex, and the plex contains one subdisk.</p></div><div class=paragraph><p>This particular volume has no specific advantage over a conventional disk partition.
It contains a single plex, so it is not redundant.
The plex contains a single subdisk, so there is no difference in storage allocation from a conventional disk partition.
The following sections illustrate various more interesting configuration methods.</p></div></div><div class=sect2><h3 id=_increased_resilience_mirroring>5.2. Increased Resilience: Mirroring<a class=anchor href=#_increased_resilience_mirroring></a></h3><div class=paragraph><p>The resilience of a volume can be increased by mirroring.
When laying out a mirrored volume, it is important to ensure that the subdisks of each plex are on different drives, so that a drive failure will not take down both plexes.
The following configuration mirrors a volume:</p></div><div class="literalblock programlisting"><div class=content><pre>	drive b device /dev/da4h
	volume mirror
      plex org concat
        sd length 512m drive a
	  plex org concat
	    sd length 512m drive b</pre></div></div><div class=paragraph><p>In this example, it was not necessary to specify a definition of drive <em>a</em> again, since <span class=filename>vinum</span> keeps track of all objects in its configuration database.
After processing this definition, the configuration looks like:</p></div><div class="literalblock programlisting"><div class=content><pre>	Drives:         2 (4 configured)
	Volumes:        2 (4 configured)
	Plexes:         3 (8 configured)
	Subdisks:       3 (16 configured)

	D a                     State: up       Device /dev/da3h       Avail: 1549/2573 MB (60%)
	D b                     State: up       Device /dev/da4h       Avail: 2061/2573 MB (80%)

    V myvol                 State: up       Plexes:       1 Size:        512 MB
    V mirror                State: up       Plexes:       2 Size:        512 MB

    P myvol.p0            C State: up       Subdisks:     1 Size:        512 MB
    P mirror.p0           C State: up       Subdisks:     1 Size:        512 MB
    P mirror.p1           C State: initializing     Subdisks:     1 Size:        512 MB

    S myvol.p0.s0           State: up       PO:        0  B Size:        512 MB
	S mirror.p0.s0          State: up       PO:        0  B Size:        512 MB
	S mirror.p1.s0          State: empty    PO:        0  B Size:        512 MB</pre></div></div><div class=paragraph><p><a href=../vinum/#vinum-mirrored-vol>A Mirrored <span class=filename>vinum</span> Volume</a> shows the structure graphically.</p></div><div id=vinum-mirrored-vol class=imageblock><div class=content><img src=../../../images/articles/vinum/vinum-mirrored-vol.png alt="vinum mirrored vol"></div><div class=title>Figure 5. A Mirrored <span class=filename>vinum</span> Volume</div></div><div class=paragraph><p>In this example, each plex contains the full 512 MB of address space.
As in the previous example, each plex contains only a single subdisk.</p></div></div><div class=sect2><h3 id=_optimizing_performance>5.3. Optimizing Performance<a class=anchor href=#_optimizing_performance></a></h3><div class=paragraph><p>The mirrored volume in the previous example is more resistant to failure than an unmirrored volume, but its performance is less as each write to the volume requires a write to both drives, using up a greater proportion of the total disk bandwidth.
Performance considerations demand a different approach: instead of mirroring, the data is striped across as many disk drives as possible.
The following configuration shows a volume with a plex striped across four disk drives:</p></div><div class="literalblock programlisting"><div class=content><pre>        drive c device /dev/da5h
	drive d device /dev/da6h
	volume stripe
	plex org striped 512k
	  sd length 128m drive a
	  sd length 128m drive b
	  sd length 128m drive c
	  sd length 128m drive d</pre></div></div><div class=paragraph><p>As before, it is not necessary to define the drives which are already known to <span class=filename>vinum</span>.
After processing this definition, the configuration looks like:</p></div><div class="literalblock programlisting"><div class=content><pre>	Drives:         4 (4 configured)
	Volumes:        3 (4 configured)
	Plexes:         4 (8 configured)
	Subdisks:       7 (16 configured)

    D a                     State: up       Device /dev/da3h        Avail: 1421/2573 MB (55%)
    D b                     State: up       Device /dev/da4h        Avail: 1933/2573 MB (75%)
    D c                     State: up       Device /dev/da5h        Avail: 2445/2573 MB (95%)
    D d                     State: up       Device /dev/da6h        Avail: 2445/2573 MB (95%)

    V myvol                 State: up       Plexes:       1 Size:        512 MB
    V mirror                State: up       Plexes:       2 Size:        512 MB
    V striped               State: up       Plexes:       1 Size:        512 MB

    P myvol.p0            C State: up       Subdisks:     1 Size:        512 MB
    P mirror.p0           C State: up       Subdisks:     1 Size:        512 MB
    P mirror.p1           C State: initializing     Subdisks:     1 Size:        512 MB
    P striped.p1            State: up       Subdisks:     1 Size:        512 MB

    S myvol.p0.s0           State: up       PO:        0  B Size:        512 MB
    S mirror.p0.s0          State: up       PO:        0  B Size:        512 MB
    S mirror.p1.s0          State: empty    PO:        0  B Size:        512 MB
    S striped.p0.s0         State: up       PO:        0  B Size:        128 MB
    S striped.p0.s1         State: up       PO:      512 kB Size:        128 MB
    S striped.p0.s2         State: up       PO:     1024 kB Size:        128 MB
    S striped.p0.s3         State: up       PO:     1536 kB Size:        128 MB</pre></div></div><div id=vinum-striped-vol class=imageblock><div class=content><img src=../../../images/articles/vinum/vinum-striped-vol.png alt="vinum striped vol"></div><div class=title>Figure 6. A Striped <span class=filename>vinum</span> Volume</div></div><div class=paragraph><p>This volume is represented in <a href=../vinum/#vinum-striped-vol>A Striped <span class=filename>vinum</span> Volume</a>.
The darkness of the stripes indicates the position within the plex address space, where the lightest stripes come first and the darkest last.</p></div></div><div class=sect2><h3 id=_resilience_and_performance>5.4. Resilience and Performance<a class=anchor href=#_resilience_and_performance></a></h3><div class=paragraph><p><a id=vinum-resilience></a>With sufficient hardware, it is possible to build volumes which show both increased resilience and increased performance compared to standard UNIX® partitions.
A typical configuration file might be:</p></div><div class="literalblock programlisting"><div class=content><pre>	volume raid10
      plex org striped 512k
        sd length 102480k drive a
        sd length 102480k drive b
        sd length 102480k drive c
        sd length 102480k drive d
        sd length 102480k drive e
      plex org striped 512k
        sd length 102480k drive c
        sd length 102480k drive d
        sd length 102480k drive e
        sd length 102480k drive a
        sd length 102480k drive b</pre></div></div><div class=paragraph><p>The subdisks of the second plex are offset by two drives from those of the first plex.
This helps to ensure that writes do not go to the same subdisks even if a transfer goes over two drives.</p></div><div class=paragraph><p><a href=../vinum/#vinum-raid10-vol>A Mirrored</a> represents the structure of this volume.</p></div><div id=vinum-raid10-vol class=imageblock><div class=content><img src=../../../images/articles/vinum/vinum-raid10-vol.png alt="vinum raid10 vol"></div><div class=title>Figure 7. A Mirrored, Striped <span class=filename>vinum</span> Volume</div></div></div></div></div><div class=sect1><h2 id=vinum-object-naming>6. Object Naming<a class=anchor href=#vinum-object-naming></a></h2><div class=sectionbody><div class=paragraph><p><span class=filename>vinum</span> assigns default names to plexes and subdisks, although they may be overridden.
Overriding the default names is not recommended as it does not bring a significant advantage and it can cause confusion.</p></div><div class=paragraph><p>Names may contain any non-blank character, but it is recommended to restrict them to letters, digits and the underscore characters.
The names of volumes, plexes, and subdisks may be up to 64 characters long, and the names of drives may be up to 32 characters long.</p></div><div class=paragraph><p><span class=filename>vinum</span> objects are assigned device nodes in the hierarchy <span class=filename>/dev/gvinum</span>.
The configuration shown above would cause <span class=filename>vinum</span> to create the following device nodes:</p></div><div class=ulist><ul><li><p>Device entries for each volume. These are the main devices used by <span class=filename>vinum</span>. The configuration above would include the devices <span class=filename>/dev/gvinum/myvol</span>, <span class=filename>/dev/gvinum/mirror</span>, <span class=filename>/dev/gvinum/striped</span>, <span class=filename>/dev/gvinum/raid5</span> and <span class=filename>/dev/gvinum/raid10</span>.</p></li><li><p>All volumes get direct entries under <span class=filename>/dev/gvinum/</span>.</p></li><li><p>The directories <span class=filename>/dev/gvinum/plex</span>, and <span class=filename>/dev/gvinum/sd</span>, which contain device nodes for each plex and for each subdisk, respectively.</p></li></ul></div><div class=paragraph><p>For example, consider the following configuration file:</p></div><div class="literalblock programlisting"><div class=content><pre>	drive drive1 device /dev/sd1h
	drive drive2 device /dev/sd2h
	drive drive3 device /dev/sd3h
	drive drive4 device /dev/sd4h
    volume s64 setupstate
      plex org striped 64k
        sd length 100m drive drive1
        sd length 100m drive drive2
        sd length 100m drive drive3
        sd length 100m drive drive4</pre></div></div><div class=paragraph><p>After processing this file, <a href="https://man.freebsd.org/cgi/man.cgi?query=gvinum&amp;sektion=8&amp;format=html">gvinum(8)</a> creates the following structure in <span class=filename>/dev/gvinum</span>:</p></div><div class="literalblock programlisting"><div class=content><pre>	drwxr-xr-x  2 root  wheel       512 Apr 13
16:46 plex
	crwxr-xr--  1 root  wheel   91,   2 Apr 13 16:46 s64
	drwxr-xr-x  2 root  wheel       512 Apr 13 16:46 sd

    /dev/vinum/plex:
    total 0
    crwxr-xr--  1 root  wheel   25, 0x10000002 Apr 13 16:46 s64.p0

    /dev/vinum/sd:
    total 0
    crwxr-xr--  1 root  wheel   91, 0x20000002 Apr 13 16:46 s64.p0.s0
    crwxr-xr--  1 root  wheel   91, 0x20100002 Apr 13 16:46 s64.p0.s1
    crwxr-xr--  1 root  wheel   91, 0x20200002 Apr 13 16:46 s64.p0.s2
    crwxr-xr--  1 root  wheel   91, 0x20300002 Apr 13 16:46 s64.p0.s3</pre></div></div><div class=paragraph><p>Although it is recommended that plexes and subdisks should not be allocated specific names, <span class=filename>vinum</span> drives must be named.
This makes it possible to move a drive to a different location and still recognize it automatically.
Drive names may be up to 32 characters long.</p></div><div class=sect2><h3 id=_creating_file_systems>6.1. Creating File Systems<a class=anchor href=#_creating_file_systems></a></h3><div class=paragraph><p>Volumes appear to the system to be identical to disks, with one exception.
Unlike UNIX® drives, <span class=filename>vinum</span> does not partition volumes, which thus do not contain a partition table.
This has required modification to some disk utilities, notably <a href="https://man.freebsd.org/cgi/man.cgi?query=newfs&amp;sektion=8&amp;format=html">newfs(8)</a>, so that it does not try to interpret the last letter of a <span class=filename>vinum</span> volume name as a partition identifier.
For example, a disk drive may have a name like <span class=filename>/dev/ad0a</span> or <span class=filename>/dev/da2h</span>.
These names represent the first partition (<span class=filename>a</span>) on the first (0) IDE disk (<span class=filename>ad</span>) and the eighth partition (<span class=filename>h</span>) on the third (2) SCSI disk (<span class=filename>da</span>) respectively.
By contrast, a <span class=filename>vinum</span> volume might be called <span class=filename>/dev/gvinum/concat</span>, which has no relationship with a partition name.</p></div><div class=paragraph><p>To create a file system on this volume, use <a href="https://man.freebsd.org/cgi/man.cgi?query=newfs&amp;sektion=8&amp;format=html">newfs(8)</a>:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># newfs /dev/gvinum/concat</span></code></pre></div></div></div></div></div><div class=sect1><h2 id=vinum-config>7. Configuring <span class=filename>vinum</span><a class=anchor href=#vinum-config></a></h2><div class=sectionbody><div class=paragraph><p>The <span class=filename>GENERIC</span> kernel does not contain <span class=filename>vinum</span>.
It is possible to build a custom kernel which includes <span class=filename>vinum</span>, but this is not recommended.
The standard way to start <span class=filename>vinum</span> is as a kernel module.
<a href="https://man.freebsd.org/cgi/man.cgi?query=kldload&amp;sektion=8&amp;format=html">kldload(8)</a> is not needed because when <a href="https://man.freebsd.org/cgi/man.cgi?query=gvinum&amp;sektion=8&amp;format=html">gvinum(8)</a> starts, it checks whether the module has been loaded, and if it is not, it loads it automatically.</p></div><div class=sect2><h3 id=_startup>7.1. Startup<a class=anchor href=#_startup></a></h3><div class=paragraph><p><span class=filename>vinum</span> stores configuration information on the disk slices in essentially the same form as in the configuration files.
When reading from the configuration database, <span class=filename>vinum</span> recognizes a number of keywords which are not allowed in the configuration files.
For example, a disk configuration might contain the following text:</p></div><div class="literalblock programlisting"><div class=content><pre>volume myvol state up
volume bigraid state down
plex name myvol.p0 state up org concat vol myvol
plex name myvol.p1 state up org concat vol myvol
plex name myvol.p2 state init org striped 512b vol myvol
plex name bigraid.p0 state initializing org raid5 512b vol bigraid
sd name myvol.p0.s0 drive a plex myvol.p0 state up len 1048576b driveoffset 265b plexoffset 0b
sd name myvol.p0.s1 drive b plex myvol.p0 state up len 1048576b driveoffset 265b plexoffset 1048576b
sd name myvol.p1.s0 drive c plex myvol.p1 state up len 1048576b driveoffset 265b plexoffset 0b
sd name myvol.p1.s1 drive d plex myvol.p1 state up len 1048576b driveoffset 265b plexoffset 1048576b
sd name myvol.p2.s0 drive a plex myvol.p2 state init len 524288b driveoffset 1048841b plexoffset 0b
sd name myvol.p2.s1 drive b plex myvol.p2 state init len 524288b driveoffset 1048841b plexoffset 524288b
sd name myvol.p2.s2 drive c plex myvol.p2 state init len 524288b driveoffset 1048841b plexoffset 1048576b
sd name myvol.p2.s3 drive d plex myvol.p2 state init len 524288b driveoffset 1048841b plexoffset 1572864b
sd name bigraid.p0.s0 drive a plex bigraid.p0 state initializing len 4194304b driveoff set 1573129b plexoffset 0b
sd name bigraid.p0.s1 drive b plex bigraid.p0 state initializing len 4194304b driveoff set 1573129b plexoffset 4194304b
sd name bigraid.p0.s2 drive c plex bigraid.p0 state initializing len 4194304b driveoff set 1573129b plexoffset 8388608b
sd name bigraid.p0.s3 drive d plex bigraid.p0 state initializing len 4194304b driveoff set 1573129b plexoffset 12582912b
sd name bigraid.p0.s4 drive e plex bigraid.p0 state initializing len 4194304b driveoff set 1573129b plexoffset 16777216b</pre></div></div><div class=paragraph><p>The obvious differences here are the presence of explicit location information and naming, both of which are allowed but discouraged, and the information on the states.
<span class=filename>vinum</span> does not store information about drives in the configuration information.
It finds the drives by scanning the configured disk drives for partitions with a <span class=filename>vinum</span> label.
This enables <span class=filename>vinum</span> to identify drives correctly even if they have been assigned different UNIX® drive IDs.</p></div><div class=sect3><h4 id=vinum-rc-startup>7.1.1. Automatic Startup<a class=anchor href=#vinum-rc-startup></a></h4><div class=paragraph><p><em>Gvinum</em> always features an automatic startup once the kernel module is loaded, via <a href="https://man.freebsd.org/cgi/man.cgi?query=loader.conf&amp;sektion=5&amp;format=html">loader.conf(5)</a>.
To load the <em>Gvinum</em> module at boot time, add <code>geom_vinum_load="YES"</code> to <span class=filename>/boot/loader.conf</span>.</p></div><div class=paragraph><p>When <span class=filename>vinum</span> is started with <code>gvinum start</code>, <span class=filename>vinum</span> reads the configuration database from one of the <span class=filename>vinum</span> drives.
Under normal circumstances, each drive contains an identical copy of the configuration database, so it does not matter which drive is read.
After a crash, however, <span class=filename>vinum</span> must determine which drive was updated most recently and read the configuration from this drive.
It then updates the configuration, if necessary, from progressively older drives.</p></div></div></div></div></div><div class=sect1><h2 id=vinum-root>8. Using <span class=filename>vinum</span> for the Root File System<a class=anchor href=#vinum-root></a></h2><div class=sectionbody><div class=paragraph><p>For a machine that has fully-mirrored file systems using <span class=filename>vinum</span>, it is desirable to also mirror the root file system.
Setting up such a configuration is less trivial than mirroring an arbitrary file system because:</p></div><div class=ulist><ul><li><p>The root file system must be available very early during the boot process, so the <span class=filename>vinum</span> infrastructure must already be available at this time.</p></li><li><p>The volume containing the root file system also contains the system bootstrap and the kernel. These must be read using the host system’s native utilities, such as the BIOS, which often cannot be taught about the details of <span class=filename>vinum</span>.</p></li></ul></div><div class=paragraph><p>In the following sections, the term "root volume" is generally used to describe the <span class=filename>vinum</span> volume that contains the root file system.</p></div><div class=sect2><h3 id=_starting_up_vinum_early_enough_for_the_root_file_system>8.1. Starting up <span class=filename>vinum</span> Early Enough for the Root File System<a class=anchor href=#_starting_up_vinum_early_enough_for_the_root_file_system></a></h3><div class=paragraph><p><span class=filename>vinum</span> must be available early in the system boot as <a href="https://man.freebsd.org/cgi/man.cgi?query=loader&amp;sektion=8&amp;format=html">loader(8)</a> must be able to load the vinum kernel module before starting the kernel.
This can be accomplished by putting this line in <span class=filename>/boot/loader.conf</span>:</p></div><div class="literalblock programlisting"><div class=content><pre>geom_vinum_load=&#34;YES&#34;</pre></div></div></div><div class=sect2><h3 id=_making_a_vinum_based_root_volume_accessible_to_the_bootstrap>8.2. Making a <span class=filename>vinum</span>-based Root Volume Accessible to the Bootstrap<a class=anchor href=#_making_a_vinum_based_root_volume_accessible_to_the_bootstrap></a></h3><div class=paragraph><p>The current FreeBSD bootstrap is only 7.5 KB of code and does not understand the internal <span class=filename>vinum</span> structures.
This means that it cannot parse the <span class=filename>vinum</span> configuration data or figure out the elements of a boot volume.
Thus, some workarounds are necessary to provide the bootstrap code with the illusion of a standard <code>a</code> partition that contains the root file system.</p></div><div class=paragraph><p>For this to be possible, the following requirements must be met for the root volume:</p></div><div class=ulist><ul><li><p>The root volume must not be a stripe or <code>RAID</code>-5.</p></li><li><p>The root volume must not contain more than one concatenated subdisk per plex.</p></li></ul></div><div class=paragraph><p>Note that it is desirable and possible to use multiple plexes, each containing one replica of the root file system.
The bootstrap process will only use one replica for finding the bootstrap and all boot files, until the kernel mounts the root file system.
Each single subdisk within these plexes needs its own <code>a</code> partition illusion, for the respective device to be bootable.
It is not strictly needed that each of these faked <code>a</code> partitions is located at the same offset within its device, compared with other devices containing plexes of the root volume.
However, it is probably a good idea to create the <span class=filename>vinum</span> volumes that way so the resulting mirrored devices are symmetric, to avoid confusion.</p></div><div class=paragraph><p>To set up these <code>a</code> partitions for each device containing part of the root volume, the following is required:</p></div><div class="exampleblock procedure"><div class=content><div class="olist arabic"><ol class=arabic><li><p>The location, offset from the beginning of the device, and size of this device’s subdisk that is part of the root volume needs to be examined, using the command:</p><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># gvinum l -rv root</span></code></pre></div></div><div class=paragraph><p><span class=filename>vinum</span> offsets and sizes are measured in bytes.
They must be divided by 512 to obtain the block numbers that are to be used by <code>bsdlabel</code>.</p></div></li><li><p>Run this command for each device that participates in the root volume:</p><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># bsdlabel -e devname</span></code></pre></div></div><div class=paragraph><p><em>devname</em> must be either the name of the disk, like <span class=filename>da0</span> for disks without a slice table, or the name of the slice, like <span class=filename>ad0s1</span>.</p></div><div class=paragraph><p>If there is already an <code>a</code> partition on the device from a pre-<span class=filename>vinum</span> root file system, it should be renamed to something else so that it remains accessible (just in case), but will no longer be used by default to bootstrap the system.
A currently mounted root file system cannot be renamed, so this must be executed either when being booted from a "Fixit" media, or in a two-step process where, in a mirror, the disk that is not been currently booted is manipulated first.</p></div><div class=paragraph><p>The offset of the <span class=filename>vinum</span> partition on this device (if any) must be added to the offset of the respective root volume subdisk on this device.
The resulting value will become the <code>offset</code> value for the new <code>a</code> partition.
The <code>size</code> value for this partition can be taken verbatim from the calculation above.
The <code>fstype</code> should be <code>4.2BSD</code>.
The <code>fsize</code>, <code>bsize</code>, and <code>cpg</code> values should be chosen to match the actual file system, though they are fairly unimportant within this context.</p></div><div class=paragraph><p>That way, a new <code>a</code> partition will be established that overlaps the <span class=filename>vinum</span> partition on this device.
<code>bsdlabel</code> will only allow for this overlap if the <span class=filename>vinum</span> partition has properly been marked using the <code>vinum</code> fstype.</p></div></li><li><p>A faked <code>a</code> partition now exists on each device that has one replica of the root volume. It is highly recommendable to verify the result using a command like:</p><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell><span class=c># fsck -n /dev/devnamea</span></code></pre></div></div></li></ol></div></div></div><div class=paragraph><p>It should be remembered that all files containing control information must be relative to the root file system in the <span class=filename>vinum</span> volume which, when setting up a new <span class=filename>vinum</span> root volume, might not match the root file system that is currently active.
So in particular, <span class=filename>/etc/fstab</span> and <span class=filename>/boot/loader.conf</span> need to be taken care of.</p></div><div class=paragraph><p>At next reboot, the bootstrap should figure out the appropriate control information from the new <span class=filename>vinum</span>-based root file system, and act accordingly.
At the end of the kernel initialization process, after all devices have been announced, the prominent notice that shows the success of this setup is a message like:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell>Mounting root from ufs:/dev/gvinum/root</code></pre></div></div></div><div class=sect2><h3 id=_example_of_a_vinum_based_root_setup>8.3. Example of a <span class=filename>vinum</span>-based Root Setup<a class=anchor href=#_example_of_a_vinum_based_root_setup></a></h3><div class=paragraph><p>After the <span class=filename>vinum</span> root volume has been set up, the output of <code>gvinum l -rv root</code> could look like:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell>...
Subdisk root.p0.s0:
		Size:        125829120 bytes <span class=o>(</span>120 MB<span class=o>)</span>
		State: up
		Plex root.p0 at offset 0 <span class=o>(</span>0  B<span class=o>)</span>
		Drive disk0 <span class=o>(</span>/dev/da0h<span class=o>)</span> at offset 135680 <span class=o>(</span>132 kB<span class=o>)</span>

Subdisk root.p1.s0:
		Size:        125829120 bytes <span class=o>(</span>120 MB<span class=o>)</span>
		State: up
		Plex root.p1 at offset 0 <span class=o>(</span>0  B<span class=o>)</span>
		Drive disk1 <span class=o>(</span>/dev/da1h<span class=o>)</span> at offset 135680 <span class=o>(</span>132 kB<span class=o>)</span></code></pre></div></div><div class=paragraph><p>The values to note are <code>135680</code> for the offset, relative to partition <span class=filename>/dev/da0h</span>.
This translates to 265 512-byte disk blocks in `bsdlabel’s terms.
Likewise, the size of this root volume is 245760 512-byte blocks.
<span class=filename>/dev/da1h</span>, containing the second replica of this root volume, has a symmetric setup.</p></div><div class=paragraph><p>The bsdlabel for these devices might look like:</p></div><div class=listingblock><div class=content><pre class="rouge highlight"><code data-lang=shell>...
8 partitions:
<span class=c>#        size   offset    fstype   [fsize bsize bps/cpg]</span>
  a:   245760      281    4.2BSD     2048 16384     0  <span class=c># (Cyl.    0*- 15*)</span>
  c: 71771688        0    unused        0     0        <span class=c># (Cyl.    0 - 4467*)</span>
  h: 71771672       16     vinum                       <span class=c># (Cyl.    0*- 4467*)</span></code></pre></div></div><div class=paragraph><p>It can be observed that the <code>size</code> parameter for the faked <code>a</code> partition matches the value outlined above, while the <code>offset</code> parameter is the sum of the offset within the <span class=filename>vinum</span> partition <code>h</code>, and the offset of this partition within the device or slice.
This is a typical setup that is necessary to avoid the problem described in
<a href=../vinum/#vinum-root-panic>Nothing Boots</a>.
The entire <code>a</code> partition is completely within the <code>h</code> partition containing all the <span class=filename>vinum</span> data for this device.</p></div><div class=paragraph><p>In the above example, the entire device is dedicated to <span class=filename>vinum</span> and there is no leftover pre-<span class=filename>vinum</span> root partition.</p></div></div><div class=sect2><h3 id=_troubleshooting>8.4. Troubleshooting<a class=anchor href=#_troubleshooting></a></h3><div class=paragraph><p>The following list contains a few known pitfalls and solutions.</p></div><div class=sect3><h4 id=_system_bootstrap_loads_but_system_does_not_boot>8.4.1. System Bootstrap Loads, but System Does Not Boot<a class=anchor href=#_system_bootstrap_loads_but_system_does_not_boot></a></h4><div class=paragraph><p>If for any reason the system does not continue to boot, the bootstrap can be interrupted by pressing <kbd>space</kbd> at the 10-seconds warning.
The loader variable <code>vinum.autostart</code> can be examined by typing <code>show</code> and manipulated using <code>set</code> or <code>unset</code>.</p></div><div class=paragraph><p>If the <span class=filename>vinum</span> kernel module was not yet in the list of modules to load automatically, type <code>load geom_vinum</code>.</p></div><div class=paragraph><p>When ready, the boot process can be continued by typing <code>boot -as</code> which <code>-as</code> requests the kernel to ask for the root file system to mount (<code>-a</code>) and make the boot process stop in single-user mode (<code>-s</code>), where the root file system is mounted read-only.
That way, even if only one plex of a multi-plex volume has been mounted, no data inconsistency between plexes is being risked.</p></div><div class=paragraph><p>At the prompt asking for a root file system to mount, any device that contains a valid root file system can be entered.
If <span class=filename>/etc/fstab</span> is set up correctly, the default should be something like <code>ufs:/dev/gvinum/root</code>.
A typical alternate choice would be something like <code>ufs:da0d</code> which could be a hypothetical partition containing the pre-<span class=filename>vinum</span> root file system.
Care should be taken if one of the alias <code>a</code> partitions is entered here, that it actually references the subdisks of the <span class=filename>vinum</span> root device, because in a mirrored setup, this would only mount one piece of a mirrored root device.
If this file system is to be mounted read-write later on, it is necessary to remove the other plex(es) of the <span class=filename>vinum</span> root volume since these plexes would otherwise carry inconsistent data.</p></div></div><div class=sect3><h4 id=_only_primary_bootstrap_loads>8.4.2. Only Primary Bootstrap Loads<a class=anchor href=#_only_primary_bootstrap_loads></a></h4><div class=paragraph><p>If <span class=filename>/boot/loader</span> fails to load, but the primary bootstrap still loads (visible by a single dash in the left column of the screen right after the boot process starts), an attempt can be made to interrupt the primary bootstrap by pressing <kbd>space</kbd>.
This will make the bootstrap stop in <a href=https://docs.freebsd.org/en/books/handbook/#boot-boot1>stage two</a>.
An attempt can be made here to boot off an alternate partition, like the partition containing the previous root file system that has been moved away from <code>a</code>.</p></div></div><div class=sect3><h4 id=vinum-root-panic>8.4.3. Nothing Boots, the Bootstrap Panics<a class=anchor href=#vinum-root-panic></a></h4><div class=paragraph><p>This situation will happen if the bootstrap had been destroyed by the <span class=filename>vinum</span> installation.
Unfortunately, <span class=filename>vinum</span> accidentally leaves only 4 KB at the beginning of its partition free before starting to write its <span class=filename>vinum</span> header information.
However, the stage one and two bootstraps plus the bsdlabel require 8 KB.
So if a <span class=filename>vinum</span> partition was started at offset 0 within a slice or disk that was meant to be bootable, the <span class=filename>vinum</span> setup will trash the bootstrap.</p></div><div class=paragraph><p>Similarly, if the above situation has been recovered, by booting from a "Fixit" media, and the bootstrap has been re-installed using <code>bsdlabel -B</code> as described in <a href=https://docs.freebsd.org/en/books/handbook/#boot-boot1>stage two</a>, the bootstrap will trash the <span class=filename>vinum</span> header, and <span class=filename>vinum</span> will no longer find its disk(s).
Though no actual <span class=filename>vinum</span> configuration data or data in <span class=filename>vinum</span> volumes will be trashed, and it would be possible to recover all the data by entering exactly the same <span class=filename>vinum</span> configuration data again, the situation is hard to fix.
It is necessary to move the entire <span class=filename>vinum</span> partition by at least 4 KB, to have the <span class=filename>vinum</span> header and the system bootstrap no longer collide.</p></div></div></div></div></div><hr><div class=last-modified><p><strong>Last modified on</strong>: August 11, 2024 by <a href="https://cgit.freebsd.org/doc/commit/?id=557464e66e" target=_blank>Fernando Apesteguía</a></p></div></div><aside class=toc><div class=toc-content><h3>Table of Contents</h3><nav id=TableOfContents><ul><li><a href=#vinum-synopsis>1. Synopsis</a></li><li><a href=#vinum-access-bottlenecks>2. Access Bottlenecks</a></li><li><a href=#vinum-data-integrity>3. Data Integrity</a></li><li><a href=#vinum-objects>4. <span class=filename>vinum</span> Objects</a></li><li><a href=#vinum-examples>5. Some Examples</a></li><li><a href=#vinum-object-naming>6. Object Naming</a></li><li><a href=#vinum-config>7. Configuring <span class=filename>vinum</span></a></li><li><a href=#vinum-root>8. Using <span class=filename>vinum</span> for the Root File System</a></li></ul></nav><hr><div class=resources><h3>Resources</h3><ul class=contents><li><i class="fa fa-file-pdf-o" aria-hidden=true title="Download PDF"></i><a href=https://download.freebsd.org/doc/en/articles/vinum/vinum_en.pdf>Download PDF</a></li><li><i class="fa fa-pencil-square-o" aria-hidden=true title="Edit this page"></i><a href=https://github.com/freebsd/freebsd-doc/blob/main/documentation/content/en/_index target=_blank>Edit this page</a></li></ul></div></div></aside></main><footer><div class=footer-container><section class=logo-column><img src=http://172.16.201.134:1313/images/FreeBSD-colors.svg width=160 height=50 alt="FreeBSD logo"><div class=options-container><div class=language-container><a id=languages href=http://172.16.201.134:1313/en/languages><img src=http://172.16.201.134:1313/images/language.png class=language-image alt="Choose language">
<span>English</span></a></div><div class=theme-container><select id=theme-chooser><option value=theme-system>System</option><option value=theme-light>Light</option><option value=theme-dark>Dark</option><option value=theme-high-contrast>High contrast</option></select></div></div></section><section class=about-column><h3 class=column-title>About</h3><ul class=column-elements-container><li><a href=https://www.freebsd.org/about/ target=_blank class=column-element>FreeBSD</a></li><li><a href=https://freebsdfoundation.org/ target=_blank class=column-element>FreeBSD Foundation</a></li><li><a href=https://www.freebsd.org/where/ target=_blank class=column-element>Get FreeBSD</a></li><li><a href=https://www.freebsd.org/internal/code-of-conduct target=_blank class=column-element>Code of Conduct</a></li><li><a href=https://www.freebsd.org/security/ target=_blank class=column-element>Security Advisories</a></li></ul></section><section class=documentation-column><h3 class=column-title>Documentation</h3><ul class=column-elements-container><li><a href=/en class=column-element>Documentation portal</a></li><li><a href=https://man.FreeBSD.org target=_blank class=column-element>Manual pages</a></li><li><a href=https://papers.FreeBSD.org target=_blank class=column-element>Presentations and papers</a></li><li><a href=https://docs-archive.freebsd.org/doc/ target=_blank class=column-element>Previous versions</a></li><li><a href=https://docs-archive.freebsd.org/44doc/ target=_blank class=column-element>4.4BSD Documents</a></li><li><a href=https://wiki.freebsd.org/ target=_blank class=column-element>Wiki</a></li></ul></section><section class=community-column><h3 class=column-title>Community</h3><ul class=column-elements-container><li><a href=http://172.16.201.134:1313/en/articles/contributing class=column-element>Get involved</a></li><li><a href=https://forums.freebsd.org/ target=_blank class=column-element>Community forum</a></li><li><a href=https://lists.freebsd.org/ target=_blank class=column-element>Mailing lists</a></li><li><a href=https://wiki.freebsd.org/IRC/Channels target=_blank class=column-element>IRC Channels</a></li><li><a href=https://bugs.freebsd.org/bugzilla/ target=_blank class=column-element>Bug Tracker</a></li></ul></section><section class=legal-column><h3 class=column-title>Legal</h3><ul class=column-elements-container><li><a href=https://freebsdfoundation.org/donate/ target=_blank class=column-element>Donations</a></li><li><a href=https://www.freebsd.org/copyright/freebsd-license/ target=_blank class=column-element>Licensing</a></li><li><a href=https://www.freebsd.org/privacy/ target=_blank class=column-element>Privacy Policy</a></li><li><a href=https://www.freebsd.org/copyright/ target=_blank class=column-element>Legal notices</a></li></ul></section><section class=copyright-column><p>&copy; 1994-2024 The FreeBSD Project. All rights reserved</p><span>Made with <span class=heart>♥</span> by the FreeBSD Community</span></section></div></footer></body></html>